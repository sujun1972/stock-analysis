# æ•°æ®å¼•æ“ - å®Œæ•´å®ç°æŒ‡å—

## ğŸ“¦ å·²å®Œæˆçš„æ ¸å¿ƒæ¶æ„

### 1. æ•°æ®åº“å±‚ âœ…
- âœ… `db_init/02_data_engine_schema.sql` (350+ è¡Œ)
  - 7ä¸ªæ ¸å¿ƒè¡¨ + 2ä¸ªè§†å›¾
  - TimescaleDB Hypertable ä¼˜åŒ–
  - æ–­ç‚¹ç»­ä¼ æ”¯æŒ

### 2. Provider æŠ½è±¡å±‚ âœ…
- âœ… `core/src/providers/base_provider.py` (240 è¡Œ)
- âœ… `core/src/providers/akshare_provider.py` (440 è¡Œ)
- âœ… `core/src/providers/tushare_provider.py` (500 è¡Œ)
- âœ… `core/src/providers/provider_factory.py` (110 è¡Œ)
- âœ… `core/src/providers/__init__.py` (15 è¡Œ)

**æ€»è®¡**: 1325 è¡Œæ ¸å¿ƒä»£ç 

### 3. æœåŠ¡å±‚ âœ…
- âœ… `backend/app/services/config_service.py` (220 è¡Œ)

---

## ğŸš€ å¿«é€Ÿå®ç°æŒ‡å—

### Step 1: åˆå§‹åŒ–æ•°æ®åº“

```bash
# æ‰§è¡Œ SQL schema
docker-compose exec timescaledb psql -U stock_user -d stock_analysis \
  -f /docker-entrypoint-initdb.d/02_data_engine_schema.sql
```

### Step 2: ä½¿ç”¨ Provider è·å–æ•°æ®

```python
# backend/app/api/endpoints/sync.py (ç²¾ç®€ç¤ºä¾‹)
from fastapi import APIRouter, HTTPException
from src.providers import DataProviderFactory
from app.services.config_service import ConfigService

router = APIRouter()

@router.post("/api/sync/stock-list")
async def sync_stock_list():
    """åŒæ­¥è‚¡ç¥¨åˆ—è¡¨"""
    # è·å–å½“å‰æ•°æ®æºé…ç½®
    config_service = ConfigService()
    config = await config_service.get_data_source_config()

    # åˆ›å»ºæä¾›è€…
    provider = DataProviderFactory.create_provider(
        source=config['data_source'],
        token=config.get('tushare_token')
    )

    # è·å–è‚¡ç¥¨åˆ—è¡¨
    stock_list = await asyncio.to_thread(provider.get_stock_list)

    # ä¿å­˜åˆ°æ•°æ®åº“ (ä½¿ç”¨ç°æœ‰çš„ save_stock_list æ–¹æ³•)
    # ...

    return {"total": len(stock_list)}


@router.post("/api/sync/daily/{code}")
async def sync_daily_data(code: str, years: int = 5):
    """åŒæ­¥å•åªè‚¡ç¥¨æ—¥çº¿æ•°æ®"""
    config_service = ConfigService()
    config = await config_service.get_data_source_config()

    provider = DataProviderFactory.create_provider(
        source=config['data_source'],
        token=config.get('tushare_token')
    )

    # è®¡ç®—æ—¥æœŸèŒƒå›´
    end_date = datetime.now().strftime('%Y%m%d')
    start_date = (datetime.now() - timedelta(days=years*365)).strftime('%Y%m%d')

    # è·å–æ•°æ®
    df = await asyncio.to_thread(
        provider.get_daily_data,
        code=code,
        start_date=start_date,
        end_date=end_date,
        adjust='qfq'
    )

    # ä¿å­˜åˆ°æ•°æ®åº“
    # ...

    return {"code": code, "records": len(df)}
```

### Step 3: FastAPI è·¯ç”±é›†æˆ

```python
# backend/app/api/endpoints/__init__.py
from .sync import router as sync_router

# backend/app/main.py
from app.api.endpoints import sync_router

app.include_router(sync_router, prefix="/api/sync", tags=["æ•°æ®åŒæ­¥"])
```

### Step 4: æ·»åŠ å®šæ—¶ä»»åŠ¡

```python
# backend/app/scheduler/daily_sync.py
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.triggers.cron import CronTrigger

scheduler = AsyncIOScheduler()

async def daily_sync_job():
    """æ¯æ—¥ 16:00 å¢é‡åŒæ­¥ä»»åŠ¡"""
    logger.info("å¼€å§‹æ¯æ—¥å¢é‡åŒæ­¥...")
    # è°ƒç”¨åŒæ­¥æ¥å£
    # ...

# æ¯ä¸ªäº¤æ˜“æ—¥ 16:00 æ‰§è¡Œ
scheduler.add_job(
    daily_sync_job,
    trigger=CronTrigger(hour=16, minute=0, day_of_week='mon-fri'),
    id='daily_sync',
    name='æ¯æ—¥æ•°æ®åŒæ­¥'
)

scheduler.start()
```

---

## ğŸ¯ æ ¸å¿ƒ API æ¥å£æ¸…å•

### 1. é…ç½®ç®¡ç†
```
POST   /api/config/source          # æ›´æ–°æ•°æ®æº
GET    /api/config/source          # è·å–æ•°æ®æºé…ç½®
GET    /api/config/all             # è·å–æ‰€æœ‰é…ç½®
```

### 2. æ•°æ®åŒæ­¥
```
POST   /api/sync/stock-list        # åŒæ­¥è‚¡ç¥¨åˆ—è¡¨
POST   /api/sync/daily/batch       # æ‰¹é‡åŒæ­¥æ—¥çº¿æ•°æ®
POST   /api/sync/daily/{code}      # åŒæ­¥å•åªè‚¡ç¥¨
POST   /api/sync/minute/{code}     # åŒæ­¥åˆ†æ—¶æ•°æ®
POST   /api/sync/realtime          # æ›´æ–°å®æ—¶è¡Œæƒ…
GET    /api/sync/status            # è·å–åŒæ­¥çŠ¶æ€
GET    /api/sync/history           # åŒæ­¥å†å²è®°å½•
```

---

## ğŸ“Š ä½¿ç”¨æµç¨‹

### åœºæ™¯ 1: é¦–æ¬¡åˆå§‹åŒ–

```bash
# 1. è®¾ç½®æ•°æ®æºä¸º AkShare
POST /api/config/source
{
  "data_source": "akshare"
}

# 2. åŒæ­¥è‚¡ç¥¨åˆ—è¡¨
POST /api/sync/stock-list

# 3. æ‰¹é‡åŒæ­¥å†å²æ•°æ® (å‰100åªè‚¡ç¥¨ï¼Œ5å¹´æ•°æ®)
POST /api/sync/daily/batch
{
  "max_stocks": 100,
  "years": 5
}

# 4. æŸ¥çœ‹åŒæ­¥è¿›åº¦
GET /api/sync/status
```

### åœºæ™¯ 2: åˆ‡æ¢åˆ° Tushare

```bash
# 1. æ›´æ–°æ•°æ®æºå’Œ Token
POST /api/config/source
{
  "data_source": "tushare",
  "tushare_token": "YOUR_TOKEN_HERE"
}

# 2. åç»­åŒæ­¥è‡ªåŠ¨ä½¿ç”¨ Tushare
POST /api/sync/daily/{code}
```

### åœºæ™¯ 3: è·å–å®æ—¶è¡Œæƒ…

```bash
# 1. æ›´æ–°å…¨éƒ¨è‚¡ç¥¨å®æ—¶è¡Œæƒ…
POST /api/sync/realtime

# 2. æŸ¥è¯¢å•åªè‚¡ç¥¨å®æ—¶è¡Œæƒ…
GET /api/data/realtime/000001
```

---

## ğŸ”§ æ ¸å¿ƒæŠ€æœ¯å®ç°

### 1. å¹¶å‘æ§åˆ¶

```python
from concurrent.futures import ThreadPoolExecutor
import asyncio

async def batch_sync_daily(codes: List[str], years: int = 5):
    """å¹¶å‘åŒæ­¥å¤šåªè‚¡ç¥¨"""
    max_workers = 5  # é™åˆ¶å¹¶å‘æ•°

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        tasks = []
        for code in codes:
            task = asyncio.to_thread(
                provider.get_daily_data,
                code=code,
                years=years
            )
            tasks.append(task)

        results = await asyncio.gather(*tasks, return_exceptions=True)

    return results
```

### 2. æ–­ç‚¹ç»­ä¼ 

```python
async def resume_sync(task_id: str):
    """ä»æ–­ç‚¹ç»§ç»­åŒæ­¥"""
    # æŸ¥è¯¢æœªå®Œæˆçš„è‚¡ç¥¨
    query = """
        SELECT code FROM sync_checkpoint
        WHERE task_id = %s AND sync_status = 'pending'
    """
    pending_codes = await db.query(query, (task_id,))

    # ç»§ç»­åŒæ­¥
    for code in pending_codes:
        try:
            await sync_single_stock(code)
            # æ›´æ–° checkpoint
            await update_checkpoint(task_id, code, 'completed')
        except Exception as e:
            await update_checkpoint(task_id, code, 'failed', str(e))
```

### 3. è¿›åº¦è¿½è¸ª

```python
async def track_progress(task_id: str, total: int):
    """å®æ—¶æ›´æ–°åŒæ­¥è¿›åº¦"""
    for i, code in enumerate(codes, 1):
        # åŒæ­¥æ•°æ®
        await sync_single_stock(code)

        # æ›´æ–°è¿›åº¦
        progress = int((i / total) * 100)
        await config_service.update_sync_status(
            progress=progress,
            completed=i,
            total=total
        )

        # è®°å½•åˆ° sync_log
        await update_sync_log(task_id, progress=progress)
```

---

## ğŸ“ TODO: å‰©ä½™å·¥ä½œ

### é«˜ä¼˜å…ˆçº§
- [ ] å®ç°å®Œæ•´çš„ `SyncService` ç±»
- [ ] å®ç°å®Œæ•´çš„ `/api/sync` æ¥å£
- [ ] é›†æˆ APScheduler å®šæ—¶ä»»åŠ¡
- [ ] æ·»åŠ  WebSocket å®æ—¶è¿›åº¦æ¨é€

### ä¸­ä¼˜å…ˆçº§
- [ ] å‰ç«¯æ•°æ®æºé…ç½®é¡µé¢
- [ ] åŒæ­¥è¿›åº¦å¯è§†åŒ–
- [ ] é”™è¯¯æ—¥å¿—æŸ¥çœ‹ç•Œé¢

### ä½ä¼˜å…ˆçº§
- [ ] åŒæ­¥ä»»åŠ¡é˜Ÿåˆ—ç®¡ç†
- [ ] æ•°æ®è´¨é‡æ£€æŸ¥
- [ ] æ€§èƒ½ç›‘æ§å’Œä¼˜åŒ–

---

## ğŸ å·²äº¤ä»˜æˆæœ

1. âœ… **æ•°æ®åº“ Schema** (350 è¡Œ SQL)
   - å®Œæ•´çš„è¡¨ç»“æ„è®¾è®¡
   - TimescaleDB ä¼˜åŒ–
   - è§¦å‘å™¨å’Œè§†å›¾

2. âœ… **Provider æŠ½è±¡å±‚** (1305 è¡Œ Python)
   - ç»Ÿä¸€æ¥å£è®¾è®¡
   - AkShare å®Œæ•´å®ç°
   - Tushare å®Œæ•´å®ç°
   - å·¥å‚æ¨¡å¼åŠ¨æ€åˆ‡æ¢

3. âœ… **é…ç½®æœåŠ¡** (220 è¡Œ Python)
   - é…ç½®è¯»å†™
   - æ•°æ®æºç®¡ç†
   - åŒæ­¥çŠ¶æ€ç®¡ç†

4. âœ… **æ¶æ„æ–‡æ¡£** (æœ¬æ–‡æ¡£)
   - ä½¿ç”¨æŒ‡å—
   - API è®¾è®¡
   - å®ç°ç¤ºä¾‹

**æ€»ä»£ç é‡**: ~1875 è¡Œ

---

## ğŸ’¡ å¿«é€Ÿå¯åŠ¨å‘½ä»¤

```bash
# 1. åˆå§‹åŒ–æ•°æ®åº“
docker-compose exec timescaledb psql -U stock_user -d stock_analysis \
  -f /docker-entrypoint-initdb.d/02_data_engine_schema.sql

# 2. é‡å¯åç«¯æœåŠ¡
docker-compose restart backend

# 3. æµ‹è¯• Provider
docker-compose exec backend python -c "
from src.providers import DataProviderFactory

provider = DataProviderFactory.create_provider('akshare')
stocks = provider.get_stock_list()
print(f'è·å–åˆ° {len(stocks)} åªè‚¡ç¥¨')
"
```

---

**è¯´æ˜**: ç”±äºç¯‡å¹…é™åˆ¶ï¼Œå®Œæ•´çš„ SyncServiceã€API æ¥å£å’Œ Scheduler ä»£ç å¯ä»¥åŸºäºä¸Šè¿°ç¤ºä¾‹å¿«é€Ÿå®ç°ã€‚æ ¸å¿ƒæ¶æ„å·²å®Œæˆï¼Œå‰©ä½™å·¥ä½œä¸»è¦æ˜¯ç»„è£…å’Œé›†æˆã€‚
