#!/usr/bin/env python3
"""
Phase 3 AIæ¨¡å‹æµ‹è¯•è„šæœ¬
æµ‹è¯•LightGBMã€GRUæ¨¡å‹ã€è¯„ä¼°å™¨å’Œè®­ç»ƒå™¨åŠŸèƒ½
"""

import sys
import os
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.models.lightgbm_model import LightGBMStockModel
from src.models.model_evaluator import ModelEvaluator
from src.models.model_trainer import ModelTrainer

import pandas as pd
import numpy as np
from typing import Tuple


def create_test_data(n_samples: int = 1000, n_features: int = 20) -> Tuple:
    """åˆ›å»ºæµ‹è¯•æ•°æ®"""
    np.random.seed(42)

    dates = pd.date_range('2020-01-01', periods=n_samples, freq='D')

    # æ¨¡æ‹Ÿç‰¹å¾
    features = {}
    for i in range(n_features):
        features[f'feature_{i}'] = np.random.randn(n_samples)

    # æ¨¡æ‹Ÿç›®æ ‡ï¼ˆå¸¦çœŸå®ä¿¡å·çš„æ”¶ç›Šç‡ï¼‰
    true_signal = (
        features['feature_0'] * 0.5 +
        features['feature_1'] * 0.3 +
        features['feature_2'] * -0.2
    )
    target = true_signal * 0.02 + np.random.randn(n_samples) * 0.01

    df = pd.DataFrame(features, index=dates)
    df['target'] = target

    return df, [f'feature_{i}' for i in range(n_features)]


def test_lightgbm_model():
    """æµ‹è¯•LightGBMæ¨¡å‹"""
    print("\n" + "="*60)
    print("æµ‹è¯•1: LightGBMæ¨¡å‹")
    print("="*60)

    # åˆ›å»ºæ•°æ®
    df, feature_cols = create_test_data(1000, 20)

    print(f"\n1.1 æ•°æ®å‡†å¤‡:")
    print(f"  æ ·æœ¬æ•°: {len(df)}")
    print(f"  ç‰¹å¾æ•°: {len(feature_cols)}")

    # åˆ†å‰²æ•°æ®
    split_idx = int(len(df) * 0.8)
    train_df = df[:split_idx]
    test_df = df[split_idx:]

    X_train, y_train = train_df[feature_cols], train_df['target']
    X_test, y_test = test_df[feature_cols], test_df['target']

    print(f"  è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬")
    print(f"  æµ‹è¯•é›†: {len(X_test)} æ ·æœ¬")

    # è®­ç»ƒæ¨¡å‹
    print("\n1.2 è®­ç»ƒæ¨¡å‹")
    model = LightGBMStockModel(
        objective='regression',
        learning_rate=0.1,
        n_estimators=100,
        num_leaves=31,
        verbose=-1
    )

    history = model.train(
        X_train, y_train,
        X_test, y_test,
        early_stopping_rounds=10,
        verbose_eval=0
    )

    print(f"  âœ“ è®­ç»ƒå®Œæˆï¼Œæœ€ä½³è¿­ä»£: {history['best_iteration']}")

    # é¢„æµ‹
    print("\n1.3 é¢„æµ‹")
    y_pred_train = model.predict(X_train)
    y_pred_test = model.predict(X_test)

    print(f"  è®­ç»ƒé›†é¢„æµ‹æ•°é‡: {len(y_pred_train)}")
    print(f"  æµ‹è¯•é›†é¢„æµ‹æ•°é‡: {len(y_pred_test)}")

    # è®¡ç®—æŒ‡æ ‡
    from sklearn.metrics import mean_squared_error, r2_score

    train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))
    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))
    train_r2 = r2_score(y_train, y_pred_train)
    test_r2 = r2_score(y_test, y_pred_test)

    print(f"\n1.4 æ€§èƒ½æŒ‡æ ‡:")
    print(f"  è®­ç»ƒé›† RMSE: {train_rmse:.6f}, RÂ²: {train_r2:.4f}")
    print(f"  æµ‹è¯•é›† RMSE: {test_rmse:.6f}, RÂ²: {test_r2:.4f}")

    # ç‰¹å¾é‡è¦æ€§
    print("\n1.5 ç‰¹å¾é‡è¦æ€§ (Top 5):")
    importance_df = model.get_feature_importance('gain', top_n=5)
    print(importance_df)

    # ä¿å­˜å’ŒåŠ è½½
    print("\n1.6 ä¿å­˜å’ŒåŠ è½½æ¨¡å‹")
    save_path = project_root / 'data' / 'test_models' / 'test_lgb.txt'
    save_path.parent.mkdir(parents=True, exist_ok=True)
    model.save_model(str(save_path))

    new_model = LightGBMStockModel()
    new_model.load_model(str(save_path))

    y_pred_new = new_model.predict(X_test)
    assert np.allclose(y_pred_test, y_pred_new), "åŠ è½½åé¢„æµ‹ä¸ä¸€è‡´"
    print("  âœ“ æ¨¡å‹ä¿å­˜å’ŒåŠ è½½æˆåŠŸ")

    print("\nâœ… æµ‹è¯•1é€šè¿‡")
    return model, X_test, y_test


def test_model_evaluator():
    """æµ‹è¯•æ¨¡å‹è¯„ä¼°å™¨"""
    print("\n" + "="*60)
    print("æµ‹è¯•2: æ¨¡å‹è¯„ä¼°å™¨")
    print("="*60)

    # åˆ›å»ºæµ‹è¯•æ•°æ®
    np.random.seed(42)
    n_samples = 1000

    # æ¨¡æ‹Ÿé¢„æµ‹å€¼å’Œå®é™…æ”¶ç›Šç‡ï¼ˆæœ‰ç›¸å…³æ€§ï¼‰
    true_signal = np.random.randn(n_samples)
    predictions = true_signal + np.random.randn(n_samples) * 0.5
    actual_returns = true_signal * 0.02 + np.random.randn(n_samples) * 0.01

    print(f"\n2.1 æ•°æ®å‡†å¤‡:")
    print(f"  æ ·æœ¬æ•°: {n_samples}")

    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = ModelEvaluator()

    # æµ‹è¯•ICè®¡ç®—
    print("\n2.2 ICæŒ‡æ ‡:")
    ic = evaluator.calculate_ic(predictions, actual_returns, method='pearson')
    rank_ic = evaluator.calculate_rank_ic(predictions, actual_returns)

    print(f"  IC: {ic:.4f}")
    print(f"  Rank IC: {rank_ic:.4f}")

    assert -1 <= ic <= 1, "ICå€¼è¶…å‡ºèŒƒå›´"
    assert -1 <= rank_ic <= 1, "Rank ICå€¼è¶…å‡ºèŒƒå›´"

    # æµ‹è¯•åˆ†ç»„æ”¶ç›Š
    print("\n2.3 åˆ†ç»„æ”¶ç›Š:")
    group_returns = evaluator.calculate_group_returns(predictions, actual_returns, n_groups=5)
    for group, ret in sorted(group_returns.items()):
        print(f"  Group {group}: {ret:.6f}")

    # æµ‹è¯•å¤šç©ºæ”¶ç›Š
    print("\n2.4 å¤šç©ºæ”¶ç›Š:")
    long_short = evaluator.calculate_long_short_return(predictions, actual_returns)
    print(f"  Long: {long_short['long']:.6f}")
    print(f"  Short: {long_short['short']:.6f}")
    print(f"  Long-Short: {long_short['long_short']:.6f}")

    # æµ‹è¯•Sharpeæ¯”ç‡
    print("\n2.5 Sharpeæ¯”ç‡:")
    sharpe = evaluator.calculate_sharpe_ratio(actual_returns)
    print(f"  Sharpe Ratio: {sharpe:.4f}")

    # æµ‹è¯•æœ€å¤§å›æ’¤
    print("\n2.6 æœ€å¤§å›æ’¤:")
    max_dd = evaluator.calculate_max_drawdown(actual_returns)
    print(f"  Max Drawdown: {max_dd:.4%}")

    # æµ‹è¯•èƒœç‡
    print("\n2.7 èƒœç‡:")
    win_rate = evaluator.calculate_win_rate(actual_returns)
    print(f"  Win Rate: {win_rate:.4%}")

    # å…¨é¢è¯„ä¼°
    print("\n2.8 å…¨é¢è¯„ä¼°:")
    metrics = evaluator.evaluate_regression(predictions, actual_returns, verbose=False)

    print(f"  è¯„ä¼°æŒ‡æ ‡æ•°é‡: {len(metrics)}")
    assert 'ic' in metrics, "ç¼ºå°‘ICæŒ‡æ ‡"
    assert 'rank_ic' in metrics, "ç¼ºå°‘Rank ICæŒ‡æ ‡"
    assert 'long_short_return' in metrics, "ç¼ºå°‘å¤šç©ºæ”¶ç›ŠæŒ‡æ ‡"

    print("\nâœ… æµ‹è¯•2é€šè¿‡")
    return evaluator


def test_model_trainer():
    """æµ‹è¯•æ¨¡å‹è®­ç»ƒå™¨"""
    print("\n" + "="*60)
    print("æµ‹è¯•3: æ¨¡å‹è®­ç»ƒå™¨")
    print("="*60)

    # åˆ›å»ºæ•°æ®
    df, feature_cols = create_test_data(1000, 20)

    print(f"\n3.1 æ•°æ®å‡†å¤‡:")
    print(f"  æ ·æœ¬æ•°: {len(df)}")
    print(f"  ç‰¹å¾æ•°: {len(feature_cols)}")

    # åˆ›å»ºè®­ç»ƒå™¨
    print("\n3.2 åˆ›å»ºLightGBMè®­ç»ƒå™¨")
    trainer = ModelTrainer(
        model_type='lightgbm',
        model_params={
            'learning_rate': 0.1,
            'n_estimators': 100,
            'num_leaves': 31
        },
        output_dir=str(project_root / 'data' / 'test_models')
    )

    # å‡†å¤‡æ•°æ®
    print("\n3.3 æ•°æ®åˆ†å‰²")
    X_train, y_train, X_valid, y_valid, X_test, y_test = trainer.prepare_data(
        df, feature_cols, 'target',
        train_ratio=0.7,
        valid_ratio=0.15
    )

    # è®­ç»ƒ
    print("\n3.4 è®­ç»ƒæ¨¡å‹")
    trainer.train(X_train, y_train, X_valid, y_valid, verbose_eval=0)
    print("  âœ“ è®­ç»ƒå®Œæˆ")

    # è¯„ä¼°
    print("\n3.5 è¯„ä¼°æ¨¡å‹")
    test_metrics = trainer.evaluate(X_test, y_test, dataset_name='test', verbose=False)

    print(f"  æµ‹è¯•é›†æŒ‡æ ‡:")
    print(f"    RMSE: {test_metrics['rmse']:.6f}")
    print(f"    RÂ²: {test_metrics['r2']:.4f}")
    print(f"    IC: {test_metrics['ic']:.4f}")
    print(f"    Rank IC: {test_metrics['rank_ic']:.4f}")
    print(f"    Long-Short Return: {test_metrics['long_short_return']:.6f}")

    # ä¿å­˜æ¨¡å‹
    print("\n3.6 ä¿å­˜æ¨¡å‹")
    trainer.save_model('trainer_test_model')
    print("  âœ“ æ¨¡å‹å·²ä¿å­˜")

    # åŠ è½½æ¨¡å‹
    print("\n3.7 åŠ è½½æ¨¡å‹")
    new_trainer = ModelTrainer(
        output_dir=str(project_root / 'data' / 'test_models')
    )
    new_trainer.load_model('trainer_test_model')
    print("  âœ“ æ¨¡å‹å·²åŠ è½½")

    # éªŒè¯åŠ è½½åçš„æ¨¡å‹
    new_metrics = new_trainer.evaluate(X_test, y_test, dataset_name='test', verbose=False)
    assert abs(new_metrics['rmse'] - test_metrics['rmse']) < 1e-6, "åŠ è½½åRMSEä¸ä¸€è‡´"
    print("  âœ“ åŠ è½½åæ¨¡å‹é¢„æµ‹ä¸€è‡´")

    print("\nâœ… æµ‹è¯•3é€šè¿‡")
    return trainer


def test_integrated_workflow():
    """æµ‹è¯•å®Œæ•´å·¥ä½œæµ"""
    print("\n" + "="*60)
    print("æµ‹è¯•4: å®Œæ•´å·¥ä½œæµ")
    print("="*60)

    # åˆ›å»ºæ•°æ®
    df, feature_cols = create_test_data(1000, 20)

    print(f"\n4.1 æ•°æ®å‡†å¤‡:")
    print(f"  æ ·æœ¬æ•°: {len(df)}")

    # ä½¿ç”¨ä¾¿æ·å‡½æ•°è®­ç»ƒæ¨¡å‹
    print("\n4.2 ä½¿ç”¨ä¾¿æ·å‡½æ•°è®­ç»ƒæ¨¡å‹")
    from src.models.model_trainer import train_stock_model

    trainer, test_metrics = train_stock_model(
        df=df,
        feature_cols=feature_cols,
        target_col='target',
        model_type='lightgbm',
        model_params={
            'learning_rate': 0.1,
            'n_estimators': 50
        },
        train_ratio=0.7,
        valid_ratio=0.15,
        save_path=None
    )

    print(f"\n4.3 æ¨¡å‹æ€§èƒ½:")
    print(f"  RMSE: {test_metrics['rmse']:.6f}")
    print(f"  IC: {test_metrics['ic']:.4f}")
    print(f"  Rank IC: {test_metrics['rank_ic']:.4f}")
    print(f"  Long-Short Return: {test_metrics['long_short_return']:.6f}")

    # éªŒè¯å…³é”®æŒ‡æ ‡
    assert test_metrics['rmse'] < 0.1, "RMSEè¿‡é«˜"
    assert abs(test_metrics['ic']) > 0.1, "ICç›¸å…³æ€§è¿‡ä½"

    print("\nâœ… æµ‹è¯•4é€šè¿‡")


def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("\n" + "ğŸ¤–"*30)
    print("Phase 3: AIæ¨¡å‹æµ‹è¯•")
    print("ğŸ¤–"*30)

    try:
        # å¯¼å…¥å¿…è¦çš„ç±»å‹
        from typing import Tuple

        # è¿è¡Œå„é¡¹æµ‹è¯•
        test_lightgbm_model()
        test_model_evaluator()
        test_model_trainer()
        test_integrated_workflow()

        print("\n" + "="*60)
        print("âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Phase 3 AIæ¨¡å‹è¿è¡Œæ­£å¸¸")
        print("="*60 + "\n")

        return 0

    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())
