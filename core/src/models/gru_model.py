"""
GRUæ—¶åºæ¨¡å‹ï¼ˆæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼‰
ç”¨äºè‚¡ç¥¨æ—¶åºæ•°æ®çš„æ”¶ç›Šç‡é¢„æµ‹
"""

import pandas as pd
import numpy as np
from typing import Optional, Dict, List, Tuple
import warnings
import pickle
from pathlib import Path
from loguru import logger

warnings.filterwarnings('ignore')

try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    from torch.utils.data import Dataset, DataLoader
    PYTORCH_AVAILABLE = True
except ImportError:
    PYTORCH_AVAILABLE = False
    logger.warning("è­¦å‘Š: PyTorchæœªå®‰è£…ï¼ŒGRUæ¨¡å‹ä¸å¯ç”¨")


if PYTORCH_AVAILABLE:
    class StockSequenceDataset(Dataset):
        """è‚¡ç¥¨æ—¶åºæ•°æ®é›†"""

        def __init__(
            self,
            sequences: np.ndarray,
            targets: np.ndarray
        ):
            """
            åˆå§‹åŒ–æ•°æ®é›†

            å‚æ•°:
                sequences: (N, T, F) - Nä¸ªæ ·æœ¬ï¼ŒTä¸ªæ—¶é—´æ­¥ï¼ŒFä¸ªç‰¹å¾
                targets: (N,) - Nä¸ªç›®æ ‡å€¼
            """
            # ä½¿ç”¨torch.from_numpyé¿å…åœ¨macOSä¸Šçš„æ®µé”™è¯¯
            # ç¡®ä¿è¾“å…¥æ˜¯float32ç±»å‹çš„numpyæ•°ç»„
            sequences = np.asarray(sequences, dtype=np.float32)
            targets = np.asarray(targets, dtype=np.float32)

            self.sequences = torch.from_numpy(sequences).float()
            self.targets = torch.from_numpy(targets).float()

        def __len__(self):
            return len(self.sequences)

        def __getitem__(self, idx):
            return self.sequences[idx], self.targets[idx]


    class GRUStockModel(nn.Module):
        """GRUè‚¡ç¥¨é¢„æµ‹æ¨¡å‹"""

        def __init__(
            self,
            input_size: int,
            hidden_size: int = 64,
            num_layers: int = 2,
            dropout: float = 0.2,
            bidirectional: bool = False
        ):
            """
            åˆå§‹åŒ–GRUæ¨¡å‹

            å‚æ•°:
                input_size: è¾“å…¥ç‰¹å¾ç»´åº¦
                hidden_size: éšè—å±‚ç»´åº¦
                num_layers: GRUå±‚æ•°
                dropout: Dropoutæ¯”ä¾‹
                bidirectional: æ˜¯å¦åŒå‘GRU
            """
            super(GRUStockModel, self).__init__()

            self.input_size = input_size
            self.hidden_size = hidden_size
            self.num_layers = num_layers
            self.bidirectional = bidirectional

            # GRUå±‚
            self.gru = nn.GRU(
                input_size=input_size,
                hidden_size=hidden_size,
                num_layers=num_layers,
                dropout=dropout if num_layers > 1 else 0,
                batch_first=True,
                bidirectional=bidirectional
            )

            # å…¨è¿æ¥å±‚
            fc_input_size = hidden_size * 2 if bidirectional else hidden_size
            self.fc = nn.Sequential(
                nn.Linear(fc_input_size, hidden_size),
                nn.ReLU(),
                nn.Dropout(dropout),
                nn.Linear(hidden_size, 1)
            )

        def forward(self, x):
            """
            å‰å‘ä¼ æ’­

            å‚æ•°:
                x: (batch_size, seq_len, input_size)

            è¿”å›:
                é¢„æµ‹å€¼: (batch_size, 1)
            """
            # GRUè¾“å‡º
            # output: (batch_size, seq_len, hidden_size * num_directions)
            # hidden: (num_layers * num_directions, batch_size, hidden_size)
            output, hidden = self.gru(x)

            # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
            if self.bidirectional:
                # æ‹¼æ¥å‰å‘å’Œåå‘çš„æœ€åéšè—çŠ¶æ€
                hidden = torch.cat((hidden[-2], hidden[-1]), dim=1)
            else:
                hidden = hidden[-1]

            # å…¨è¿æ¥å±‚
            out = self.fc(hidden)

            return out.squeeze(-1)


class GRUStockTrainer:
    """GRUæ¨¡å‹è®­ç»ƒå™¨ï¼ˆæ”¯æŒGPUåŠ é€Ÿï¼‰"""

    def __init__(
        self,
        input_size: int,
        hidden_size: int = 64,
        num_layers: int = 2,
        dropout: float = 0.2,
        bidirectional: bool = False,
        learning_rate: float = 0.001,
        device: str = None,
        use_gpu: bool = True,
        batch_size: int = None,
        num_workers: int = 4
    ):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨ï¼ˆæ”¯æŒGPUåŠ é€Ÿï¼‰

        å‚æ•°:
            input_size: è¾“å…¥ç‰¹å¾ç»´åº¦
            hidden_size: éšè—å±‚ç»´åº¦
            num_layers: GRUå±‚æ•°
            dropout: Dropoutæ¯”ä¾‹
            bidirectional: æ˜¯å¦åŒå‘
            learning_rate: å­¦ä¹ ç‡
            device: è®¾å¤‡ ('cpu', 'cuda', 'mps'ï¼ŒNoneè¡¨ç¤ºè‡ªåŠ¨é€‰æ‹©)
            use_gpu: æ˜¯å¦ä¼˜å…ˆä½¿ç”¨GPUï¼ˆé»˜è®¤Trueï¼‰
            batch_size: æ‰¹æ¬¡å¤§å°ï¼ˆNoneè¡¨ç¤ºè‡ªåŠ¨è®¡ç®—ï¼‰
            num_workers: DataLoaderå·¥ä½œè¿›ç¨‹æ•°ï¼ˆé»˜è®¤4ï¼‰
        """
        if not PYTORCH_AVAILABLE:
            raise ImportError("éœ€è¦å®‰è£…PyTorch: pip install torch")

        # macOSä¸Šä½¿ç”¨å¤šè¿›ç¨‹DataLoaderå¯èƒ½å¯¼è‡´æ®µé”™è¯¯ï¼Œå¼ºåˆ¶è®¾ä¸º0
        import platform
        if platform.system() == 'Darwin' and num_workers > 0:
            logger.warning(f"æ£€æµ‹åˆ°macOSç³»ç»Ÿï¼Œå°†num_workersä»{num_workers}è®¾ä¸º0ä»¥é¿å…å¤šè¿›ç¨‹é—®é¢˜")
            num_workers = 0

        # å°è¯•å¯¼å…¥GPUç®¡ç†å™¨
        try:
            from src.utils.gpu_utils import gpu_manager
            self.gpu_manager = gpu_manager
        except ImportError:
            self.gpu_manager = None
            logger.warning("GPUç®¡ç†å™¨æœªå®‰è£…")

        # è®¾å¤‡é€‰æ‹©ï¼ˆä¼˜å…ˆä½¿ç”¨GPUç®¡ç†å™¨ï¼‰
        if device is None:
            if self.gpu_manager is not None:
                device = self.gpu_manager.get_device(prefer_gpu=use_gpu)
            elif use_gpu and torch.cuda.is_available():
                device = 'cuda'
            elif use_gpu and torch.backends.mps.is_available():
                # MPSåœ¨RNNè®­ç»ƒä¸­æ•°å€¼ä¸ç¨³å®šï¼Œå»ºè®®ä½¿ç”¨CPU
                logger.warning("æ£€æµ‹åˆ°MPSè®¾å¤‡ï¼Œä½†GRU/RNNåœ¨MPSä¸Šå¯èƒ½æ•°å€¼ä¸ç¨³å®š")
                logger.warning("å»ºè®®ä½¿ç”¨use_gpu=Falseå¼ºåˆ¶ä½¿ç”¨CPUï¼Œæˆ–ç­‰å¾…PyTorch MPSä¼˜åŒ–")
                device = 'mps'
            else:
                device = 'cpu'

        self.device = torch.device(device)
        logger.info(f"ğŸš€ GRUæ¨¡å‹ä½¿ç”¨è®¾å¤‡: {self.device}")

        # åˆ›å»ºæ¨¡å‹å¹¶ç§»åˆ°è®¾å¤‡
        self.model = GRUStockModel(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            dropout=dropout,
            bidirectional=bidirectional
        ).to(self.device)

        # è‡ªåŠ¨è®¡ç®—æ‰¹æ¬¡å¤§å°
        if batch_size is None and 'cuda' in str(self.device) and self.gpu_manager is not None:
            # ä¼°ç®—æ¨¡å‹å¤§å°
            model_size_mb = sum(
                p.numel() * p.element_size()
                for p in self.model.parameters()
            ) / (1024 ** 2)

            # ä¼°ç®—æ ·æœ¬å¤§å°ï¼ˆå‡è®¾åºåˆ—é•¿åº¦20ï¼‰
            sample_size_mb = (input_size * 20 * 4) / (1024 ** 2)

            self.batch_size = self.gpu_manager.get_optimal_batch_size(
                model_size_mb, sample_size_mb
            )
            logger.info(f"è‡ªåŠ¨è®¾ç½®æ‰¹æ¬¡å¤§å°: {self.batch_size}")
        else:
            self.batch_size = batch_size or 64

        self.num_workers = num_workers

        # ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
        # PyTorch 2.10åœ¨macOSä¸ŠAdamä¼˜åŒ–å™¨å­˜åœ¨æ®µé”™è¯¯é—®é¢˜ï¼Œä½¿ç”¨SGD with momentum
        import platform
        if platform.system() == 'Darwin':
            logger.warning("æ£€æµ‹åˆ°macOSç³»ç»Ÿï¼Œä½¿ç”¨SGDä¼˜åŒ–å™¨ä»£æ›¿Adamé¿å…æ®µé”™è¯¯")
            self.optimizer = optim.SGD(
                self.model.parameters(),
                lr=learning_rate,
                momentum=0.9
            )
        else:
            self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)

        self.criterion = nn.MSELoss()

        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer,
            mode='min',
            factor=0.5,
            patience=5
        )

        # æ··åˆç²¾åº¦è®­ç»ƒï¼ˆé’ˆå¯¹è¾ƒæ–°çš„GPUï¼‰
        self.use_amp = 'cuda' in str(self.device) and torch.cuda.get_device_capability()[0] >= 7
        self.scaler = torch.cuda.amp.GradScaler() if self.use_amp else None

        if self.use_amp:
            logger.info("âœ¨ å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼ˆAMPï¼‰")

        # è®­ç»ƒå†å²
        self.history = {
            'train_loss': [],
            'valid_loss': []
        }

    def create_sequences(
        self,
        data: pd.DataFrame,
        target: pd.Series,
        seq_length: int = 20
    ) -> Tuple[np.ndarray, np.ndarray]:
        """
        åˆ›å»ºæ—¶åºåºåˆ—

        å‚æ•°:
            data: ç‰¹å¾DataFrame
            target: ç›®æ ‡Series
            seq_length: åºåˆ—é•¿åº¦

        è¿”å›:
            (sequences, targets)
        """
        sequences = []
        targets = []

        data_array = data.values
        target_array = target.values

        for i in range(len(data) - seq_length):
            seq = data_array[i:i + seq_length]
            tgt = target_array[i + seq_length]

            sequences.append(seq)
            targets.append(tgt)

        return np.array(sequences), np.array(targets)

    def train_epoch(
        self,
        train_loader: 'DataLoader'
    ) -> float:
        """è®­ç»ƒä¸€ä¸ªepochï¼ˆGPUä¼˜åŒ–ç‰ˆï¼‰"""
        self.model.train()
        total_loss = 0
        num_batches = 0

        for sequences, targets in train_loader:
            sequences = sequences.to(self.device, non_blocking=True)
            targets = targets.to(self.device, non_blocking=True)

            self.optimizer.zero_grad(set_to_none=True)  # æ›´é«˜æ•ˆçš„æ¢¯åº¦æ¸…é›¶

            if self.use_amp:
                # æ··åˆç²¾åº¦è®­ç»ƒ
                with torch.cuda.amp.autocast():
                    predictions = self.model(sequences)
                    loss = self.criterion(predictions, targets)

                self.scaler.scale(loss).backward()
                self.scaler.step(self.optimizer)
                self.scaler.update()
            else:
                # æ ‡å‡†è®­ç»ƒ
                predictions = self.model(sequences)
                loss = self.criterion(predictions, targets)

                # æ£€æŸ¥lossæ˜¯å¦æœ‰æ•ˆ
                if not torch.isfinite(loss):
                    logger.warning(f"æ£€æµ‹åˆ°æ— æ•ˆlosså€¼: {loss.item()}ï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡")
                    continue

                loss.backward()
                self.optimizer.step()

            total_loss += loss.item()
            num_batches += 1

        # é˜²æ­¢é™¤é›¶é”™è¯¯
        return total_loss / num_batches if num_batches > 0 else 0.0

    def validate(
        self,
        valid_loader: 'DataLoader'
    ) -> float:
        """éªŒè¯ï¼ˆGPUä¼˜åŒ–ç‰ˆï¼‰"""
        self.model.eval()
        total_loss = 0
        num_batches = 0

        with torch.no_grad():
            for sequences, targets in valid_loader:
                sequences = sequences.to(self.device, non_blocking=True)
                targets = targets.to(self.device, non_blocking=True)

                predictions = self.model(sequences)
                loss = self.criterion(predictions, targets)

                total_loss += loss.item()
                num_batches += 1

        # é˜²æ­¢é™¤é›¶é”™è¯¯
        return total_loss / num_batches if num_batches > 0 else 0.0

    def train(
        self,
        X_train: pd.DataFrame,
        y_train: pd.Series,
        X_valid: Optional[pd.DataFrame] = None,
        y_valid: Optional[pd.Series] = None,
        seq_length: int = 20,
        batch_size: int = None,
        epochs: int = 100,
        early_stopping_patience: int = 10,
        verbose: int = 10
    ) -> Dict:
        """
        è®­ç»ƒæ¨¡å‹ï¼ˆGPUä¼˜åŒ–ç‰ˆï¼‰

        å‚æ•°:
            X_train: è®­ç»ƒç‰¹å¾
            y_train: è®­ç»ƒæ ‡ç­¾
            X_valid: éªŒè¯ç‰¹å¾
            y_valid: éªŒè¯æ ‡ç­¾
            seq_length: åºåˆ—é•¿åº¦
            batch_size: æ‰¹æ¬¡å¤§å°ï¼ˆNoneè¡¨ç¤ºä½¿ç”¨åˆå§‹åŒ–æ—¶çš„è‡ªåŠ¨æ‰¹æ¬¡ï¼‰
            epochs: è®­ç»ƒè½®æ•°
            early_stopping_patience: æ—©åœè€å¿ƒå€¼
            verbose: è¾“å‡ºé—´éš”

        è¿”å›:
            è®­ç»ƒå†å²
        """
        # ä½¿ç”¨è‡ªåŠ¨æ‰¹æ¬¡å¤§å°æˆ–æŒ‡å®šæ‰¹æ¬¡
        batch_size = batch_size or self.batch_size

        logger.info(f"\nå¼€å§‹è®­ç»ƒGRUæ¨¡å‹...")
        logger.info(f"åºåˆ—é•¿åº¦: {seq_length}, æ‰¹æ¬¡å¤§å°: {batch_size}, è®­ç»ƒè½®æ•°: {epochs}")

        # åˆ›å»ºåºåˆ—
        logger.info("\nåˆ›å»ºè®­ç»ƒåºåˆ—...")
        X_train_seq, y_train_seq = self.create_sequences(X_train, y_train, seq_length)
        logger.info(f"è®­ç»ƒåºåˆ—: {X_train_seq.shape}")

        # åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼ˆGPUä¼˜åŒ–ï¼‰
        # macOSä¸Špin_memoryå¯èƒ½å¯¼è‡´æ®µé”™è¯¯ï¼Œä»…åœ¨CUDAè®¾å¤‡ä¸Šå¯ç”¨
        use_pin_memory = ('cuda' in str(self.device) and torch.cuda.is_available())

        train_dataset = StockSequenceDataset(X_train_seq, y_train_seq)
        train_loader = DataLoader(
            train_dataset,
            batch_size=batch_size,
            shuffle=True,
            num_workers=self.num_workers,
            pin_memory=use_pin_memory
        )

        # éªŒè¯é›†
        valid_loader = None
        if X_valid is not None and y_valid is not None:
            logger.info("åˆ›å»ºéªŒè¯åºåˆ—...")
            X_valid_seq, y_valid_seq = self.create_sequences(X_valid, y_valid, seq_length)
            logger.info(f"éªŒè¯åºåˆ—: {X_valid_seq.shape}")

            valid_dataset = StockSequenceDataset(X_valid_seq, y_valid_seq)
            valid_loader = DataLoader(
                valid_dataset,
                batch_size=batch_size,
                shuffle=False,
                num_workers=self.num_workers,
                pin_memory=use_pin_memory
            )

        # è®­ç»ƒå¾ªç¯
        best_valid_loss = float('inf')
        patience_counter = 0

        for epoch in range(epochs):
            train_loss = self.train_epoch(train_loader)
            self.history['train_loss'].append(train_loss)

            # éªŒè¯
            if valid_loader is not None:
                valid_loss = self.validate(valid_loader)
                self.history['valid_loss'].append(valid_loss)

                # å­¦ä¹ ç‡è°ƒæ•´
                self.scheduler.step(valid_loss)

                # æ—©åœ
                if valid_loss < best_valid_loss:
                    best_valid_loss = valid_loss
                    patience_counter = 0
                else:
                    patience_counter += 1

                if patience_counter >= early_stopping_patience:
                    logger.info(f"\nEarly stopping at epoch {epoch + 1}")
                    break

                # è¾“å‡º
                if verbose > 0 and (epoch + 1) % verbose == 0:
                    logger.info(f"Epoch {epoch + 1}/{epochs} - "
                          f"Train Loss: {train_loss:.6f}, "
                          f"Valid Loss: {valid_loss:.6f}")
            else:
                if verbose > 0 and (epoch + 1) % verbose == 0:
                    logger.info(f"Epoch {epoch + 1}/{epochs} - Train Loss: {train_loss:.6f}")

            # å®šæœŸæ¸…ç†GPUç¼“å­˜
            if 'cuda' in str(self.device) and (epoch + 1) % 20 == 0 and self.gpu_manager is not None:
                self.gpu_manager.clear_cache()

        logger.success(f"\nâœ“ è®­ç»ƒå®Œæˆ")

        return self.history

    def predict(
        self,
        X: pd.DataFrame,
        seq_length: int = 20,
        batch_size: int = None
    ) -> np.ndarray:
        """
        é¢„æµ‹ï¼ˆGPUä¼˜åŒ–ç‰ˆï¼‰

        å‚æ•°:
            X: ç‰¹å¾DataFrame
            seq_length: åºåˆ—é•¿åº¦
            batch_size: æ‰¹æ¬¡å¤§å°ï¼ˆNoneè¡¨ç¤ºä½¿ç”¨è‡ªåŠ¨æ‰¹æ¬¡çš„2å€ï¼‰

        è¿”å›:
            é¢„æµ‹å€¼æ•°ç»„
        """
        self.model.eval()

        # æ¨ç†å¯ç”¨æ›´å¤§æ‰¹æ¬¡
        batch_size = batch_size or (self.batch_size * 2)

        # åˆ›å»ºåºåˆ—ï¼ˆä½¿ç”¨0ä½œä¸ºå ä½ç¬¦ç›®æ ‡ï¼‰
        sequences, _ = self.create_sequences(
            X,
            pd.Series(np.zeros(len(X))),
            seq_length
        )

        # åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼ˆGPUä¼˜åŒ–ï¼‰
        # macOSä¸Špin_memoryå¯èƒ½å¯¼è‡´æ®µé”™è¯¯ï¼Œä»…åœ¨CUDAè®¾å¤‡ä¸Šå¯ç”¨
        use_pin_memory = ('cuda' in str(self.device) and torch.cuda.is_available())

        dataset = StockSequenceDataset(sequences, np.zeros(len(sequences)))
        loader = DataLoader(
            dataset,
            batch_size=batch_size,
            shuffle=False,
            num_workers=self.num_workers,
            pin_memory=use_pin_memory
        )

        # é¢„æµ‹
        predictions = []
        with torch.no_grad():
            for sequences, _ in loader:
                sequences = sequences.to(self.device, non_blocking=True)
                preds = self.model(sequences)
                predictions.extend(preds.cpu().numpy())

        return np.array(predictions)

    def save_model(
        self,
        model_path: str
    ):
        """ä¿å­˜æ¨¡å‹"""
        model_path = Path(model_path)
        model_path.parent.mkdir(parents=True, exist_ok=True)

        # ä¿å­˜æ¨¡å‹æƒé‡
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'model_config': {
                'input_size': self.model.input_size,
                'hidden_size': self.model.hidden_size,
                'num_layers': self.model.num_layers,
                'bidirectional': self.model.bidirectional
            },
            'history': self.history
        }, model_path)

        logger.success(f"âœ“ æ¨¡å‹å·²ä¿å­˜è‡³: {model_path}")

    def load_model(
        self,
        model_path: str
    ):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(model_path, map_location=self.device)

        # é‡å»ºæ¨¡å‹
        config = checkpoint['model_config']
        self.model = GRUStockModel(
            input_size=config['input_size'],
            hidden_size=config['hidden_size'],
            num_layers=config['num_layers'],
            bidirectional=config['bidirectional']
        ).to(self.device)

        # åŠ è½½æƒé‡
        self.model.load_state_dict(checkpoint['model_state_dict'])

        # é‡æ–°åˆ›å»ºoptimizerä»¥åŒ¹é…æ–°æ¨¡å‹
        self.optimizer = optim.Adam(self.model.parameters())
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

        self.history = checkpoint.get('history', {'train_loss': [], 'valid_loss': []})

        logger.success(f"âœ“ æ¨¡å‹å·²åŠ è½½: {model_path}")


# ==================== ä½¿ç”¨ç¤ºä¾‹ ====================

if __name__ == "__main__":
    if not PYTORCH_AVAILABLE:
        logger.info("PyTorchæœªå®‰è£…ï¼Œæ— æ³•è¿è¡Œæµ‹è¯•")
        exit(1)

    logger.info("GRUæ¨¡å‹æµ‹è¯•\n")

    # åˆ›å»ºæµ‹è¯•æ•°æ®
    np.random.seed(42)
    n_samples = 1000
    n_features = 10

    # æ¨¡æ‹Ÿæ—¶åºæ•°æ®
    X = pd.DataFrame(
        np.random.randn(n_samples, n_features),
        columns=[f'feature_{i}' for i in range(n_features)]
    )

    # æ¨¡æ‹Ÿç›®æ ‡ï¼ˆä¸‹ä¸€æœŸæ”¶ç›Šç‡ï¼‰
    y = pd.Series(np.random.randn(n_samples) * 0.02)

    # åˆ†å‰²è®­ç»ƒé›†å’ŒéªŒè¯é›†
    split_idx = int(n_samples * 0.8)
    X_train, X_valid = X[:split_idx], X[split_idx:]
    y_train, y_valid = y[:split_idx], y[split_idx:]

    logger.info("æ•°æ®å‡†å¤‡:")
    logger.info(f"  è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬")
    logger.info(f"  éªŒè¯é›†: {len(X_valid)} æ ·æœ¬")
    logger.info(f"  ç‰¹å¾æ•°: {len(X.columns)}")

    # è®­ç»ƒæ¨¡å‹
    logger.info("\nè®­ç»ƒGRUæ¨¡å‹:")
    trainer = GRUStockTrainer(
        input_size=n_features,
        hidden_size=32,
        num_layers=2,
        dropout=0.2,
        learning_rate=0.001
    )

    history = trainer.train(
        X_train, y_train,
        X_valid, y_valid,
        seq_length=20,
        batch_size=32,
        epochs=50,
        early_stopping_patience=5,
        verbose=10
    )

    # é¢„æµ‹
    logger.info("\né¢„æµ‹:")
    y_pred_train = trainer.predict(X_train, seq_length=20)
    y_pred_valid = trainer.predict(X_valid, seq_length=20)

    logger.info(f"è®­ç»ƒé›†é¢„æµ‹æ•°é‡: {len(y_pred_train)}")
    logger.info(f"éªŒè¯é›†é¢„æµ‹æ•°é‡: {len(y_pred_valid)}")

    # ä¿å­˜å’ŒåŠ è½½
    logger.info("\nä¿å­˜æ¨¡å‹:")
    trainer.save_model('test_gru_model.pth')

    logger.success("\nâœ“ GRUæ¨¡å‹æµ‹è¯•å®Œæˆ")
