# æ¨¡å‹é›†æˆæ¡†æ¶ä½¿ç”¨æŒ‡å—

## ğŸ“‹ ç›®å½•

- [ç®€ä»‹](#ç®€ä»‹)
- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [ä¸‰ç§é›†æˆæ–¹æ³•](#ä¸‰ç§é›†æˆæ–¹æ³•)
- [API æ–‡æ¡£](#api-æ–‡æ¡£)
- [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)
- [æ€§èƒ½å¯¹æ¯”](#æ€§èƒ½å¯¹æ¯”)

---

## ç®€ä»‹

æ¨¡å‹é›†æˆï¼ˆEnsembleï¼‰æ˜¯æå‡é¢„æµ‹æ€§èƒ½çš„é‡è¦æ‰‹æ®µã€‚æœ¬æ¡†æ¶æä¾›äº†ä¸‰ç§ä¸»æµé›†æˆæ–¹æ³•ï¼š

1. **åŠ æƒå¹³å‡é›†æˆ** (Weighted Average) - ç®€å•æœ‰æ•ˆ
2. **æŠ•ç¥¨æ³•é›†æˆ** (Voting) - é€‚åˆé€‰è‚¡
3. **Stackingé›†æˆ** - æ€§èƒ½æœ€ä¼˜

### ä¸ºä»€ä¹ˆéœ€è¦é›†æˆï¼Ÿ

- âœ… **æå‡æ€§èƒ½**ï¼šé€šå¸¸æ¯”å•æ¨¡å‹æå‡ 5-15% IC
- âœ… **é™ä½æ–¹å·®**ï¼šå‡å°‘å•æ¨¡å‹çš„é¢„æµ‹æ³¢åŠ¨
- âœ… **æ¨¡å‹äº’è¡¥**ï¼šèåˆä¸åŒæ¨¡å‹çš„ä¼˜åŠ¿
- âœ… **é˜²æ­¢è¿‡æ‹Ÿåˆ**ï¼šå¹³æ»‘å•æ¨¡å‹çš„è¿‡æ‹Ÿåˆé£é™©

---

## å¿«é€Ÿå¼€å§‹

### 1. åŸºæœ¬ç”¨æ³•

```python
from models import (
    LightGBMStockModel,
    RidgeStockModel,
    WeightedAverageEnsemble
)

# è®­ç»ƒåŸºç¡€æ¨¡å‹
ridge = RidgeStockModel()
ridge.train(X_train, y_train)

lgb = LightGBMStockModel()
lgb.train(X_train, y_train, X_valid, y_valid)

# åˆ›å»ºé›†æˆ
ensemble = WeightedAverageEnsemble(
    models=[ridge, lgb],
    weights=[0.4, 0.6],
    model_names=['Ridge', 'LightGBM']
)

# é¢„æµ‹
predictions = ensemble.predict(X_test)
```

### 2. ä½¿ç”¨ä¾¿æ·å‡½æ•°

```python
from models import create_ensemble

# ä¸€è¡Œä»£ç åˆ›å»ºé›†æˆ
ensemble = create_ensemble(
    models=[ridge, lgb],
    method='weighted_average',
    weights=[0.4, 0.6]
)
```

---

## ä¸‰ç§é›†æˆæ–¹æ³•

### 1. åŠ æƒå¹³å‡é›†æˆ (Weighted Average)

**åŸç†**ï¼šå¯¹æ‰€æœ‰æ¨¡å‹çš„é¢„æµ‹è¿›è¡ŒåŠ æƒå¹³å‡

```
prediction = w1 * pred1 + w2 * pred2 + ... + wn * predn
```

#### ä½¿ç”¨åœºæ™¯

- æ¨¡å‹é¢„æµ‹å€¼åˆ†å¸ƒç›¸ä¼¼
- éœ€è¦å¿«é€Ÿå®éªŒ
- æ¨¡å‹æ•°é‡è¾ƒå°‘ï¼ˆ2-5ä¸ªï¼‰

#### ä»£ç ç¤ºä¾‹

```python
from models import WeightedAverageEnsemble

# æ–¹æ³•1: ç­‰æƒé‡
ensemble = WeightedAverageEnsemble([model1, model2, model3])

# æ–¹æ³•2: è‡ªå®šä¹‰æƒé‡
ensemble = WeightedAverageEnsemble(
    [model1, model2, model3],
    weights=[0.5, 0.3, 0.2]
)

# æ–¹æ³•3: è‡ªåŠ¨ä¼˜åŒ–æƒé‡
ensemble = WeightedAverageEnsemble([model1, model2, model3])
ensemble.optimize_weights(X_valid, y_valid, metric='ic')

predictions = ensemble.predict(X_test)
```

#### æƒé‡ä¼˜åŒ–

```python
# åœ¨éªŒè¯é›†ä¸Šä¼˜åŒ–æƒé‡
optimized_weights = ensemble.optimize_weights(
    X_valid,
    y_valid,
    metric='ic'  # å¯é€‰: 'ic', 'rank_ic', 'mse'
)

print(f"ä¼˜åŒ–åæƒé‡: {optimized_weights}")
```

#### ä¼˜ç¼ºç‚¹

âœ… **ä¼˜ç‚¹**ï¼š
- ç®€å•æ˜“ç”¨
- è®¡ç®—å¿«é€Ÿ
- å¯¹å¼‚å¸¸é¢„æµ‹æœ‰å¹³æ»‘ä½œç”¨

âŒ **ç¼ºç‚¹**ï¼š
- éœ€è¦æ‰‹åŠ¨è°ƒæ•´æƒé‡ï¼ˆæˆ–ä¼˜åŒ–ï¼‰
- æ— æ³•æ•è·æ¨¡å‹é—´å¤æ‚å…³ç³»

---

### 2. æŠ•ç¥¨æ³•é›†æˆ (Voting)

**åŸç†**ï¼šæ¯ä¸ªæ¨¡å‹å¯¹æ ·æœ¬è¿›è¡Œæ’åºæŠ•ç¥¨ï¼Œç»Ÿè®¡æ€»ç¥¨æ•°

```
score = w1 * rank_score1 + w2 * rank_score2 + ... + wn * rank_scoren
```

#### ä½¿ç”¨åœºæ™¯

- **é€‰è‚¡ç­–ç•¥**ï¼šéœ€è¦é€‰å‡º Top N è‚¡ç¥¨
- æ¨¡å‹é¢„æµ‹å°ºåº¦ä¸ä¸€è‡´
- å…³æ³¨æ’åºè€Œéç»å¯¹å€¼

#### ä»£ç ç¤ºä¾‹

```python
from models import VotingEnsemble

# åˆ›å»ºæŠ•ç¥¨é›†æˆ
ensemble = VotingEnsemble(
    models=[model1, model2, model3],
    model_names=['Ridge', 'LightGBM-1', 'LightGBM-2'],
    voting_weights=[1.0, 1.5, 1.0]  # LightGBM-1 æƒé‡æ›´é«˜
)

# æ–¹æ³•1: è·å–æŠ•ç¥¨åˆ†æ•°
scores = ensemble.predict(X_test)

# æ–¹æ³•2: ç›´æ¥é€‰æ‹© Top N
top_50_indices = ensemble.select_top_n(X_test, top_n=50)

# æ–¹æ³•3: è·å– Top N åŠå…¶åˆ†æ•°
top_indices, top_scores = ensemble.select_top_n(
    X_test,
    top_n=50,
    return_scores=True
)

print(f"é€‰å‡ºçš„è‚¡ç¥¨: {top_indices}")
print(f"æŠ•ç¥¨åˆ†æ•°: {top_scores}")
```

#### åº”ç”¨ï¼šé‡åŒ–é€‰è‚¡

```python
# é€‰è‚¡ç­–ç•¥ç¤ºä¾‹
def select_stocks(X, ensemble, top_n=50):
    """
    ä½¿ç”¨æŠ•ç¥¨æ³•é€‰è‚¡

    Args:
        X: è‚¡ç¥¨ç‰¹å¾æ•°æ®
        ensemble: æŠ•ç¥¨é›†æˆæ¨¡å‹
        top_n: é€‰æ‹©æ•°é‡

    Returns:
        é€‰å‡ºçš„è‚¡ç¥¨ç´¢å¼•
    """
    top_indices = ensemble.select_top_n(X, top_n=top_n)
    return top_indices

# ä½¿ç”¨
selected_stocks = select_stocks(X_test, ensemble, top_n=30)
```

#### ä¼˜ç¼ºç‚¹

âœ… **ä¼˜ç‚¹**ï¼š
- å¯¹é¢„æµ‹å°ºåº¦ä¸æ•æ„Ÿ
- é€‚åˆæ’åºé—®é¢˜
- é™ä½å•æ¨¡å‹é€‰è‚¡åå·®

âŒ **ç¼ºç‚¹**ï¼š
- åªä¿ç•™æ’åºä¿¡æ¯ï¼Œä¸¢å¤±ç»å¯¹å€¼ä¿¡æ¯
- ä¸é€‚åˆå›å½’é¢„æµ‹

---

### 3. Stacking é›†æˆ

**åŸç†**ï¼šä½¿ç”¨å…ƒå­¦ä¹ å™¨ï¼ˆMeta-Learnerï¼‰å­¦ä¹ å¦‚ä½•æœ€ä¼˜ç»„åˆåŸºç¡€æ¨¡å‹

```
ç¬¬ä¸€å±‚: base_pred1, base_pred2, ..., base_predn
ç¬¬äºŒå±‚: meta_learner.predict([base_pred1, base_pred2, ..., base_predn])
```

#### ä½¿ç”¨åœºæ™¯

- æ•°æ®å……è¶³ï¼ˆéœ€è¦é¢å¤–è®­ç»ƒé›†ï¼‰
- è¿½æ±‚æœ€ä¼˜æ€§èƒ½
- æ¨¡å‹é—´å­˜åœ¨å¤æ‚äº’è¡¥å…³ç³»

#### ä»£ç ç¤ºä¾‹

```python
from models import StackingEnsemble, RidgeStockModel

# æ–¹æ³•1: ä»…ä½¿ç”¨åŸºç¡€æ¨¡å‹é¢„æµ‹
ensemble = StackingEnsemble(
    base_models=[model1, model2, model3],
    meta_learner=RidgeStockModel(alpha=0.5),
    model_names=['Ridge', 'LGB-1', 'LGB-2']
)

# è®­ç»ƒå…ƒå­¦ä¹ å™¨
ensemble.train_meta_learner(
    X_train, y_train,
    X_valid, y_valid
)

# é¢„æµ‹
predictions = ensemble.predict(X_test)

# æ–¹æ³•2: ç»“åˆåŸå§‹ç‰¹å¾
ensemble_full = StackingEnsemble(
    base_models=[model1, model2, model3],
    meta_learner=RidgeStockModel(alpha=0.5),
    use_original_features=True  # å°†åŸå§‹ç‰¹å¾ä¹Ÿä¼ ç»™å…ƒå­¦ä¹ å™¨
)

ensemble_full.train_meta_learner(X_train, y_train, X_valid, y_valid)
predictions_full = ensemble_full.predict(X_test)
```

#### è‡ªå®šä¹‰å…ƒå­¦ä¹ å™¨

```python
from models import LightGBMStockModel

# ä½¿ç”¨ LightGBM ä½œä¸ºå…ƒå­¦ä¹ å™¨
meta_lgb = LightGBMStockModel(
    learning_rate=0.05,
    n_estimators=100,
    num_leaves=15
)

ensemble = StackingEnsemble(
    base_models=[model1, model2, model3],
    meta_learner=meta_lgb
)

ensemble.train_meta_learner(X_train, y_train, X_valid, y_valid)
```

#### æ•°æ®åˆ†å‰²ç­–ç•¥

**é‡è¦**ï¼šStacking éœ€è¦é˜²æ­¢æ•°æ®æ³„éœ²

```python
# æ¨èåˆ†å‰²æ–¹å¼
# è®­ç»ƒé›†: 60% (è®­ç»ƒåŸºç¡€æ¨¡å‹)
# éªŒè¯é›†: 20% (è®­ç»ƒå…ƒå­¦ä¹ å™¨)
# æµ‹è¯•é›†: 20% (æœ€ç»ˆè¯„ä¼°)

train_size = int(len(X) * 0.6)
valid_size = int(len(X) * 0.8)

X_train, y_train = X[:train_size], y[:train_size]
X_valid, y_valid = X[train_size:valid_size], y[train_size:valid_size]
X_test, y_test = X[valid_size:], y[valid_size:]

# 1. åœ¨è®­ç»ƒé›†ä¸Šè®­ç»ƒåŸºç¡€æ¨¡å‹
for model in base_models:
    model.train(X_train, y_train)

# 2. åœ¨éªŒè¯é›†ä¸Šè®­ç»ƒå…ƒå­¦ä¹ å™¨
ensemble.train_meta_learner(X_train, y_train, X_valid, y_valid)

# 3. åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
predictions = ensemble.predict(X_test)
```

#### ä¼˜ç¼ºç‚¹

âœ… **ä¼˜ç‚¹**ï¼š
- æ€§èƒ½é€šå¸¸æœ€ä¼˜
- è‡ªåŠ¨å­¦ä¹ æ¨¡å‹æƒé‡
- èƒ½æ•è·æ¨¡å‹é—´å¤æ‚å…³ç³»

âŒ **ç¼ºç‚¹**ï¼š
- éœ€è¦é¢å¤–è®­ç»ƒæ•°æ®
- è®­ç»ƒæ—¶é—´è¾ƒé•¿
- å¯èƒ½è¿‡æ‹Ÿåˆï¼ˆéœ€è¦æ­£åˆ™åŒ–ï¼‰

---

## API æ–‡æ¡£

### BaseEnsemble (åŸºç±»)

æ‰€æœ‰é›†æˆæ¨¡å‹çš„æŠ½è±¡åŸºç±»

```python
class BaseEnsemble(ABC):
    def __init__(self, models: List[Any], model_names: Optional[List[str]] = None)
    def predict(self, X: pd.DataFrame) -> np.ndarray
    def get_individual_predictions(self, X: pd.DataFrame) -> Dict[str, np.ndarray]
    def save(self, filepath: str)
```

### WeightedAverageEnsemble

åŠ æƒå¹³å‡é›†æˆ

```python
class WeightedAverageEnsemble(BaseEnsemble):
    def __init__(
        self,
        models: List[Any],
        weights: Optional[List[float]] = None,
        model_names: Optional[List[str]] = None
    )

    def predict(self, X: pd.DataFrame) -> np.ndarray

    def optimize_weights(
        self,
        X_valid: pd.DataFrame,
        y_valid: pd.Series,
        metric: str = 'ic'
    ) -> np.ndarray
```

**å‚æ•°**ï¼š
- `models`: æ¨¡å‹åˆ—è¡¨
- `weights`: æƒé‡åˆ—è¡¨ï¼ˆNone=ç­‰æƒé‡ï¼Œè‡ªåŠ¨å½’ä¸€åŒ–ï¼‰
- `model_names`: æ¨¡å‹åç§°
- `metric`: ä¼˜åŒ–æŒ‡æ ‡ ('ic', 'rank_ic', 'mse')

### VotingEnsemble

æŠ•ç¥¨æ³•é›†æˆ

```python
class VotingEnsemble(BaseEnsemble):
    def __init__(
        self,
        models: List[Any],
        model_names: Optional[List[str]] = None,
        voting_weights: Optional[List[float]] = None
    )

    def predict(self, X: pd.DataFrame) -> np.ndarray

    def select_top_n(
        self,
        X: pd.DataFrame,
        top_n: int,
        return_scores: bool = False
    ) -> Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]
```

**å‚æ•°**ï¼š
- `models`: æ¨¡å‹åˆ—è¡¨
- `model_names`: æ¨¡å‹åç§°
- `voting_weights`: æŠ•ç¥¨æƒé‡ï¼ˆNone=ç­‰æƒé‡ï¼‰
- `top_n`: é€‰æ‹©æ•°é‡
- `return_scores`: æ˜¯å¦è¿”å›åˆ†æ•°

### StackingEnsemble

Stacking é›†æˆ

```python
class StackingEnsemble(BaseEnsemble):
    def __init__(
        self,
        base_models: List[Any],
        meta_learner: Optional[Any] = None,
        model_names: Optional[List[str]] = None,
        use_original_features: bool = False
    )

    def train_meta_learner(
        self,
        X_train: pd.DataFrame,
        y_train: pd.Series,
        X_valid: Optional[pd.DataFrame] = None,
        y_valid: Optional[pd.Series] = None
    )

    def predict(self, X: pd.DataFrame) -> np.ndarray
```

**å‚æ•°**ï¼š
- `base_models`: åŸºç¡€æ¨¡å‹åˆ—è¡¨
- `meta_learner`: å…ƒå­¦ä¹ å™¨ï¼ˆNone=ä½¿ç”¨Ridgeï¼‰
- `model_names`: æ¨¡å‹åç§°
- `use_original_features`: æ˜¯å¦ä½¿ç”¨åŸå§‹ç‰¹å¾

### create_ensemble (ä¾¿æ·å‡½æ•°)

å¿«é€Ÿåˆ›å»ºé›†æˆæ¨¡å‹

```python
def create_ensemble(
    models: List[Any],
    method: str = 'weighted_average',
    model_names: Optional[List[str]] = None,
    **kwargs
) -> BaseEnsemble
```

**å‚æ•°**ï¼š
- `models`: æ¨¡å‹åˆ—è¡¨
- `method`: é›†æˆæ–¹æ³• ('weighted_average', 'voting', 'stacking')
- `model_names`: æ¨¡å‹åç§°
- `**kwargs`: ä¼ é€’ç»™å…·ä½“é›†æˆç±»çš„å‚æ•°

**ç¤ºä¾‹**ï¼š

```python
# åŠ æƒå¹³å‡
ensemble = create_ensemble(
    [model1, model2],
    method='weighted_average',
    weights=[0.6, 0.4]
)

# æŠ•ç¥¨æ³•
ensemble = create_ensemble(
    [model1, model2, model3],
    method='voting'
)

# Stacking
ensemble = create_ensemble(
    [model1, model2],
    method='stacking',
    meta_learner=RidgeStockModel()
)
```

---

## æœ€ä½³å®è·µ

### 1. é€‰æ‹©åŸºç¡€æ¨¡å‹

âœ… **å¥½çš„ç»„åˆ**ï¼š
- Ridge + LightGBMï¼ˆçº¿æ€§ + éçº¿æ€§ï¼‰
- LightGBM + GRUï¼ˆæ ‘æ¨¡å‹ + æ·±åº¦å­¦ä¹ ï¼‰
- å¤šä¸ªä¸åŒå‚æ•°çš„ LightGBM

âŒ **ä¸å¥½çš„ç»„åˆ**ï¼š
- å¤šä¸ªå®Œå…¨ç›¸åŒçš„æ¨¡å‹
- é«˜åº¦ç›¸å…³çš„æ¨¡å‹ï¼ˆå¦‚ä¸¤ä¸ªå‡ ä¹ç›¸åŒå‚æ•°çš„ Ridgeï¼‰

### 2. æƒé‡è®¾ç½®åŸåˆ™

```python
# åŸåˆ™1: æ€§èƒ½å¥½çš„æ¨¡å‹æƒé‡é«˜
# å•æ¨¡å‹ IC: Ridge=0.92, LightGBM=0.95
weights = [0.4, 0.6]  # æ ¹æ®æ€§èƒ½åˆ†é…

# åŸåˆ™2: ç¨³å®šçš„æ¨¡å‹æƒé‡é«˜
# Ridge æ›´ç¨³å®šï¼ŒLightGBM å¯èƒ½è¿‡æ‹Ÿåˆ
weights = [0.6, 0.4]  # ç¨³å®šæ€§ä¼˜å…ˆ

# åŸåˆ™3: ä½¿ç”¨éªŒè¯é›†è‡ªåŠ¨ä¼˜åŒ–
ensemble.optimize_weights(X_valid, y_valid, metric='ic')
```

### 3. é˜²æ­¢æ•°æ®æ³„éœ²

```python
# âŒ é”™è¯¯ï¼šåœ¨åŒä¸€æ•°æ®ä¸Šè®­ç»ƒåŸºç¡€æ¨¡å‹å’Œå…ƒå­¦ä¹ å™¨
ensemble.train_meta_learner(X_train, y_train)

# âœ… æ­£ç¡®ï¼šä½¿ç”¨ç‹¬ç«‹çš„éªŒè¯é›†è®­ç»ƒå…ƒå­¦ä¹ å™¨
ensemble.train_meta_learner(
    X_train, y_train,  # åŸºç¡€æ¨¡å‹å·²åœ¨æ­¤è®­ç»ƒ
    X_valid, y_valid   # å…ƒå­¦ä¹ å™¨ä½¿ç”¨ç‹¬ç«‹æ•°æ®
)
```

### 4. é›†æˆæ¨¡å‹æ•°é‡

- **2-3ä¸ªæ¨¡å‹**ï¼šæ€§ä»·æ¯”æœ€é«˜
- **3-5ä¸ªæ¨¡å‹**ï¼šæ€§èƒ½æ¥è¿‘æœ€ä¼˜
- **5+ä¸ªæ¨¡å‹**ï¼šè¾¹é™…æ”¶ç›Šé€’å‡ï¼Œå¢åŠ è®¡ç®—æˆæœ¬

### 5. æ€§èƒ½è¯„ä¼°

```python
# å¯¹æ¯”å•æ¨¡å‹å’Œé›†æˆæ•ˆæœ
def compare_performance(models, ensemble, X_test, y_test):
    """å¯¹æ¯”æ€§èƒ½"""
    results = {}

    # å•æ¨¡å‹
    for name, model in models.items():
        pred = model.predict(X_test)
        ic = np.corrcoef(pred, y_test)[0, 1]
        results[name] = ic

    # é›†æˆ
    ensemble_pred = ensemble.predict(X_test)
    ensemble_ic = np.corrcoef(ensemble_pred, y_test)[0, 1]
    results['Ensemble'] = ensemble_ic

    return results

# ä½¿ç”¨
results = compare_performance(
    {'Ridge': ridge, 'LightGBM': lgb},
    ensemble,
    X_test, y_test
)

for name, ic in results.items():
    print(f"{name}: IC={ic:.6f}")
```

---

## æ€§èƒ½å¯¹æ¯”

### å®éªŒè®¾ç½®

- æ•°æ®ï¼š1000 æ ·æœ¬ï¼Œ30 ç‰¹å¾
- åŸºç¡€æ¨¡å‹ï¼šRidgeã€LightGBM-1ã€LightGBM-2
- è¯„ä¼°æŒ‡æ ‡ï¼šIC (Information Coefficient)

### ç»“æœ

| æ–¹æ³• | Test IC | æå‡ | è®­ç»ƒæ—¶é—´ |
|------|---------|------|----------|
| Ridge (å•æ¨¡å‹) | 0.9986 | - | 0.1s |
| LightGBM-1 (å•æ¨¡å‹) | 0.9843 | - | 0.5s |
| LightGBM-2 (å•æ¨¡å‹) | 0.9860 | - | 0.4s |
| **åŠ æƒå¹³å‡ (ç­‰æƒé‡)** | 0.9938 | -0.48% | 0.1s |
| **åŠ æƒå¹³å‡ (ä¼˜åŒ–)** | **0.9986** | +0.00% | 0.3s |
| **æŠ•ç¥¨æ³•** | 0.9677 | -3.09% | 0.1s |
| **Stacking (åŸºç¡€)** | 0.9979 | -0.07% | 0.8s |
| **Stacking (å®Œæ•´)** | 0.9986 | -0.00% | 0.9s |

### åˆ†æ

1. **åŠ æƒå¹³å‡ï¼ˆä¼˜åŒ–ï¼‰**ï¼šæ€§èƒ½æœ€ä¼˜ï¼Œè®¡ç®—æœ€å¿«
2. **Stacking**ï¼šæ€§èƒ½æ¥è¿‘æœ€ä¼˜ï¼Œä½†è®­ç»ƒæ—¶é—´è¾ƒé•¿
3. **æŠ•ç¥¨æ³•**ï¼šé€‚åˆé€‰è‚¡ï¼Œä¸é€‚åˆå›å½’é¢„æµ‹

---

## å¸¸è§é—®é¢˜

### Q1: é›†æˆåæ€§èƒ½åè€Œä¸‹é™ï¼Ÿ

**åŸå› **ï¼š
- åŸºç¡€æ¨¡å‹è´¨é‡å·®æˆ–é«˜åº¦ç›¸å…³
- æƒé‡è®¾ç½®ä¸åˆç†
- Stacking æ•°æ®æ³„éœ²

**è§£å†³**ï¼š
- ä½¿ç”¨æ€§èƒ½å·®å¼‚å¤§çš„æ¨¡å‹
- ä½¿ç”¨ `optimize_weights()` è‡ªåŠ¨ä¼˜åŒ–
- ç¡®ä¿ Stacking ä½¿ç”¨ç‹¬ç«‹éªŒè¯é›†

### Q2: å¦‚ä½•é€‰æ‹©é›†æˆæ–¹æ³•ï¼Ÿ

| åœºæ™¯ | æ¨èæ–¹æ³• |
|------|----------|
| æ•°æ®å……è¶³ï¼Œè¿½æ±‚æœ€ä¼˜ | Stacking |
| å¿«é€Ÿå®éªŒ | åŠ æƒå¹³å‡ï¼ˆä¼˜åŒ–ï¼‰ |
| é€‰è‚¡ç­–ç•¥ | æŠ•ç¥¨æ³• |
| æ¨¡å‹å·®å¼‚å¤§ | åŠ æƒå¹³å‡ |
| è®¡ç®—èµ„æºæœ‰é™ | åŠ æƒå¹³å‡ï¼ˆç­‰æƒé‡ï¼‰ |

### Q3: æƒé‡ä¼˜åŒ–éœ€è¦å¤šä¹…ï¼Ÿ

- é€šå¸¸ < 1ç§’ï¼ˆscipy.optimize.minimizeï¼‰
- å–å†³äºæ¨¡å‹æ•°é‡å’ŒéªŒè¯é›†å¤§å°
- å»ºè®®éªŒè¯é›† > 200 æ ·æœ¬

### Q4: å¯ä»¥é›†æˆä¸åŒç±»å‹çš„æ¨¡å‹å—ï¼Ÿ

âœ… å¯ä»¥ï¼ç”šè‡³é¼“åŠ±è¿™æ ·åšï¼š

```python
from models import LightGBMStockModel, RidgeStockModel, GRUStockTrainer

# çº¿æ€§ + æ ‘æ¨¡å‹ + æ·±åº¦å­¦ä¹ 
ensemble = WeightedAverageEnsemble([
    RidgeStockModel(),
    LightGBMStockModel(),
    GRUStockTrainer()
])
```

---

## å‚è€ƒèµ„æ–™

- **è®ºæ–‡**ï¼š[Stacking and Blending](https://www.sciencedirect.com/science/article/abs/pii/S0893608005800231)
- **æ¡ˆä¾‹**ï¼šNetflix Prize è·å¥–æ–¹æ¡ˆä½¿ç”¨äº†å¤§é‡é›†æˆæŠ€æœ¯
- **ä»£ç **ï¼š[examples/ensemble_example.py](examples/ensemble_example.py) å®Œæ•´ç¤ºä¾‹
- **æµ‹è¯•**ï¼š`core/tests/unit/test_ensemble.py` å•å…ƒæµ‹è¯•

---

## ç‰ˆæœ¬å†å²

- **v1.0.0** (2026-01-29)
  - âœ… åŠ æƒå¹³å‡é›†æˆ
  - âœ… æŠ•ç¥¨æ³•é›†æˆ
  - âœ… Stacking é›†æˆ
  - âœ… æƒé‡è‡ªåŠ¨ä¼˜åŒ–
  - âœ… 33 ä¸ªå•å…ƒæµ‹è¯•

---

**è´¡çŒ®è€…**ï¼šClaude Code
**æœ€åæ›´æ–°**ï¼š2026-01-29
**åé¦ˆæ¸ é“**ï¼šGitHub Issues
