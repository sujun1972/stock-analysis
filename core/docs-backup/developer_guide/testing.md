# æµ‹è¯•æŒ‡å—

**Testing Guide for Stock-Analysis Core**

**ç‰ˆæœ¬**: v3.0.0
**æœ€åæ›´æ–°**: 2026-02-01

---

> ğŸ’¡ **å¿«é€Ÿè¿è¡Œæµ‹è¯•**ï¼šæŸ¥çœ‹ [../../tests/README.md](../../tests/README.md)
>
> æœ¬æ–‡æ¡£å…³æ³¨**å¦‚ä½•ç¼–å†™æµ‹è¯•**ï¼ŒåŒ…æ‹¬æµ‹è¯•å“²å­¦ã€è§„èŒƒã€æœ€ä½³å®è·µã€‚
>
> å¦‚éœ€**è¿è¡Œæµ‹è¯•**ã€æŸ¥çœ‹æµ‹è¯•ç»Ÿè®¡æˆ–ä½¿ç”¨äº¤äº’å¼èœå•ï¼Œè¯·æŸ¥çœ‹ä¸Šè¿°é“¾æ¥ã€‚

---

## ğŸ“š æ–‡æ¡£å¯¼èˆª

### æœ¬æ–‡æ¡£ (æµ‹è¯•ç¼–å†™æŒ‡å—)
- [æµ‹è¯•å“²å­¦](#æµ‹è¯•å“²å­¦) - æ ¸å¿ƒåŸåˆ™å’Œæµ‹è¯•é‡‘å­—å¡”
- [å•å…ƒæµ‹è¯•](#å•å…ƒæµ‹è¯•) - ç¼–å†™å•å…ƒæµ‹è¯•çš„æ–¹æ³•
- [é›†æˆæµ‹è¯•](#é›†æˆæµ‹è¯•) - æ¨¡å—é—´äº¤äº’æµ‹è¯•
- [æ€§èƒ½æµ‹è¯•](#æ€§èƒ½æµ‹è¯•) - æ€§èƒ½åŸºå‡†æµ‹è¯•
- [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ) - æµ‹è¯•ç¼–å†™æŠ€å·§

### å…¶ä»–æµ‹è¯•æ–‡æ¡£
- ğŸ“‹ [æµ‹è¯•è¿è¡ŒæŒ‡å—](../../tests/README.md) - å¦‚ä½•è¿è¡Œæµ‹è¯•ã€äº¤äº’å¼èœå•
- ğŸ”— [é›†æˆæµ‹è¯•è¯¦è§£](../../tests/integration/README.md) - é›†æˆæµ‹è¯•è¯´æ˜
- âš¡ [æ€§èƒ½æµ‹è¯•è¯¦è§£](../../tests/performance/README.md) - æ€§èƒ½åŸºå‡†æµ‹è¯•
- ğŸ–¥ï¸ [CLIæµ‹è¯•è¯´æ˜](../../tests/cli/README.md) - å‘½ä»¤è¡Œå·¥å…·æµ‹è¯•

---

## ğŸ¯ æµ‹è¯•å“²å­¦

### æ ¸å¿ƒåŸåˆ™

- âœ… **æµ‹è¯•è¦†ç›–ç‡â‰¥90%**: æ‰€æœ‰æ ¸å¿ƒä»£ç å¿…é¡»æœ‰å……åˆ†æµ‹è¯•
- âœ… **æµ‹è¯•å…ˆè¡Œ**: ä¼˜å…ˆç¼–å†™æµ‹è¯•ï¼Œå†å®ç°åŠŸèƒ½
- âœ… **ç‹¬ç«‹æ€§**: æ¯ä¸ªæµ‹è¯•ç‹¬ç«‹è¿è¡Œï¼Œä¸ä¾èµ–å…¶ä»–æµ‹è¯•
- âœ… **å¯é‡å¤æ€§**: æµ‹è¯•ç»“æœå¯é‡å¤ï¼Œä¸å—å¤–éƒ¨ç¯å¢ƒå½±å“
- âœ… **å¿«é€Ÿåé¦ˆ**: å•å…ƒæµ‹è¯•åº”åœ¨ç§’çº§å®Œæˆ

### æµ‹è¯•é‡‘å­—å¡”

```
         /\
        /  \  E2Eæµ‹è¯• (5%)
       /----\
      / é›†æˆ  \ é›†æˆæµ‹è¯• (15%)
     /--------\
    /   å•å…ƒ    \ å•å…ƒæµ‹è¯• (80%)
   /____________\
```

**æ¯”ä¾‹å»ºè®®**:
- **å•å…ƒæµ‹è¯•**: 80% - å¿«é€Ÿã€ç‹¬ç«‹ã€è¦†ç›–æ ¸å¿ƒé€»è¾‘
- **é›†æˆæµ‹è¯•**: 15% - éªŒè¯æ¨¡å—é—´äº¤äº’
- **E2Eæµ‹è¯•**: 5% - éªŒè¯å®Œæ•´å·¥ä½œæµ

---

## ğŸ§ª å•å…ƒæµ‹è¯•

### 1. åŸºæœ¬ç»“æ„

**ä½ç½®**: `tests/unit/`

**å‘½åè§„èŒƒ**:
```
tests/unit/
â”œâ”€â”€ features/
â”‚   â”œâ”€â”€ test_alpha_factors.py      # æµ‹è¯• src/features/alpha_factors/
â”‚   â”œâ”€â”€ test_technical_indicators.py
â”‚   â””â”€â”€ test_feature_engineering.py
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ test_lightgbm_model.py
â”‚   â””â”€â”€ test_gru_model.py
â””â”€â”€ data/
    â”œâ”€â”€ test_database_manager.py
    â””â”€â”€ test_data_validator.py
```

### 2. æµ‹è¯•ç”¨ä¾‹ç¼–å†™

#### åŸºæœ¬ç¤ºä¾‹

```python
# tests/unit/features/test_alpha_factors.py

import pytest
import pandas as pd
import numpy as np
from src.features.alpha_factors import calculate_momentum, calculate_volatility


class TestMomentumFactor:
    """åŠ¨é‡å› å­æµ‹è¯•å¥—ä»¶"""

    @pytest.fixture
    def sample_data(self):
        """æµ‹è¯•æ•°æ®fixture"""
        return pd.DataFrame({
            'close': [100, 102, 101, 103, 105, 107, 106, 108, 110, 112]
        })

    def test_basic_calculation(self, sample_data):
        """æµ‹è¯•åŸºæœ¬è®¡ç®—é€»è¾‘"""
        result = calculate_momentum(sample_data, window=5)

        # éªŒè¯è¿”å›ç±»å‹
        assert isinstance(result, pd.Series)
        # éªŒè¯é•¿åº¦
        assert len(result) == len(sample_data)
        # éªŒè¯å‰æœŸNaN
        assert result.iloc[:4].isna().all()
        # éªŒè¯æ•°å€¼èŒƒå›´
        assert result.iloc[4:].between(-1, 2).all()

    def test_edge_cases(self):
        """æµ‹è¯•è¾¹ç•Œæƒ…å†µ"""
        # ç©ºæ•°æ®
        empty_df = pd.DataFrame({'close': []})
        result = calculate_momentum(empty_df, window=5)
        assert len(result) == 0

        # å•ä¸ªæ•°æ®ç‚¹
        single_df = pd.DataFrame({'close': [100]})
        result = calculate_momentum(single_df, window=5)
        assert result.isna().all()

    def test_invalid_inputs(self, sample_data):
        """æµ‹è¯•å¼‚å¸¸è¾“å…¥"""
        # æ— æ•ˆçª—å£
        with pytest.raises(ValueError, match="window must be positive"):
            calculate_momentum(sample_data, window=0)

        with pytest.raises(ValueError, match="window must be positive"):
            calculate_momentum(sample_data, window=-5)

        # ç¼ºå°‘å¿…éœ€åˆ—
        invalid_df = pd.DataFrame({'price': [100, 102, 101]})
        with pytest.raises(ValueError, match="must contain 'close' column"):
            calculate_momentum(invalid_df, window=5)

    @pytest.mark.parametrize("window,expected_nan_count", [
        (5, 4),
        (10, 9),
        (20, 19)
    ])
    def test_different_windows(self, sample_data, window, expected_nan_count):
        """å‚æ•°åŒ–æµ‹è¯•ï¼šä¸åŒçª—å£å¤§å°"""
        result = calculate_momentum(sample_data, window=window)
        nan_count = result.isna().sum()
        assert nan_count == expected_nan_count

    def test_numerical_accuracy(self):
        """æµ‹è¯•æ•°å€¼ç²¾åº¦"""
        data = pd.DataFrame({'close': [100, 105, 110, 115, 120]})
        result = calculate_momentum(data, window=2)

        # æ‰‹åŠ¨è®¡ç®—é¢„æœŸå€¼
        expected = pd.Series([np.nan, 0.05, 0.0476, 0.0455, 0.0435])

        # ä½¿ç”¨è¿‘ä¼¼ç›¸ç­‰æ¯”è¾ƒ
        pd.testing.assert_series_equal(
            result,
            expected,
            check_exact=False,
            rtol=1e-3  # ç›¸å¯¹è¯¯å·®å®¹å¿åº¦
        )
```

### 3. Fixtureä½¿ç”¨

#### å…±äº«Fixture

```python
# tests/conftest.py

import pytest
import pandas as pd
from src.data.database_manager import DatabaseManager


@pytest.fixture(scope="session")
def db_manager():
    """ä¼šè¯çº§æ•°æ®åº“ç®¡ç†å™¨"""
    manager = DatabaseManager(test_mode=True)
    yield manager
    manager.close()


@pytest.fixture(scope="module")
def sample_stock_data():
    """æ¨¡å—çº§æµ‹è¯•æ•°æ®"""
    return pd.DataFrame({
        'stock_code': ['000001.SZ'] * 100,
        'trade_date': pd.date_range('2023-01-01', periods=100),
        'open': np.random.uniform(10, 20, 100),
        'high': np.random.uniform(15, 25, 100),
        'low': np.random.uniform(5, 15, 100),
        'close': np.random.uniform(10, 20, 100),
        'volume': np.random.uniform(1000000, 5000000, 100)
    })


@pytest.fixture
def mock_api_response():
    """Mock APIå“åº”"""
    return {
        'code': 0,
        'message': 'success',
        'data': {
            'stock_code': '000001.SZ',
            'price': 15.23
        }
    }
```

#### Fixtureä½œç”¨åŸŸ

```python
# scope="function" (é»˜è®¤): æ¯ä¸ªæµ‹è¯•å‡½æ•°åˆ›å»ºä¸€æ¬¡
@pytest.fixture
def fresh_data():
    return pd.DataFrame({'value': [1, 2, 3]})

# scope="class": æ¯ä¸ªæµ‹è¯•ç±»åˆ›å»ºä¸€æ¬¡
@pytest.fixture(scope="class")
def class_data():
    return load_large_dataset()

# scope="module": æ¯ä¸ªæ¨¡å—åˆ›å»ºä¸€æ¬¡
@pytest.fixture(scope="module")
def module_data():
    return setup_database()

# scope="session": æ•´ä¸ªæµ‹è¯•ä¼šè¯åˆ›å»ºä¸€æ¬¡
@pytest.fixture(scope="session")
def session_config():
    return load_config()
```

### 4. Mockå’ŒPatch

#### Mockå¤–éƒ¨ä¾èµ–

```python
# tests/unit/data/test_data_fetcher.py

import pytest
from unittest.mock import Mock, patch, MagicMock
from src.data.data_fetcher import StockDataFetcher


class TestStockDataFetcher:
    @pytest.fixture
    def fetcher(self):
        return StockDataFetcher()

    @patch('src.data.data_fetcher.requests.get')
    def test_fetch_stock_data(self, mock_get, fetcher):
        """Mock HTTPè¯·æ±‚"""
        # é…ç½®Mockè¿”å›å€¼
        mock_response = Mock()
        mock_response.status_code = 200
        mock_response.json.return_value = {
            'data': [
                {'date': '2023-01-01', 'close': 15.23}
            ]
        }
        mock_get.return_value = mock_response

        # æ‰§è¡Œæµ‹è¯•
        result = fetcher.fetch('000001.SZ')

        # éªŒè¯
        assert len(result) == 1
        mock_get.assert_called_once()

    @patch('src.data.data_fetcher.DatabaseManager')
    def test_save_to_database(self, mock_db_manager, fetcher):
        """Mockæ•°æ®åº“æ“ä½œ"""
        # åˆ›å»ºMockå®ä¾‹
        mock_db_instance = MagicMock()
        mock_db_manager.return_value = mock_db_instance

        # æ‰§è¡Œæµ‹è¯•
        data = pd.DataFrame({'close': [15.23]})
        fetcher.save(data)

        # éªŒè¯æ•°æ®åº“è°ƒç”¨
        mock_db_instance.insert.assert_called_once()
```

#### Mockæ—¶é—´

```python
from unittest.mock import patch
from datetime import datetime


@patch('src.utils.time_utils.datetime')
def test_time_dependent_function(mock_datetime):
    """Mockæ—¶é—´ä»¥æµ‹è¯•æ—¶é—´ç›¸å…³åŠŸèƒ½"""
    # å›ºå®šæ—¶é—´
    mock_datetime.now.return_value = datetime(2023, 1, 1, 12, 0, 0)

    result = get_current_trading_day()
    assert result == '2023-01-01'
```

---

## ğŸ”— é›†æˆæµ‹è¯•

### 1. æ•°æ®åº“é›†æˆæµ‹è¯•

**ä½ç½®**: `tests/integration/test_database_integration.py`

```python
import pytest
from src.data.database_manager import DatabaseManager
from src.data.data_validator import DataValidator


class TestDatabaseIntegration:
    @pytest.fixture(scope="class")
    def db(self):
        """æµ‹è¯•æ•°æ®åº“"""
        db = DatabaseManager(database="test_stock_db")
        db.create_tables()
        yield db
        db.drop_tables()
        db.close()

    def test_insert_and_query(self, db):
        """æµ‹è¯•æ’å…¥å’ŒæŸ¥è¯¢"""
        # æ’å…¥æµ‹è¯•æ•°æ®
        test_data = pd.DataFrame({
            'stock_code': ['000001.SZ'],
            'trade_date': ['2023-01-01'],
            'close': [15.23]
        })
        db.insert('stock_data', test_data)

        # æŸ¥è¯¢éªŒè¯
        result = db.query(
            "SELECT * FROM stock_data WHERE stock_code='000001.SZ'"
        )
        assert len(result) == 1
        assert result['close'].iloc[0] == 15.23

    def test_data_validation_pipeline(self, db):
        """æµ‹è¯•æ•°æ®éªŒè¯æµç¨‹"""
        # æ’å…¥æ— æ•ˆæ•°æ®
        invalid_data = pd.DataFrame({
            'stock_code': ['000001.SZ'],
            'trade_date': ['2023-01-01'],
            'close': [-999]  # æ— æ•ˆä»·æ ¼
        })

        # éªŒè¯
        validator = DataValidator()
        is_valid, errors = validator.validate(invalid_data)

        assert not is_valid
        assert 'close' in errors
```

### 2. ç‰¹å¾å·¥ç¨‹é›†æˆæµ‹è¯•

```python
# tests/integration/test_feature_pipeline.py

import pytest
from src.features.feature_engineer import FeatureEngineer
from src.data.database_manager import DatabaseManager


class TestFeaturePipeline:
    @pytest.fixture(scope="class")
    def feature_engineer(self):
        return FeatureEngineer()

    def test_complete_feature_calculation(self, feature_engineer, sample_stock_data):
        """æµ‹è¯•å®Œæ•´ç‰¹å¾è®¡ç®—æµç¨‹"""
        # è®¡ç®—æ‰€æœ‰ç‰¹å¾
        features = feature_engineer.calculate_all_features(sample_stock_data)

        # éªŒè¯ç‰¹å¾æ•°é‡
        assert len(features.columns) >= 100  # è‡³å°‘100ä¸ªç‰¹å¾

        # éªŒè¯å…³é”®ç‰¹å¾å­˜åœ¨
        required_features = [
            'momentum_5', 'momentum_20',
            'volatility_20', 'volatility_60',
            'rsi_14', 'macd'
        ]
        for feature in required_features:
            assert feature in features.columns

        # éªŒè¯æ— NaNï¼ˆå‰æœŸçª—å£é™¤å¤–ï¼‰
        assert features.iloc[60:].notna().all().all()
```

---

## ğŸ­ ç«¯åˆ°ç«¯æµ‹è¯•

### 1. å®Œæ•´å·¥ä½œæµæµ‹è¯•

**ä½ç½®**: `tests/integration/test_end_to_end_workflow.py`

```python
import pytest
from src.data.database_manager import DatabaseManager
from src.features.feature_engineer import FeatureEngineer
from src.models.model_factory import ModelFactory
from src.backtest.backtest_engine import BacktestEngine


class TestEndToEndWorkflow:
    """ç«¯åˆ°ç«¯å·¥ä½œæµæµ‹è¯•"""

    @pytest.fixture(scope="class")
    def setup_environment(self):
        """è®¾ç½®æµ‹è¯•ç¯å¢ƒ"""
        db = DatabaseManager(database="test_e2e")
        db.create_tables()

        # æ’å…¥æµ‹è¯•æ•°æ®
        test_data = load_test_dataset()
        db.insert('stock_data', test_data)

        yield db

        db.drop_tables()
        db.close()

    def test_complete_trading_workflow(self, setup_environment):
        """æµ‹è¯•å®Œæ•´äº¤æ˜“å·¥ä½œæµ"""
        db = setup_environment

        # 1. æ•°æ®åŠ è½½
        data = db.query_stock_data('000001.SZ', '2023-01-01', '2023-12-31')
        assert len(data) > 0

        # 2. ç‰¹å¾è®¡ç®—
        engineer = FeatureEngineer()
        features = engineer.calculate_all_features(data)
        assert len(features.columns) >= 100

        # 3. æ¨¡å‹é¢„æµ‹
        model = ModelFactory.create_model('lightgbm')
        model.load('models/lightgbm_latest.pkl')
        predictions = model.predict(features)
        assert len(predictions) == len(features)

        # 4. å›æµ‹
        strategy = create_strategy_from_predictions(predictions)
        engine = BacktestEngine(strategy)
        result = engine.run(data)

        # éªŒè¯å›æµ‹ç»“æœ
        assert result.total_return is not None
        assert result.sharpe_ratio is not None
        assert result.max_drawdown < 0  # æœ€å¤§å›æ’¤åº”ä¸ºè´Ÿæ•°
```

---

## ğŸ“Š æ€§èƒ½æµ‹è¯•

### 1. åŸºå‡†æµ‹è¯•

```python
# tests/performance/test_feature_performance.py

import pytest
import time
from src.features.alpha_factors import calculate_all_alpha_factors


@pytest.mark.benchmark
class TestFeaturePerformance:
    """ç‰¹å¾è®¡ç®—æ€§èƒ½åŸºå‡†æµ‹è¯•"""

    @pytest.fixture
    def large_dataset(self):
        """å¤§è§„æ¨¡æµ‹è¯•æ•°æ®"""
        return generate_test_data(n_stocks=1000, n_days=252)

    def test_feature_calculation_speed(self, large_dataset, benchmark):
        """æµ‹è¯•ç‰¹å¾è®¡ç®—é€Ÿåº¦"""
        # ä½¿ç”¨pytest-benchmark
        result = benchmark(calculate_all_alpha_factors, large_dataset)

        # éªŒè¯æ€§èƒ½è¦æ±‚
        assert benchmark.stats['mean'] < 5.0  # å¹³å‡æ—¶é—´<5ç§’

    def test_memory_usage(self, large_dataset):
        """æµ‹è¯•å†…å­˜ä½¿ç”¨"""
        import tracemalloc

        tracemalloc.start()
        features = calculate_all_alpha_factors(large_dataset)
        current, peak = tracemalloc.get_traced_memory()
        tracemalloc.stop()

        # éªŒè¯å†…å­˜ä½¿ç”¨
        peak_mb = peak / 1024 / 1024
        assert peak_mb < 500  # å³°å€¼å†…å­˜<500MB
```

### 2. æ€§èƒ½å›å½’æµ‹è¯•

```python
# tests/performance/test_performance_regression.py

import pytest
import json
from pathlib import Path


class TestPerformanceRegression:
    """æ€§èƒ½å›å½’æµ‹è¯•"""

    BASELINE_FILE = Path("tests/performance/baseline.json")

    @pytest.fixture
    def baseline(self):
        """åŠ è½½åŸºå‡†æ€§èƒ½æ•°æ®"""
        if self.BASELINE_FILE.exists():
            return json.loads(self.BASELINE_FILE.read_text())
        return {}

    def test_feature_calculation_regression(self, large_dataset, baseline):
        """ç‰¹å¾è®¡ç®—æ€§èƒ½å›å½’æµ‹è¯•"""
        start = time.time()
        result = calculate_all_alpha_factors(large_dataset)
        elapsed = time.time() - start

        # ä¸åŸºå‡†æ¯”è¾ƒ
        if 'feature_calculation' in baseline:
            baseline_time = baseline['feature_calculation']
            # å…è®¸10%çš„æ€§èƒ½æ³¢åŠ¨
            assert elapsed < baseline_time * 1.1, \
                f"Performance regression: {elapsed:.2f}s > {baseline_time:.2f}s"

        # æ›´æ–°åŸºå‡†ï¼ˆå¯é€‰ï¼‰
        baseline['feature_calculation'] = elapsed
        self.BASELINE_FILE.write_text(json.dumps(baseline, indent=2))
```

---

## ğŸ¯ æµ‹è¯•è¦†ç›–ç‡

### 1. è¿è¡Œè¦†ç›–ç‡æµ‹è¯•

```bash
# ç”Ÿæˆè¦†ç›–ç‡æŠ¥å‘Š
pytest --cov=src --cov-report=html --cov-report=term

# åªçœ‹æ ¸å¿ƒæ¨¡å—
pytest --cov=src/features --cov=src/models --cov-report=term

# æŸ¥çœ‹ç¼ºå¤±çš„è¡Œ
pytest --cov=src --cov-report=term-missing
```

### 2. è¦†ç›–ç‡è¦æ±‚

**æœ€ä½è¦†ç›–ç‡æ ‡å‡†**:
- âœ… **æ•´ä½“**: â‰¥90%
- âœ… **æ ¸å¿ƒæ¨¡å—**: â‰¥95% (features, models, backtest)
- âœ… **æ•°æ®å±‚**: â‰¥85%
- âœ… **å·¥å…·ç±»**: â‰¥80%

**é…ç½®æ–‡ä»¶** (`pyproject.toml`):
```toml
[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = "test_*.py"
python_classes = "Test*"
python_functions = "test_*"
addopts = "--cov=src --cov-report=html --cov-report=term --cov-fail-under=90"

[tool.coverage.run]
source = ["src"]
omit = [
    "*/tests/*",
    "*/__init__.py",
    "*/config.py"
]

[tool.coverage.report]
exclude_lines = [
    "pragma: no cover",
    "def __repr__",
    "raise AssertionError",
    "raise NotImplementedError",
    "if __name__ == .__main__.:",
    "if TYPE_CHECKING:",
]
```

---

## ğŸƒ è¿è¡Œæµ‹è¯•

### 1. åŸºæœ¬å‘½ä»¤

```bash
# è¿è¡Œæ‰€æœ‰æµ‹è¯•
pytest

# è¿è¡Œç‰¹å®šç›®å½•
pytest tests/unit/features/

# è¿è¡Œç‰¹å®šæ–‡ä»¶
pytest tests/unit/features/test_alpha_factors.py

# è¿è¡Œç‰¹å®šæµ‹è¯•ç±»
pytest tests/unit/features/test_alpha_factors.py::TestMomentumFactor

# è¿è¡Œç‰¹å®šæµ‹è¯•æ–¹æ³•
pytest tests/unit/features/test_alpha_factors.py::TestMomentumFactor::test_basic_calculation
```

### 2. é«˜çº§é€‰é¡¹

```bash
# è¯¦ç»†è¾“å‡º
pytest -v

# éå¸¸è¯¦ç»†è¾“å‡º
pytest -vv

# æ˜¾ç¤ºprintè¾“å‡º
pytest -s

# åªè¿è¡Œå¤±è´¥çš„æµ‹è¯•
pytest --lf

# é‡åˆ°ç¬¬ä¸€ä¸ªå¤±è´¥å°±åœæ­¢
pytest -x

# å¹¶è¡Œè¿è¡Œï¼ˆéœ€è¦pytest-xdistï¼‰
pytest -n 4

# è¿è¡Œç‰¹å®šæ ‡è®°çš„æµ‹è¯•
pytest -m "not slow"
```

### 3. æµ‹è¯•æ ‡è®°

```python
# æ ‡è®°æ…¢é€Ÿæµ‹è¯•
@pytest.mark.slow
def test_large_dataset_processing():
    pass

# æ ‡è®°éœ€è¦GPUçš„æµ‹è¯•
@pytest.mark.gpu
def test_gpu_training():
    pass

# æ ‡è®°é›†æˆæµ‹è¯•
@pytest.mark.integration
def test_database_integration():
    pass

# è·³è¿‡æµ‹è¯•
@pytest.mark.skip(reason="Not implemented yet")
def test_future_feature():
    pass

# æ¡ä»¶è·³è¿‡
@pytest.mark.skipif(sys.platform == 'win32', reason="Unix only")
def test_unix_feature():
    pass
```

**è¿è¡Œç‰¹å®šæ ‡è®°**:
```bash
# åªè¿è¡Œå•å…ƒæµ‹è¯•
pytest -m "not integration and not slow"

# åªè¿è¡Œé›†æˆæµ‹è¯•
pytest -m integration

# æ’é™¤æ…¢é€Ÿæµ‹è¯•
pytest -m "not slow"
```

---

## ğŸ” æµ‹è¯•æœ€ä½³å®è·µ

### 1. AAAæ¨¡å¼

**Arrange-Act-Assert**:
```python
def test_calculate_returns():
    # Arrange: å‡†å¤‡æµ‹è¯•æ•°æ®
    prices = pd.Series([100, 105, 110])

    # Act: æ‰§è¡Œè¢«æµ‹è¯•çš„ä»£ç 
    returns = calculate_returns(prices)

    # Assert: éªŒè¯ç»“æœ
    expected = pd.Series([np.nan, 0.05, 0.0476])
    pd.testing.assert_series_equal(returns, expected, rtol=1e-3)
```

### 2. æµ‹è¯•å‘½å

```python
# âœ… å¥½çš„å‘½åï¼šæè¿°æµ‹è¯•å†…å®¹
def test_momentum_returns_nan_for_insufficient_data():
    pass

def test_volatility_raises_error_for_negative_window():
    pass

def test_backtest_calculates_correct_sharpe_ratio():
    pass

# âŒ é¿å…ï¼šæ¨¡ç³Šçš„å‘½å
def test_momentum():
    pass

def test_case_1():
    pass
```

### 3. ä¸€ä¸ªæµ‹è¯•ä¸€ä¸ªæ–­è¨€

```python
# âœ… å¥½çš„ï¼šæ¯ä¸ªæµ‹è¯•ä¸€ä¸ªæ¦‚å¿µ
def test_momentum_returns_correct_type():
    result = calculate_momentum(data)
    assert isinstance(result, pd.Series)

def test_momentum_returns_correct_length():
    result = calculate_momentum(data)
    assert len(result) == len(data)

# âŒ é¿å…ï¼šæµ‹è¯•å¤šä¸ªä¸ç›¸å…³çš„æ¦‚å¿µ
def test_momentum_everything():
    result = calculate_momentum(data)
    assert isinstance(result, pd.Series)  # ç±»å‹
    assert len(result) == len(data)  # é•¿åº¦
    assert result.mean() > 0  # æ•°å€¼
    assert result.std() < 1  # åˆ†å¸ƒ
```

### 4. ä½¿ç”¨å‚æ•°åŒ–å‡å°‘é‡å¤

```python
# âœ… å¥½çš„ï¼šå‚æ•°åŒ–æµ‹è¯•
@pytest.mark.parametrize("window,expected_nan_count", [
    (5, 4),
    (10, 9),
    (20, 19),
    (60, 59)
])
def test_momentum_nan_count(window, expected_nan_count):
    result = calculate_momentum(data, window=window)
    assert result.isna().sum() == expected_nan_count

# âŒ é¿å…ï¼šé‡å¤çš„æµ‹è¯•ä»£ç 
def test_momentum_window_5():
    result = calculate_momentum(data, window=5)
    assert result.isna().sum() == 4

def test_momentum_window_10():
    result = calculate_momentum(data, window=10)
    assert result.isna().sum() == 9
```

---

## ğŸš€ CI/CDé›†æˆ

### 1. GitHub Actionsé…ç½®

**æ–‡ä»¶**: `.github/workflows/tests.yml`

```yaml
name: Tests

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'

    - name: Install dependencies
      run: |
        pip install -r requirements.txt
        pip install -r requirements-dev.txt

    - name: Run tests
      run: |
        pytest --cov=src --cov-report=xml --cov-fail-under=90

    - name: Upload coverage
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
```

### 2. é¢„æäº¤é’©å­

**æ–‡ä»¶**: `.pre-commit-config.yaml`

```yaml
repos:
  - repo: local
    hooks:
      - id: pytest-check
        name: pytest-check
        entry: pytest
        language: system
        pass_filenames: false
        always_run: true
        args: [--cov=src, --cov-fail-under=90]
```

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- ğŸ¨ [ä»£ç è§„èŒƒ](coding_standards.md)
- ğŸ¤ [è´¡çŒ®æŒ‡å—](contributing.md)
- ğŸ—ï¸ [æ¶æ„æ€»è§ˆ](../architecture/overview.md)

---

## ğŸ“Š æµ‹è¯•ç±»å‹æ¦‚è§ˆ

### æµ‹è¯•å¥—ä»¶ç»Ÿè®¡

| æµ‹è¯•ç±»å‹ | æ–‡ä»¶æ•° | æµ‹è¯•æ•° | è¦†ç›–èŒƒå›´ | è¯¦ç»†æ–‡æ¡£ |
|---------|--------|--------|---------|---------|
| **å•å…ƒæµ‹è¯•** | ~80 | ~2,600+ | æ‰€æœ‰æ ¸å¿ƒæ¨¡å— | [tests/README.md](../../tests/README.md) |
| **é›†æˆæµ‹è¯•** | 23 | 134 | æ¨¡å—é—´äº¤äº’ | [tests/integration/README.md](../../tests/integration/README.md) |
| **æ€§èƒ½æµ‹è¯•** | ~10 | ~100 | æ€§èƒ½åŸºå‡† | [tests/performance/README.md](../../tests/performance/README.md) |
| **CLIæµ‹è¯•** | 8 | 142 | å‘½ä»¤è¡Œå·¥å…· | [tests/cli/README.md](../../tests/cli/README.md) |
| **æ€»è®¡** | **~121** | **~2,976** | **90%+è¦†ç›–ç‡** | - |

### æµ‹è¯•ç›®å½•ç»“æ„

```
tests/
â”œâ”€â”€ README.md                    # æµ‹è¯•è¿è¡ŒæŒ‡å— â­
â”œâ”€â”€ run_tests.py                 # äº¤äº’å¼æµ‹è¯•è¿è¡Œå™¨
â”‚
â”œâ”€â”€ unit/                        # å•å…ƒæµ‹è¯• (~2,600ä¸ª)
â”‚   â”œâ”€â”€ features/                # ç‰¹å¾å±‚æµ‹è¯• (125+ Alphaå› å­)
â”‚   â”œâ”€â”€ models/                  # æ¨¡å‹å±‚æµ‹è¯• (LightGBM/GRU/Ridge)
â”‚   â”œâ”€â”€ strategies/              # ç­–ç•¥å±‚æµ‹è¯• (åŠ¨é‡/å‡å€¼å›å½’/å¤šå› å­)
â”‚   â”œâ”€â”€ backtest/                # å›æµ‹å¼•æ“æµ‹è¯•
â”‚   â”œâ”€â”€ data/                    # æ•°æ®å±‚æµ‹è¯•
â”‚   â””â”€â”€ ...                      # å…¶ä»–æ¨¡å—æµ‹è¯•
â”‚
â”œâ”€â”€ integration/                 # é›†æˆæµ‹è¯• (134ä¸ª)
â”‚   â”œâ”€â”€ README.md                # é›†æˆæµ‹è¯•è¯´æ˜
â”‚   â”œâ”€â”€ test_end_to_end_workflow.py
â”‚   â”œâ”€â”€ test_database_*.py
â”‚   â””â”€â”€ providers/               # å¤–éƒ¨APIé›†æˆæµ‹è¯•
â”‚
â”œâ”€â”€ performance/                 # æ€§èƒ½æµ‹è¯• (~100ä¸ª)
â”‚   â”œâ”€â”€ README.md                # æ€§èƒ½åŸºå‡†è¯´æ˜
â”‚   â”œâ”€â”€ test_feature_calculation_benchmarks.py
â”‚   â”œâ”€â”€ test_backtest_benchmarks.py
â”‚   â””â”€â”€ run_benchmarks.py        # æ€§èƒ½æµ‹è¯•è¿è¡Œå™¨
â”‚
â””â”€â”€ cli/                         # CLIæµ‹è¯• (142ä¸ª)
    â”œâ”€â”€ README.md                # CLIæµ‹è¯•è¯´æ˜
    â”œâ”€â”€ utils/                   # CLIå·¥å…·æµ‹è¯•
    â””â”€â”€ commands/                # CLIå‘½ä»¤æµ‹è¯•
```

### å¿«é€Ÿå¼€å§‹æµ‹è¯•

**æ—¥å¸¸å¼€å‘ï¼ˆæ¨èï¼‰**:
```bash
cd tests
python run_tests.py --fast  # å¿«é€Ÿå•å…ƒæµ‹è¯• (~38ç§’)
```

**æäº¤å‰æ£€æŸ¥**:
```bash
python run_tests.py --all   # æ‰€æœ‰æµ‹è¯• + è¦†ç›–ç‡ (~4.5åˆ†é’Ÿ)
```

**æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯**: å‚è§ [tests/README.md](../../tests/README.md)

---

## â“ å¸¸è§é—®é¢˜

### Q: å¦‚ä½•æµ‹è¯•å¼‚æ­¥ä»£ç ï¼Ÿ

```python
import pytest

@pytest.mark.asyncio
async def test_async_function():
    result = await fetch_data_async()
    assert result is not None
```

### Q: å¦‚ä½•å¤„ç†æµ‹è¯•æ•°æ®ï¼Ÿ

**å»ºè®®**:
- ä½¿ç”¨fixturesç”Ÿæˆæµ‹è¯•æ•°æ®
- é¿å…ä¾èµ–çœŸå®æ•°æ®åº“
- ä½¿ç”¨å·¥å‚æ¨¡å¼åˆ›å»ºæµ‹è¯•å¯¹è±¡

### Q: æµ‹è¯•è¦†ç›–ç‡ä¸å¤Ÿæ€ä¹ˆåŠï¼Ÿ

**æ­¥éª¤**:
1. è¿è¡Œ `pytest --cov=src --cov-report=html`
2. æ‰“å¼€ `htmlcov/index.html` æŸ¥çœ‹æœªè¦†ç›–ä»£ç 
3. ä¸ºæœªè¦†ç›–çš„è¾¹ç•Œæƒ…å†µæ·»åŠ æµ‹è¯•

### Q: å¦‚ä½•æµ‹è¯•éš¾ä»¥å¤ç°çš„Bugï¼Ÿ

**æ–¹æ³•**:
1. æ·»åŠ å›å½’æµ‹è¯•ç”¨ä¾‹
2. ä½¿ç”¨å›ºå®šéšæœºç§å­
3. Mockå¤–éƒ¨ä¾èµ–

---

**æ–‡æ¡£ç‰ˆæœ¬**: v3.0.0
**ç»´æŠ¤å›¢é˜Ÿ**: Quant Team
**æœ€åæ›´æ–°**: 2026-02-01
