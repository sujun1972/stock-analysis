# æ¨¡å‹ä½¿ç”¨æŒ‡å—

## ğŸ“‹ ç›®å½•

- [ç®€ä»‹](#ç®€ä»‹)
- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [åŸºç¡€æ¨¡å‹](#åŸºç¡€æ¨¡å‹)
- [æ¨¡å‹é›†æˆ](#æ¨¡å‹é›†æˆ)
- [æ¨¡å‹ç®¡ç†](#æ¨¡å‹ç®¡ç†)
- [é«˜çº§åŠŸèƒ½](#é«˜çº§åŠŸèƒ½)
- [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)
- [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)

---

## ç®€ä»‹

Core æ¨¡å—æä¾›äº†å®Œæ•´çš„é‡åŒ–äº¤æ˜“æ¨¡å‹æ¡†æ¶ï¼ŒåŒ…æ‹¬ï¼š

- **3ç§åŸºç¡€æ¨¡å‹**ï¼šRidgeã€LightGBMã€GRU
- **3ç§é›†æˆæ–¹æ³•**ï¼šåŠ æƒå¹³å‡ã€æŠ•ç¥¨æ³•ã€Stacking
- **æ¨¡å‹æ³¨å†Œè¡¨**ï¼šç‰ˆæœ¬ç®¡ç†ã€å…ƒæ•°æ®è¿½è¸ª
- **è‡ªåŠ¨è°ƒä¼˜**ï¼šè¶…å‚æ•°ä¼˜åŒ–

### æ¨¡å‹æ€§èƒ½å¯¹æ¯”

| æ¨¡å‹ | è®­ç»ƒé€Ÿåº¦ | é¢„æµ‹æ€§èƒ½ | å¯è§£é‡Šæ€§ | é€‚ç”¨åœºæ™¯ |
|------|----------|----------|----------|----------|
| Ridge | â­â­â­â­â­ | â­â­â­ | â­â­â­â­â­ | åŸºå‡†æ¨¡å‹ã€ç‰¹å¾é€‰æ‹© |
| LightGBM | â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­ | ç”Ÿäº§ç¯å¢ƒã€å¤æ‚ç‰¹å¾ |
| GRU | â­â­ | â­â­â­â­ | â­â­ | æ—¶åºæ•°æ®ã€æ·±åº¦å­¦ä¹  |
| é›†æˆæ¨¡å‹ | â­â­â­ | â­â­â­â­â­ | â­â­â­ | è¿½æ±‚æœ€ä¼˜æ€§èƒ½ |

---

## å¿«é€Ÿå¼€å§‹

### å®‰è£…ä¾èµ–

```bash
# å¿…éœ€ä¾èµ–
pip install pandas numpy scikit-learn lightgbm loguru

# å¯é€‰ä¾èµ–ï¼ˆGRUæ¨¡å‹ï¼‰
pip install torch

# å¯é€‰ä¾èµ–ï¼ˆè´å¶æ–¯ä¼˜åŒ–ï¼‰
pip install scikit-optimize
```

### 30ç§’å¿«é€Ÿä¸Šæ‰‹

```python
from models import LightGBMStockModel
import pandas as pd
import numpy as np

# 1. å‡†å¤‡æ•°æ®
X_train = pd.DataFrame(np.random.randn(1000, 30))
y_train = pd.Series(np.random.randn(1000))

# 2. è®­ç»ƒæ¨¡å‹
model = LightGBMStockModel()
model.train(X_train, y_train)

# 3. é¢„æµ‹
X_test = pd.DataFrame(np.random.randn(100, 30))
predictions = model.predict(X_test)

print(f"é¢„æµ‹ç»“æœ: {predictions[:5]}")
```

---

## åŸºç¡€æ¨¡å‹

### 1. Ridge å›å½’æ¨¡å‹

**ç‰¹ç‚¹**ï¼šç®€å•å¿«é€Ÿã€ç¨³å®šå¯é ã€é€‚åˆåšåŸºå‡†

```python
from models import RidgeStockModel

# åˆ›å»ºæ¨¡å‹
model = RidgeStockModel(alpha=1.0)

# è®­ç»ƒ
model.train(X_train, y_train)

# é¢„æµ‹
predictions = model.predict(X_test)

# ä¿å­˜/åŠ è½½
model.save('ridge_model.pkl')
loaded_model = RidgeStockModel.load('ridge_model.pkl')
```

**å‚æ•°è¯´æ˜**ï¼š

- `alpha`: L2æ­£åˆ™åŒ–ç³»æ•°ï¼ˆé»˜è®¤1.0ï¼‰
  - è¾ƒå¤§å€¼ï¼šé˜²æ­¢è¿‡æ‹Ÿåˆï¼Œæ¨¡å‹æ›´ä¿å®ˆ
  - è¾ƒå°å€¼ï¼šæ‹Ÿåˆæ›´å¥½ï¼Œå¯èƒ½è¿‡æ‹Ÿåˆ

**ä½¿ç”¨å»ºè®®**ï¼š

- âœ… é€‚åˆï¼šç‰¹å¾æ•°é‡ > æ ·æœ¬æ•°é‡
- âœ… é€‚åˆï¼šéœ€è¦å¿«é€Ÿè®­ç»ƒå’Œé¢„æµ‹
- âœ… é€‚åˆï¼šä½œä¸ºåŸºå‡†æ¨¡å‹å¯¹æ¯”
- âŒ ä¸é€‚åˆï¼šç‰¹å¾é—´å­˜åœ¨å¤æ‚éçº¿æ€§å…³ç³»

---

### 2. LightGBM æ¨¡å‹

**ç‰¹ç‚¹**ï¼šæ€§èƒ½ä¼˜ç§€ã€è®­ç»ƒå¿«é€Ÿã€ç”Ÿäº§é¦–é€‰

```python
from models import LightGBMStockModel

# åˆ›å»ºæ¨¡å‹
model = LightGBMStockModel(
    n_estimators=100,
    learning_rate=0.05,
    max_depth=5,
    num_leaves=31,
    min_child_samples=20,
    subsample=0.8,
    colsample_bytree=0.8
)

# è®­ç»ƒï¼ˆå¸¦éªŒè¯é›†å’Œæ—©åœï¼‰
model.train(
    X_train, y_train,
    X_valid, y_valid,
    early_stopping_rounds=20,
    verbose_eval=10
)

# é¢„æµ‹
predictions = model.predict(X_test)

# ç‰¹å¾é‡è¦æ€§
importance = model.get_feature_importance('gain', top_n=10)
print(importance)
```

**å…³é”®å‚æ•°**ï¼š

| å‚æ•° | è¯´æ˜ | æ¨èèŒƒå›´ |
|------|------|----------|
| `n_estimators` | æ ‘çš„æ•°é‡ | 100-500 |
| `learning_rate` | å­¦ä¹ ç‡ | 0.01-0.1 |
| `num_leaves` | å¶å­èŠ‚ç‚¹æ•° | 15-127 |
| `max_depth` | æœ€å¤§æ·±åº¦ | 3-9 |
| `min_child_samples` | å¶å­æœ€å°æ ·æœ¬æ•° | 20-50 |
| `subsample` | è¡Œé‡‡æ ·æ¯”ä¾‹ | 0.6-1.0 |
| `colsample_bytree` | åˆ—é‡‡æ ·æ¯”ä¾‹ | 0.6-1.0 |

**è°ƒä¼˜å»ºè®®**ï¼š

```python
# é˜²æ­¢è¿‡æ‹Ÿåˆ
model = LightGBMStockModel(
    max_depth=3,              # é™åˆ¶æ·±åº¦
    num_leaves=15,            # å‡å°‘å¶å­æ•°
    min_child_samples=50,     # å¢åŠ æœ€å°æ ·æœ¬æ•°
    reg_alpha=0.1,            # L1æ­£åˆ™åŒ–
    reg_lambda=0.1            # L2æ­£åˆ™åŒ–
)

# è¿½æ±‚æ€§èƒ½
model = LightGBMStockModel(
    n_estimators=500,
    learning_rate=0.01,       # é™ä½å­¦ä¹ ç‡
    num_leaves=127,           # å¢åŠ å¤æ‚åº¦
    max_depth=-1              # ä¸é™åˆ¶æ·±åº¦
)
```

---

### 3. GRU æ·±åº¦å­¦ä¹ æ¨¡å‹

**ç‰¹ç‚¹**ï¼šå¤„ç†æ—¶åºã€æ•è·é•¿æœŸä¾èµ–

```python
from models import GRUStockModel

# å‡†å¤‡æ—¶åºæ•°æ® (n_samples, sequence_length, n_features)
# æ³¨æ„ï¼šéœ€è¦é‡å¡‘æ•°æ®æ ¼å¼

# åˆ›å»ºæ¨¡å‹
model = GRUStockModel(
    input_size=20,           # ç‰¹å¾æ•°
    hidden_size=64,          # éšè—å±‚å¤§å°
    num_layers=2,            # GRUå±‚æ•°
    dropout=0.2,             # Dropoutæ¯”ä¾‹
    sequence_length=10       # åºåˆ—é•¿åº¦
)

# è®­ç»ƒ
model.train(
    X_train, y_train,
    X_valid, y_valid,
    epochs=50,
    batch_size=32,
    learning_rate=0.001
)

# é¢„æµ‹
predictions = model.predict(X_test)
```

**ä½¿ç”¨åœºæ™¯**ï¼š

- âœ… è‚¡ç¥¨ä»·æ ¼æ—¶åºé¢„æµ‹
- âœ… é•¿æœŸè¶‹åŠ¿å»ºæ¨¡
- âœ… æ•°æ®é‡å……è¶³ï¼ˆ>10,000æ ·æœ¬ï¼‰
- âŒ ç®€å•æˆªé¢æ•°æ®ï¼ˆæ¨èç”¨LightGBMï¼‰

---

## æ¨¡å‹é›†æˆ

### ä¸ºä»€ä¹ˆä½¿ç”¨é›†æˆï¼Ÿ

- **æå‡æ€§èƒ½**ï¼šé€šå¸¸æå‡ 5-15% IC
- **é™ä½æ–¹å·®**ï¼šå‡å°‘å•æ¨¡å‹æ³¢åŠ¨
- **æ¨¡å‹äº’è¡¥**ï¼šèåˆä¸åŒæ¨¡å‹ä¼˜åŠ¿

### 1. åŠ æƒå¹³å‡é›†æˆ

**æœ€å¸¸ç”¨ã€æœ€ç®€å•**

```python
from models import WeightedAverageEnsemble

# è®­ç»ƒåŸºç¡€æ¨¡å‹
ridge = RidgeStockModel(alpha=1.0)
ridge.train(X_train, y_train)

lgb = LightGBMStockModel()
lgb.train(X_train, y_train, X_valid, y_valid)

# æ–¹æ³•1: ç­‰æƒé‡
ensemble = WeightedAverageEnsemble([ridge, lgb])

# æ–¹æ³•2: è‡ªå®šä¹‰æƒé‡
ensemble = WeightedAverageEnsemble(
    [ridge, lgb],
    weights=[0.3, 0.7]  # LightGBMæƒé‡æ›´é«˜
)

# æ–¹æ³•3: è‡ªåŠ¨ä¼˜åŒ–æƒé‡ï¼ˆæ¨èï¼ï¼‰
ensemble = WeightedAverageEnsemble([ridge, lgb])
ensemble.optimize_weights(X_valid, y_valid, metric='ic')

# é¢„æµ‹
predictions = ensemble.predict(X_test)
```

---

### 2. æŠ•ç¥¨æ³•é›†æˆ

**é€‚åˆé€‰è‚¡ç­–ç•¥**

```python
from models import VotingEnsemble

# åˆ›å»ºæŠ•ç¥¨é›†æˆ
ensemble = VotingEnsemble(
    models=[ridge, lgb1, lgb2],
    voting_weights=[1.0, 1.5, 1.0]  # lgb1æƒé‡æ›´é«˜
)

# æ–¹æ³•1: é€‰æ‹©Top Nè‚¡ç¥¨
top_50_indices = ensemble.select_top_n(X_test, top_n=50)

# æ–¹æ³•2: è·å–æŠ•ç¥¨åˆ†æ•°
scores = ensemble.predict(X_test)

# æ–¹æ³•3: åŒæ—¶è·å–ç´¢å¼•å’Œåˆ†æ•°
indices, scores = ensemble.select_top_n(
    X_test, top_n=50, return_scores=True
)
```

**åº”ç”¨ï¼šé‡åŒ–é€‰è‚¡**

```python
# æ¯å‘¨é€‰è‚¡ç¤ºä¾‹
def weekly_stock_selection(X, ensemble, n_stocks=30):
    """ä½¿ç”¨æŠ•ç¥¨æ³•é€‰è‚¡"""
    top_indices = ensemble.select_top_n(X, top_n=n_stocks)
    return top_indices

# ä½¿ç”¨
selected_stocks = weekly_stock_selection(X_latest, ensemble)
print(f"æœ¬å‘¨é€‰è‚¡: {selected_stocks}")
```

---

### 3. Stacking é›†æˆ

**æ€§èƒ½æœ€ä¼˜ï¼Œéœ€è¦ç‹¬ç«‹éªŒè¯é›†**

```python
from models import StackingEnsemble, RidgeStockModel

# æ•°æ®åˆ†å‰²ï¼ˆé‡è¦ï¼ï¼‰
# 60% è®­ç»ƒ | 20% éªŒè¯ï¼ˆè®­ç»ƒå…ƒå­¦ä¹ å™¨ï¼‰ | 20% æµ‹è¯•
X_train, y_train = X[:600], y[:600]
X_valid, y_valid = X[600:800], y[600:800]
X_test, y_test = X[800:], y[800:]

# è®­ç»ƒåŸºç¡€æ¨¡å‹ï¼ˆåœ¨è®­ç»ƒé›†ä¸Šï¼‰
base_models = [ridge, lgb1, lgb2]
for model in base_models:
    model.train(X_train, y_train)

# åˆ›å»ºStacking
ensemble = StackingEnsemble(
    base_models=base_models,
    meta_learner=RidgeStockModel(alpha=0.5),
    use_original_features=True  # ç»“åˆåŸå§‹ç‰¹å¾
)

# è®­ç»ƒå…ƒå­¦ä¹ å™¨ï¼ˆåœ¨éªŒè¯é›†ä¸Šï¼‰
ensemble.train_meta_learner(X_train, y_train, X_valid, y_valid)

# é¢„æµ‹
predictions = ensemble.predict(X_test)
```

**å…³é”®è¦ç‚¹**ï¼š

- âš ï¸ å¿…é¡»ä½¿ç”¨ç‹¬ç«‹éªŒè¯é›†è®­ç»ƒå…ƒå­¦ä¹ å™¨
- âš ï¸ é˜²æ­¢æ•°æ®æ³„éœ²
- âœ… æ€§èƒ½é€šå¸¸æ¯”åŠ æƒå¹³å‡é«˜ 1-3%

---

## æ¨¡å‹ç®¡ç†

### æ¨¡å‹æ³¨å†Œè¡¨

**ç»Ÿä¸€ç®¡ç†æ‰€æœ‰æ¨¡å‹ç‰ˆæœ¬**

```python
from models import ModelRegistry

# åˆ›å»ºæ³¨å†Œè¡¨
registry = ModelRegistry(base_dir='model_registry')

# ä¿å­˜æ¨¡å‹
registry.save_model(
    model=my_model,
    name='lightgbm_v1',
    metadata={
        'train_ic': 0.95,
        'test_ic': 0.92,
        'train_date': '2024-01-01'
    },
    model_type='lightgbm',
    description='ç”Ÿäº§ç¯å¢ƒæ¨¡å‹'
)

# åŠ è½½æœ€æ–°ç‰ˆæœ¬
model, metadata = registry.load_model('lightgbm_v1')

# åŠ è½½æŒ‡å®šç‰ˆæœ¬
model, metadata = registry.load_model('lightgbm_v1', version=2)

# æŸ¥çœ‹æ¨¡å‹å†å²
history = registry.get_model_history('lightgbm_v1')
print(history)

# åˆ—å‡ºæ‰€æœ‰æ¨¡å‹
models = registry.list_models()
print(models)

# å¯¹æ¯”ç‰ˆæœ¬
comparison = registry.compare_versions('lightgbm_v1', 1, 2)
print(comparison)

# å¯¼å‡ºæ¨¡å‹
registry.export_model('lightgbm_v1', version=None, output_path='exports/')
```

**å…ƒæ•°æ®è¿½è¸ª**ï¼š

```python
# ä¿å­˜æ—¶è‡ªåŠ¨è®°å½•
metadata = {
    'model_name': 'lightgbm_v1',
    'version': 1,
    'timestamp': '2024-01-15T10:30:00',
    'model_type': 'lightgbm',
    'feature_names': ['feature_0', 'feature_1', ...],
    'performance_metrics': {
        'train_ic': 0.95,
        'test_ic': 0.92
    },
    'training_config': {...}
}
```

---

## é«˜çº§åŠŸèƒ½

### 1. è‡ªåŠ¨è¶…å‚æ•°è°ƒä¼˜

```python
# LightGBM è‡ªåŠ¨è°ƒä¼˜
model = LightGBMStockModel()

best_model, results = model.auto_tune(
    X_train, y_train,
    X_valid, y_valid,
    metric='ic',
    method='grid',  # 'grid' æˆ– 'random'
    n_trials=20
)

print(f"æœ€ä½³å‚æ•°: {results['best_params']}")
print(f"æœ€ä½³IC: {results['best_score']:.6f}")

# ä½¿ç”¨æœ€ä½³æ¨¡å‹é¢„æµ‹
predictions = best_model.predict(X_test)
```

**è‡ªå®šä¹‰æœç´¢ç©ºé—´**ï¼š

```python
param_grid = {
    'learning_rate': [0.01, 0.03, 0.05, 0.1],
    'num_leaves': [15, 31, 63],
    'max_depth': [3, 5, 7],
    'n_estimators': [100, 200, 300]
}

best_model, results = model.auto_tune(
    X_train, y_train, X_valid, y_valid,
    param_grid=param_grid,
    metric='ic'
)
```

---

### 2. å®Œæ•´è®­ç»ƒæµæ°´çº¿

```python
from models import ModelRegistry, LightGBMStockModel, WeightedAverageEnsemble

class ProductionPipeline:
    """ç”Ÿäº§ç¯å¢ƒè®­ç»ƒæµæ°´çº¿"""

    def __init__(self):
        self.registry = ModelRegistry()

    def train_and_deploy(self, X_train, y_train, X_valid, y_valid):
        """è®­ç»ƒå¹¶éƒ¨ç½²æ¨¡å‹"""

        # 1. è®­ç»ƒå¤šä¸ªæ¨¡å‹
        models = {}

        # RidgeåŸºå‡†
        ridge = RidgeStockModel(alpha=1.0)
        ridge.train(X_train, y_train)
        models['ridge'] = ridge

        # LightGBMï¼ˆè‡ªåŠ¨è°ƒä¼˜ï¼‰
        lgb = LightGBMStockModel()
        lgb_tuned, _ = lgb.auto_tune(
            X_train, y_train, X_valid, y_valid,
            metric='ic', n_trials=20
        )
        models['lightgbm'] = lgb_tuned

        # 2. åˆ›å»ºé›†æˆ
        ensemble = WeightedAverageEnsemble(list(models.values()))
        ensemble.optimize_weights(X_valid, y_valid, metric='ic')

        # 3. è¯„ä¼°
        y_pred = ensemble.predict(X_valid)
        ic = np.corrcoef(y_pred, y_valid)[0, 1]

        # 4. ä¿å­˜åˆ°æ³¨å†Œè¡¨
        self.registry.save_model(
            model=ensemble,
            name='ensemble_prod',
            metadata={'valid_ic': ic},
            model_type='ensemble',
            description='ç”Ÿäº§é›†æˆæ¨¡å‹'
        )

        return ensemble, ic

# ä½¿ç”¨
pipeline = ProductionPipeline()
model, ic = pipeline.train_and_deploy(X_train, y_train, X_valid, y_valid)
```

---

## æœ€ä½³å®è·µ

### 1. æ•°æ®åˆ†å‰²

```python
# æ¨èåˆ†å‰²æ¯”ä¾‹
# è®­ç»ƒé›†: 60%
# éªŒè¯é›†: 20% (è°ƒå‚ã€æ—©åœ)
# æµ‹è¯•é›†: 20% (æœ€ç»ˆè¯„ä¼°)

n = len(X)
train_end = int(n * 0.6)
valid_end = int(n * 0.8)

X_train, y_train = X[:train_end], y[:train_end]
X_valid, y_valid = X[train_end:valid_end], y[train_end:valid_end]
X_test, y_test = X[valid_end:], y[valid_end:]
```

### 2. æ¨¡å‹é€‰æ‹©æµç¨‹

```
1. ä»RidgeåŸºå‡†å¼€å§‹
   â†“
2. å°è¯•LightGBMï¼ˆé€šå¸¸æå‡æ˜æ˜¾ï¼‰
   â†“
3. è°ƒä¼˜LightGBMå‚æ•°
   â†“
4. å°è¯•é›†æˆï¼ˆåŠ æƒå¹³å‡ï¼‰
   â†“
5. å¦‚æœæ•°æ®å……è¶³ï¼Œå°è¯•Stacking
```

### 3. é˜²æ­¢è¿‡æ‹Ÿåˆ

```python
# ç­–ç•¥1: ä½¿ç”¨æ­£åˆ™åŒ–
model = LightGBMStockModel(
    reg_alpha=0.1,
    reg_lambda=0.1
)

# ç­–ç•¥2: é™åˆ¶å¤æ‚åº¦
model = LightGBMStockModel(
    max_depth=5,
    num_leaves=31,
    min_child_samples=20
)

# ç­–ç•¥3: æ—©åœ
model.train(
    X_train, y_train,
    X_valid, y_valid,
    early_stopping_rounds=20
)

# ç­–ç•¥4: ç›‘æ§è®­ç»ƒ/éªŒè¯é›†å·®å¼‚
if train_ic - valid_ic > 0.1:
    print("è­¦å‘Šï¼šå¯èƒ½è¿‡æ‹Ÿåˆï¼")
```

### 4. æ€§èƒ½è¯„ä¼°

```python
# å¤šä¸ªæŒ‡æ ‡ç»¼åˆè¯„ä¼°
def evaluate_model(model, X_test, y_test):
    y_pred = model.predict(X_test)

    # IC (Information Coefficient)
    ic = np.corrcoef(y_pred, y_test)[0, 1]

    # Rank IC
    rank_ic = pd.Series(y_test.values).corr(
        pd.Series(y_pred), method='spearman'
    )

    # MSE
    mse = np.mean((y_test - y_pred) ** 2)

    # Top 20% æ”¶ç›Š
    top_20_mask = y_pred >= np.quantile(y_pred, 0.8)
    top_20_return = y_test[top_20_mask].mean()

    return {
        'ic': ic,
        'rank_ic': rank_ic,
        'mse': mse,
        'top_20_return': top_20_return
    }
```

---

## å¸¸è§é—®é¢˜

### Q1: å¦‚ä½•é€‰æ‹©æ¨¡å‹ï¼Ÿ

**ç­”**ï¼šæŒ‰æ­¤é¡ºåºå°è¯•ï¼š

1. **Ridge**ï¼šå¿«é€ŸåŸºå‡†ï¼Œäº†è§£æ•°æ®
2. **LightGBM**ï¼šç”Ÿäº§é¦–é€‰ï¼Œæ€§èƒ½ä¼˜ç§€
3. **é›†æˆ**ï¼šè¿½æ±‚æè‡´æ€§èƒ½
4. **GRU**ï¼šä»…ç”¨äºæ—¶åºæ•°æ®

### Q2: é›†æˆæ¨¡å‹æ•ˆæœä¸å¥½ï¼Ÿ

**å¯èƒ½åŸå› **ï¼š

- åŸºç¡€æ¨¡å‹é«˜åº¦ç›¸å…³ï¼ˆé¢„æµ‹ç›¸å…³æ€§>0.95ï¼‰
- åŸºç¡€æ¨¡å‹è´¨é‡å·®
- æƒé‡è®¾ç½®ä¸åˆç†

**è§£å†³æ–¹æ¡ˆ**ï¼š

```python
# æ£€æŸ¥æ¨¡å‹ç›¸å…³æ€§
pred1 = model1.predict(X_test)
pred2 = model2.predict(X_test)
corr = np.corrcoef(pred1, pred2)[0, 1]

if corr > 0.95:
    print("æ¨¡å‹è¿‡äºç›¸ä¼¼ï¼Œé›†æˆæ”¶ç›Šæœ‰é™")
else:
    print(f"æ¨¡å‹ç›¸å…³æ€§={corr:.2f}ï¼Œé€‚åˆé›†æˆ")

# ä½¿ç”¨è‡ªåŠ¨æƒé‡ä¼˜åŒ–
ensemble.optimize_weights(X_valid, y_valid, metric='ic')
```

### Q3: è®­ç»ƒæ—¶é—´å¤ªé•¿ï¼Ÿ

**ä¼˜åŒ–æ–¹æ³•**ï¼š

```python
# 1. å‡å°‘ç‰¹å¾æ•°é‡
from sklearn.feature_selection import SelectKBest
selector = SelectKBest(k=50)
X_selected = selector.fit_transform(X, y)

# 2. å‡å°‘æ ·æœ¬æ•°é‡
X_sample = X.sample(frac=0.5)

# 3. ä½¿ç”¨æ›´å¿«çš„å‚æ•°
model = LightGBMStockModel(
    n_estimators=50,  # å‡å°‘æ ‘æ•°é‡
    max_depth=3       # é™åˆ¶æ·±åº¦
)

# 4. å¹¶è¡Œè®­ç»ƒï¼ˆå¦‚æœæœ‰å¤šä¸ªæ¨¡å‹ï¼‰
from joblib import Parallel, delayed

models = Parallel(n_jobs=-1)(
    delayed(train_single_model)(params)
    for params in param_list
)
```

### Q4: å¦‚ä½•åœ¨ç”Ÿäº§ç¯å¢ƒä½¿ç”¨ï¼Ÿ

**å®Œæ•´æµç¨‹**ï¼š

```python
# 1. è®­ç»ƒé˜¶æ®µ
registry = ModelRegistry()

# è®­ç»ƒå¹¶ä¿å­˜
model = LightGBMStockModel()
model.train(X_train, y_train, X_valid, y_valid)

ic = np.corrcoef(model.predict(X_test), y_test)[0, 1]

registry.save_model(
    model=model,
    name='prod_model',
    metadata={'test_ic': ic, 'date': '2024-01-15'},
    model_type='lightgbm'
)

# 2. é¢„æµ‹é˜¶æ®µï¼ˆå¦ä¸€ä¸ªè„šæœ¬ï¼‰
registry = ModelRegistry()
model, metadata = registry.load_model('prod_model')

# éªŒè¯å…ƒæ•°æ®
print(f"æ¨¡å‹ç‰ˆæœ¬: {metadata.version}")
print(f"è®­ç»ƒæ—¥æœŸ: {metadata.timestamp}")
print(f"æµ‹è¯•IC: {metadata.performance_metrics['test_ic']}")

# é¢„æµ‹
predictions = model.predict(X_new)
```

---

## ç¤ºä¾‹ä»£ç 

å®Œæ•´ç¤ºä¾‹ä½äº `core/examples/` ç›®å½•ï¼š

1. **[model_basic_usage.py](../examples/model_basic_usage.py)** - åŸºç¡€æ¨¡å‹ä½¿ç”¨
2. **[ensemble_example.py](../examples/ensemble_example.py)** - é›†æˆæ¨¡å‹ç¤ºä¾‹
3. **[model_training_pipeline.py](../examples/model_training_pipeline.py)** - å®Œæ•´è®­ç»ƒæµç¨‹
4. **[model_comparison_demo.py](../examples/model_comparison_demo.py)** - æ¨¡å‹å¯¹æ¯”

è¿è¡Œç¤ºä¾‹ï¼š

```bash
cd core/examples
python model_basic_usage.py
python ensemble_example.py
python model_training_pipeline.py
python model_comparison_demo.py
```

---

## å‚è€ƒèµ„æ–™

- **é›†æˆå­¦ä¹ æŒ‡å—**: [ENSEMBLE_GUIDE.md](ENSEMBLE_GUIDE.md)
- **å› å­åˆ†ææŒ‡å—**: [FACTOR_ANALYSIS_GUIDE.md](FACTOR_ANALYSIS_GUIDE.md)
- **å¼€å‘è·¯çº¿å›¾**: [DEVELOPMENT_ROADMAP.md](../DEVELOPMENT_ROADMAP.md)

---

**ç‰ˆæœ¬**: v1.0
**æ›´æ–°æ—¥æœŸ**: 2026-01-29
**è´¡çŒ®è€…**: Claude Code
