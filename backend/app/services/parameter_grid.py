"""
å‚æ•°ç½‘æ ¼ç”Ÿæˆå™¨
ç”Ÿæˆç”¨äºæ‰¹é‡å®éªŒçš„å‚æ•°ç»„åˆ
"""

from typing import List, Dict, Any, Optional
from itertools import product
import random
import hashlib
import json
from loguru import logger


class ParameterGrid:
    """
    å‚æ•°ç½‘æ ¼ç”Ÿæˆå™¨
    æ”¯æŒä¸‰ç§ç­–ç•¥: grid (ç½‘æ ¼æœç´¢), random (éšæœºé‡‡æ ·), bayesian (è´å¶æ–¯ä¼˜åŒ–)
    """

    def __init__(self, param_space: Dict[str, Any]):
        """
        åˆå§‹åŒ–å‚æ•°ç½‘æ ¼

        Args:
            param_space: å‚æ•°ç©ºé—´å®šä¹‰
                {
                    'symbols': ['000001', '600000'],
                    'date_ranges': [['20200101', '20231231']],
                    'model_types': ['lightgbm', 'gru'],
                    'target_periods': [5, 10, 20],
                    'scaler_types': ['robust', 'standard'],
                    'balance_samples': [False, True],
                    'lightgbm': {
                        'num_leaves': [31, 63, 127],
                        'learning_rate': [0.01, 0.05, 0.1]
                    },
                    'gru': {
                        'hidden_size': [32, 64, 128],
                        'num_layers': [1, 2, 3]
                    }
                }
        """
        self.param_space = param_space
        self._validate_param_space()

    def _validate_param_space(self):
        """éªŒè¯å‚æ•°ç©ºé—´å®šä¹‰"""
        required_keys = ['symbols', 'model_types']

        for key in required_keys:
            if key not in self.param_space:
                raise ValueError(f"å‚æ•°ç©ºé—´ç¼ºå°‘å¿…éœ€å­—æ®µ: {key}")

            if not isinstance(self.param_space[key], list) or len(self.param_space[key]) == 0:
                raise ValueError(f"å­—æ®µ {key} å¿…é¡»æ˜¯éç©ºåˆ—è¡¨")

    def generate(
        self,
        strategy: str = 'grid',
        max_experiments: Optional[int] = None,
        random_seed: int = 42
    ) -> List[Dict[str, Any]]:
        """
        ç”Ÿæˆå‚æ•°ç»„åˆ

        Args:
            strategy: ç”Ÿæˆç­–ç•¥ ('grid', 'random', 'bayesian')
            max_experiments: æœ€å¤§å®éªŒæ•°é‡ï¼ˆä»…å¯¹randomå’Œbayesianæœ‰æ•ˆï¼‰
            random_seed: éšæœºç§å­ï¼ˆç”¨äºå¯å¤ç°æ€§ï¼‰

        Returns:
            å‚æ•°ç»„åˆåˆ—è¡¨
        """
        logger.info(f"ğŸ² ä½¿ç”¨ {strategy} ç­–ç•¥ç”Ÿæˆå‚æ•°ç»„åˆ...")

        if strategy == 'grid':
            configs = self._grid_search()
        elif strategy == 'random':
            configs = self._random_search(max_experiments or 100, random_seed)
        elif strategy == 'bayesian':
            # è´å¶æ–¯ä¼˜åŒ–éœ€è¦è¿­ä»£è®­ç»ƒï¼Œè¿™é‡Œå…ˆç”¨éšæœºé‡‡æ ·æ›¿ä»£
            logger.warning("è´å¶æ–¯ä¼˜åŒ–æš‚æœªå®ç°ï¼Œä½¿ç”¨éšæœºé‡‡æ ·ä»£æ›¿")
            configs = self._random_search(max_experiments or 100, random_seed)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ç­–ç•¥: {strategy}")

        # ä¸ºæ¯ä¸ªé…ç½®ç”Ÿæˆå”¯ä¸€å“ˆå¸Œ
        for config in configs:
            config['experiment_hash'] = self._generate_hash(config)

        logger.info(f"âœ… ç”Ÿæˆäº† {len(configs)} ä¸ªå‚æ•°ç»„åˆ")
        return configs

    def _grid_search(self) -> List[Dict[str, Any]]:
        """å®Œå…¨ç½‘æ ¼æœç´¢ï¼ˆæ‰€æœ‰å‚æ•°ç»„åˆï¼‰"""

        configs = []

        # æå–é€šç”¨å‚æ•°
        symbols = self.param_space.get('symbols', [])
        date_ranges = self.param_space.get('date_ranges', [['20200101', '20231231']])
        model_types = self.param_space.get('model_types', ['lightgbm'])
        target_periods = self.param_space.get('target_periods', [5])
        scaler_types = self.param_space.get('scaler_types', ['robust'])
        balance_samples_options = self.param_space.get('balance_samples', [False])

        # éå†æ‰€æœ‰åŸºç¡€ç»„åˆ
        for symbol in symbols:
            for date_range in date_ranges:
                for model_type in model_types:
                    for target_period in target_periods:
                        for scaler_type in scaler_types:
                            for balance_samples in balance_samples_options:

                                # åŸºç¡€é…ç½®
                                base_config = {
                                    'symbol': symbol,
                                    'start_date': date_range[0],
                                    'end_date': date_range[1],
                                    'model_type': model_type,
                                    'target_period': target_period,
                                    'scaler_type': scaler_type,
                                    'balance_samples': balance_samples
                                }

                                # æ ¹æ®æ¨¡å‹ç±»å‹æ·»åŠ è¶…å‚æ•°ç»„åˆ
                                if model_type == 'lightgbm':
                                    lgb_configs = self._generate_lightgbm_configs(base_config)
                                    configs.extend(lgb_configs)
                                elif model_type == 'gru':
                                    gru_configs = self._generate_gru_configs(base_config)
                                    configs.extend(gru_configs)
                                else:
                                    # æœªçŸ¥æ¨¡å‹ç±»å‹ï¼Œåªæ·»åŠ åŸºç¡€é…ç½®
                                    configs.append(base_config)

        return configs

    def _generate_lightgbm_configs(self, base_config: Dict) -> List[Dict]:
        """ç”ŸæˆLightGBMè¶…å‚æ•°ç»„åˆ"""

        lgb_params = self.param_space.get('lightgbm', {})

        if not lgb_params:
            # å¦‚æœæ²¡æœ‰å®šä¹‰è¶…å‚æ•°ï¼Œä½¿ç”¨é»˜è®¤å€¼
            return [base_config]

        # æå–è¶…å‚æ•°
        num_leaves_list = lgb_params.get('num_leaves', [31])
        learning_rate_list = lgb_params.get('learning_rate', [0.05])
        n_estimators_list = lgb_params.get('n_estimators', [100])
        max_depth_list = lgb_params.get('max_depth', [-1])

        configs = []
        for num_leaves, lr, n_est, max_depth in product(
            num_leaves_list, learning_rate_list, n_estimators_list, max_depth_list
        ):
            config = base_config.copy()
            config['model_params'] = {
                'num_leaves': num_leaves,
                'learning_rate': lr,
                'n_estimators': n_est,
                'max_depth': max_depth
            }
            configs.append(config)

        return configs

    def _generate_gru_configs(self, base_config: Dict) -> List[Dict]:
        """ç”ŸæˆGRUè¶…å‚æ•°ç»„åˆ"""

        gru_params = self.param_space.get('gru', {})

        if not gru_params:
            return [base_config]

        # æå–è¶…å‚æ•°
        hidden_size_list = gru_params.get('hidden_size', [64])
        num_layers_list = gru_params.get('num_layers', [2])
        dropout_list = gru_params.get('dropout', [0.2])
        seq_length_list = gru_params.get('seq_length', [20])
        epochs_list = gru_params.get('epochs', [100])

        configs = []
        for hidden_size, num_layers, dropout, seq_length, epochs in product(
            hidden_size_list, num_layers_list, dropout_list, seq_length_list, epochs_list
        ):
            config = base_config.copy()
            config['seq_length'] = seq_length
            config['epochs'] = epochs
            config['model_params'] = {
                'hidden_size': hidden_size,
                'num_layers': num_layers,
                'dropout': dropout
            }
            configs.append(config)

        return configs

    def _random_search(self, n_samples: int, random_seed: int) -> List[Dict[str, Any]]:
        """éšæœºé‡‡æ ·å‚æ•°ç»„åˆ"""

        random.seed(random_seed)
        configs = []

        symbols = self.param_space.get('symbols', [])
        date_ranges = self.param_space.get('date_ranges', [['20200101', '20231231']])
        model_types = self.param_space.get('model_types', ['lightgbm'])
        target_periods = self.param_space.get('target_periods', [5])
        scaler_types = self.param_space.get('scaler_types', ['robust'])
        balance_samples_options = self.param_space.get('balance_samples', [False])

        for _ in range(n_samples):
            # éšæœºé€‰æ‹©åŸºç¡€å‚æ•°
            symbol = random.choice(symbols)
            date_range = random.choice(date_ranges)
            model_type = random.choice(model_types)
            target_period = random.choice(target_periods)
            scaler_type = random.choice(scaler_types)
            balance_samples = random.choice(balance_samples_options)

            config = {
                'symbol': symbol,
                'start_date': date_range[0],
                'end_date': date_range[1],
                'model_type': model_type,
                'target_period': target_period,
                'scaler_type': scaler_type,
                'balance_samples': balance_samples
            }

            # éšæœºé€‰æ‹©æ¨¡å‹è¶…å‚æ•°
            if model_type == 'lightgbm':
                lgb_params = self.param_space.get('lightgbm', {})
                if lgb_params:
                    config['model_params'] = {
                        'num_leaves': random.choice(lgb_params.get('num_leaves', [31])),
                        'learning_rate': random.choice(lgb_params.get('learning_rate', [0.05])),
                        'n_estimators': random.choice(lgb_params.get('n_estimators', [100])),
                        'max_depth': random.choice(lgb_params.get('max_depth', [-1]))
                    }

            elif model_type == 'gru':
                gru_params = self.param_space.get('gru', {})
                if gru_params:
                    config['seq_length'] = random.choice(gru_params.get('seq_length', [20]))
                    config['epochs'] = random.choice(gru_params.get('epochs', [100]))
                    config['model_params'] = {
                        'hidden_size': random.choice(gru_params.get('hidden_size', [64])),
                        'num_layers': random.choice(gru_params.get('num_layers', [2])),
                        'dropout': random.choice(gru_params.get('dropout', [0.2]))
                    }

            configs.append(config)

        return configs

    def _generate_hash(self, config: Dict[str, Any]) -> str:
        """
        ä¸ºé…ç½®ç”ŸæˆMD5å“ˆå¸Œ
        ç”¨äºé¿å…é‡å¤å®éªŒ
        """
        # æ’åºååºåˆ—åŒ–ï¼Œç¡®ä¿å“ˆå¸Œä¸€è‡´æ€§
        config_str = json.dumps(config, sort_keys=True)
        return hashlib.md5(config_str.encode()).hexdigest()

    def estimate_total_combinations(self) -> int:
        """ä¼°ç®—æ€»å‚æ•°ç»„åˆæ•°ï¼ˆç”¨äºgridç­–ç•¥ï¼‰"""

        count = 1

        # åŸºç¡€å‚æ•°
        count *= len(self.param_space.get('symbols', []))
        count *= len(self.param_space.get('date_ranges', [['20200101', '20231231']]))
        count *= len(self.param_space.get('model_types', ['lightgbm']))
        count *= len(self.param_space.get('target_periods', [5]))
        count *= len(self.param_space.get('scaler_types', ['robust']))
        count *= len(self.param_space.get('balance_samples', [False]))

        # è¶…å‚æ•°ï¼ˆå–æœ€å¤§ï¼‰
        max_hyperparams = 1

        lgb_params = self.param_space.get('lightgbm', {})
        if lgb_params and 'lightgbm' in self.param_space.get('model_types', []):
            lgb_count = (
                len(lgb_params.get('num_leaves', [1])) *
                len(lgb_params.get('learning_rate', [1])) *
                len(lgb_params.get('n_estimators', [1])) *
                len(lgb_params.get('max_depth', [1]))
            )
            max_hyperparams = max(max_hyperparams, lgb_count)

        gru_params = self.param_space.get('gru', {})
        if gru_params and 'gru' in self.param_space.get('model_types', []):
            gru_count = (
                len(gru_params.get('hidden_size', [1])) *
                len(gru_params.get('num_layers', [1])) *
                len(gru_params.get('dropout', [1])) *
                len(gru_params.get('seq_length', [1])) *
                len(gru_params.get('epochs', [1]))
            )
            max_hyperparams = max(max_hyperparams, gru_count)

        count *= max_hyperparams

        return count


# ============================================================
# é¢„å®šä¹‰çš„å‚æ•°ç©ºé—´æ¨¡æ¿
# ============================================================

class ParameterSpaceTemplates:
    """å¸¸ç”¨å‚æ•°ç©ºé—´æ¨¡æ¿"""

    @staticmethod
    def minimal_test() -> Dict:
        """æœ€å°æµ‹è¯•æ¨¡æ¿ï¼ˆå¿«é€ŸéªŒè¯ï¼‰"""
        return {
            'symbols': ['000001'],
            'date_ranges': [['20220101', '20231231']],
            'model_types': ['lightgbm'],
            'target_periods': [5],
            'scaler_types': ['robust'],
            'balance_samples': [False],
            'lightgbm': {
                'num_leaves': [31],
                'learning_rate': [0.05],
                'n_estimators': [100],
                'max_depth': [-1]
            }
        }

    @staticmethod
    def small_grid() -> Dict:
        """å°è§„æ¨¡ç½‘æ ¼ï¼ˆçº¦100ä¸ªå®éªŒï¼‰"""
        return {
            'symbols': ['000001', '000002', '600000', '600036', '600519'],
            'date_ranges': [
                ['20200101', '20231231'],
                ['20210101', '20231231']
            ],
            'model_types': ['lightgbm'],
            'target_periods': [5, 10],
            'scaler_types': ['robust', 'standard'],
            'balance_samples': [False],
            'lightgbm': {
                'num_leaves': [31, 63],
                'learning_rate': [0.05, 0.1],
                'n_estimators': [100],
                'max_depth': [-1]
            }
        }

    @staticmethod
    def medium_grid() -> Dict:
        """ä¸­ç­‰è§„æ¨¡ç½‘æ ¼ï¼ˆçº¦500ä¸ªå®éªŒï¼‰"""
        return {
            'symbols': ['000001', '000002', '000333', '000651', '000858',
                       '600000', '600036', '600519', '600585', '600900'],
            'date_ranges': [
                ['20200101', '20231231'],
                ['20210101', '20231231'],
                ['20220101', '20231231']
            ],
            'model_types': ['lightgbm', 'gru'],
            'target_periods': [5, 10, 20],
            'scaler_types': ['robust', 'standard'],
            'balance_samples': [False, True],
            'lightgbm': {
                'num_leaves': [31, 63],
                'learning_rate': [0.05, 0.1],
                'n_estimators': [100, 200],
                'max_depth': [-1, 5]
            },
            'gru': {
                'hidden_size': [64, 128],
                'num_layers': [2],
                'dropout': [0.2],
                'seq_length': [20],
                'epochs': [50, 100]
            }
        }

    @staticmethod
    def large_random() -> Dict:
        """å¤§è§„æ¨¡éšæœºé‡‡æ ·ï¼ˆé…åˆrandomç­–ç•¥ä½¿ç”¨ï¼‰"""
        return {
            'symbols': ['000001', '000002', '000333', '000651', '000858',
                       '002594', '002714', '002920', '300059', '300750',
                       '600000', '600036', '600276', '600309', '600519',
                       '600585', '600690', '600887', '600900', '601318'],
            'date_ranges': [
                ['20180101', '20231231'],
                ['20190101', '20231231'],
                ['20200101', '20231231'],
                ['20210101', '20231231'],
                ['20220101', '20231231']
            ],
            'model_types': ['lightgbm', 'gru'],
            'target_periods': [1, 3, 5, 10, 20],
            'scaler_types': ['standard', 'robust', 'minmax'],
            'balance_samples': [False, True],
            'lightgbm': {
                'num_leaves': [15, 31, 63, 127],
                'learning_rate': [0.01, 0.03, 0.05, 0.1, 0.2],
                'n_estimators': [50, 100, 200, 500],
                'max_depth': [-1, 3, 5, 7, 10]
            },
            'gru': {
                'hidden_size': [32, 64, 128, 256],
                'num_layers': [1, 2, 3],
                'dropout': [0.1, 0.2, 0.3],
                'seq_length': [10, 20, 30, 60],
                'epochs': [50, 100, 200]
            }
        }
