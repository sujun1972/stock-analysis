"""
æœºå™¨å­¦ä¹ è®­ç»ƒæœåŠ¡
ç®¡ç†è®­ç»ƒä»»åŠ¡çš„ç”Ÿå‘½å‘¨æœŸ
"""

import asyncio
import uuid
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, Optional
import json
import pickle
import pandas as pd
import numpy as np

from loguru import logger

# å¯¼å…¥ core æ¨¡å—ï¼ˆå·²é€šè¿‡ setup.py å®‰è£…ä¸ºå¯å¯¼å…¥åŒ…ï¼‰
from data_pipeline import DataPipeline, get_full_training_data
from models.model_trainer import ModelTrainer

# å¯¼å…¥æ ¸å¿ƒè®­ç»ƒæ¨¡å—
from app.services.core_training import CoreTrainingService

# å¯¼å…¥å·¥å…·å‡½æ•°
from app.utils.data_cleaning import sanitize_float_values


class MLTrainingService:
    """æœºå™¨å­¦ä¹ è®­ç»ƒæœåŠ¡"""

    def __init__(self):
        """åˆå§‹åŒ–æœåŠ¡"""
        self.tasks: Dict[str, Dict[str, Any]] = {}  # å†…å­˜ä¸­çš„ä»»åŠ¡çŠ¶æ€
        self.models_dir = Path('/data/models/ml_models')
        self.models_dir.mkdir(parents=True, exist_ok=True)

        # ä»»åŠ¡å…ƒæ•°æ®å­˜å‚¨
        self.metadata_file = self.models_dir / 'tasks_metadata.json'
        self._load_metadata()

        # æ•°æ®åº“è¿æ¥
        from database.db_manager import DatabaseManager
        self.db = DatabaseManager()

    def _load_metadata(self):
        """åŠ è½½ä»»åŠ¡å…ƒæ•°æ®"""
        if self.metadata_file.exists():
            try:
                with open(self.metadata_file, 'r') as f:
                    self.tasks = json.load(f)
                logger.info(f"åŠ è½½äº† {len(self.tasks)} ä¸ªå†å²ä»»åŠ¡")
            except Exception as e:
                logger.error(f"åŠ è½½å…ƒæ•°æ®å¤±è´¥: {e}")
                self.tasks = {}

    def _save_metadata(self):
        """ä¿å­˜ä»»åŠ¡å…ƒæ•°æ®"""
        try:
            with open(self.metadata_file, 'w') as f:
                json.dump(self.tasks, f, indent=2, default=str)
        except Exception as e:
            logger.error(f"ä¿å­˜å…ƒæ•°æ®å¤±è´¥: {e}")

    async def create_task(self, config: Dict[str, Any]) -> str:
        """
        åˆ›å»ºè®­ç»ƒä»»åŠ¡

        å‚æ•°:
            config: è®­ç»ƒé…ç½®

        è¿”å›:
            task_id: ä»»åŠ¡ID
        """
        task_id = str(uuid.uuid4())

        task = {
            'task_id': task_id,
            'status': 'pending',
            'created_at': datetime.now().isoformat(),
            'started_at': None,
            'completed_at': None,
            'config': config,
            'progress': 0.0,
            'current_step': 'Created',
            'metrics': None,
            'model_path': None,
            'feature_importance': None,
            'error_message': None
        }

        self.tasks[task_id] = task
        self._save_metadata()

        logger.info(f"åˆ›å»ºè®­ç»ƒä»»åŠ¡: {task_id}")

        return task_id

    async def start_training(self, task_id: str):
        """
        å¼€å§‹è®­ç»ƒä»»åŠ¡ï¼ˆå¼‚æ­¥æ‰§è¡Œï¼‰

        å‚æ•°:
            task_id: ä»»åŠ¡ID
        """
        if task_id not in self.tasks:
            raise ValueError(f"ä»»åŠ¡ä¸å­˜åœ¨: {task_id}")

        task = self.tasks[task_id]

        if task['status'] == 'running':
            raise ValueError(f"ä»»åŠ¡å·²åœ¨è¿è¡Œ: {task_id}")

        # æ›´æ–°çŠ¶æ€
        task['status'] = 'running'
        task['started_at'] = datetime.now().isoformat()
        task['progress'] = 0.0
        task['current_step'] = 'Initializing'
        self._save_metadata()

        # å¼‚æ­¥æ‰§è¡Œè®­ç»ƒ
        asyncio.create_task(self._run_training(task_id))

        logger.info(f"å¼€å§‹è®­ç»ƒä»»åŠ¡: {task_id}")

    async def _run_training(self, task_id: str):
        """
        æ‰§è¡Œè®­ç»ƒï¼ˆå†…éƒ¨æ–¹æ³•ï¼‰
        ä½¿ç”¨ç»Ÿä¸€çš„CoreTrainingService

        å‚æ•°:
            task_id: ä»»åŠ¡ID
        """
        task = self.tasks[task_id]
        config = task['config']

        try:
            # ======== æ­¥éª¤1: æ•°æ®å‡†å¤‡ ========
            task['current_step'] = 'Fetching Data'
            task['progress'] = 10.0
            self._save_metadata()

            logger.info(f"[{task_id}] å¼€å§‹è®­ç»ƒæµç¨‹...")

            # ======== æ­¥éª¤2: è®­ç»ƒæ¨¡å‹ï¼ˆä½¿ç”¨æ ¸å¿ƒè®­ç»ƒæœåŠ¡ï¼‰ ========
            task['current_step'] = 'Training Model'
            task['progress'] = 30.0
            self._save_metadata()

            # ä½¿ç”¨ç»Ÿä¸€çš„æ ¸å¿ƒè®­ç»ƒæœåŠ¡
            core_service = CoreTrainingService(models_dir=str(self.models_dir))

            # ç”Ÿæˆæ¨¡å‹åç§°ï¼ˆä½¿ç”¨task_idçš„å‰8ä½ï¼‰
            model_id = f"{config['symbol']}_{config['model_type']}_{task_id[:8]}"

            # è°ƒç”¨æ ¸å¿ƒè®­ç»ƒæœåŠ¡
            result = await core_service.train_model(
                config=config,
                model_id=model_id,
                save_features=True,  # æ‰‹åŠ¨è®­ç»ƒéœ€è¦ä¿å­˜ç‰¹å¾ç”¨äºç‰¹å¾å¿«ç…§
                save_training_history=True,  # ä¿å­˜è®­ç»ƒå†å²ï¼ˆGRUæŸå¤±æ›²çº¿ï¼‰
                evaluate_on='test',  # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
                use_async=True  # ä½¿ç”¨å¼‚æ­¥è®­ç»ƒ
            )

            # ======== æ­¥éª¤3: æ›´æ–°ä»»åŠ¡çŠ¶æ€ ========
            task['current_step'] = 'Saving Model'
            task['progress'] = 90.0
            self._save_metadata()

            # ======== æ­¥éª¤4: ä»»åŠ¡å®Œæˆ ========
            task['status'] = 'completed'
            task['progress'] = 100.0
            task['current_step'] = 'Finished'
            completed_at = datetime.now()
            task['completed_at'] = completed_at.isoformat()

            # æ¸…ç†æŒ‡æ ‡å’Œç‰¹å¾é‡è¦æ€§ä¸­çš„ NaN/Inf å€¼ï¼Œé¿å… JSON åºåˆ—åŒ–é”™è¯¯
            # æ·»åŠ æ ·æœ¬æ•°åˆ° metrics ä¸­ï¼ˆç”¨äºå‰ç«¯æ˜¾ç¤ºï¼‰
            metrics_with_samples = result['metrics'].copy()
            metrics_with_samples['samples'] = result.get('test_samples', 0)  # ä½¿ç”¨æµ‹è¯•é›†æ ·æœ¬æ•°

            task['metrics'] = sanitize_float_values(metrics_with_samples)
            task['feature_importance'] = sanitize_float_values(result['feature_importance'])
            task['training_history'] = sanitize_float_values(result['training_history'])
            task['model_name'] = result['model_name']
            task['model_path'] = result['model_path']

            self._save_metadata()

            logger.info(f"[{task_id}] âœ… è®­ç»ƒå®Œæˆ! IC={result['metrics'].get('ic', 0):.4f}")

            # ======== æ­¥éª¤5: è‡ªåŠ¨å›æµ‹ï¼ˆä¸è‡ªåŠ¨å®éªŒç³»ç»Ÿä¿æŒä¸€è‡´ï¼‰ ========
            task['current_step'] = 'Running Backtest'
            task['progress'] = 92.0
            self._save_metadata()

            logger.info(f"[{task_id}] ğŸ“Š å¼€å§‹è‡ªåŠ¨å›æµ‹...")

            backtest_metrics = None
            try:
                # ä½¿ç”¨å›æµ‹æœåŠ¡è¿è¡Œå›æµ‹
                from app.services.backtest_service import BacktestService
                backtest_service = BacktestService()

                backtest_result = await backtest_service.run_backtest(
                    symbols=config['symbol'],
                    start_date=config['start_date'],
                    end_date=config['end_date'],
                    strategy_id='ml_model',
                    strategy_params={
                        'model_id': result['model_name'],
                        'buy_threshold': 0.15,   # é™ä½åˆ°0.15%ä»¥äº§ç”Ÿæ›´å¤šä¹°å…¥ä¿¡å·
                        'sell_threshold': -0.3    # ä¿æŒ-0.3%
                    }
                )

                backtest_metrics = backtest_result.get('metrics', {})
                task['backtest_metrics'] = sanitize_float_values(backtest_metrics)

                logger.info(
                    f"[{task_id}] âœ… å›æµ‹å®Œæˆ! "
                    f"å¹´åŒ–æ”¶ç›Š={backtest_metrics.get('annualized_return', 0) * 100:.2f}%, "
                    f"å¤æ™®æ¯”ç‡={backtest_metrics.get('sharpe_ratio', 0):.2f}"
                )

            except Exception as e:
                logger.warning(f"[{task_id}] âš ï¸ å›æµ‹å¤±è´¥ï¼ˆä¸å½±å“è®­ç»ƒï¼‰: {e}")
                # å›æµ‹å¤±è´¥ä¸å½±å“è®­ç»ƒç»“æœï¼Œç»§ç»­ä¿å­˜
                backtest_metrics = None

            self._save_metadata()

            # å°†æ¨¡å‹ä¿¡æ¯å†™å…¥æ•°æ®åº“çš„ experiments è¡¨ï¼ˆåŒ…å«å›æµ‹æŒ‡æ ‡ï¼‰
            await self._save_to_database(
                task_id,
                result['model_name'],
                metrics_with_samples,  # ä½¿ç”¨åŒ…å« samples çš„ metrics
                result['feature_importance'],
                completed_at,
                backtest_metrics  # æ–°å¢å›æµ‹æŒ‡æ ‡å‚æ•°
            )

        except Exception as e:
            logger.error(f"[{task_id}] âŒ è®­ç»ƒå¤±è´¥: {e}", exc_info=True)

            task['status'] = 'failed'
            task['current_step'] = 'Failed'
            task['completed_at'] = datetime.now().isoformat()
            task['error_message'] = str(e)

            self._save_metadata()

    async def _save_to_database(
        self,
        task_id: str,
        model_name: str,
        metrics: Dict,
        feature_importance: Dict,
        completed_at: datetime,
        backtest_metrics: Optional[Dict] = None
    ):
        """
        å°†æ‰‹åŠ¨è®­ç»ƒçš„æ¨¡å‹ä¿å­˜åˆ°æ•°æ®åº“çš„ experiments è¡¨

        å‚æ•°:
            task_id: ä»»åŠ¡ID
            model_name: æ¨¡å‹åç§°
            metrics: è®­ç»ƒæŒ‡æ ‡
            feature_importance: ç‰¹å¾é‡è¦æ€§
            completed_at: å®Œæˆæ—¶é—´
            backtest_metrics: å›æµ‹æŒ‡æ ‡ï¼ˆå¯é€‰ï¼‰
        """
        try:
            task = self.tasks[task_id]
            config = task['config']

            # å‡†å¤‡æ’å…¥æ•°æ®
            experiment_name = f"æ‰‹åŠ¨è®­ç»ƒ_{model_name}"
            experiment_hash = f"manual_{task_id}"

            # æ„å»ºconfig JSONï¼ˆä¿å­˜å®Œæ•´é…ç½®ï¼‰
            config_json = json.dumps({
                # åŸºæœ¬é…ç½®
                'symbol': config['symbol'],
                'model_type': config['model_type'],
                'target_period': config.get('target_period', 5),
                'start_date': config['start_date'],
                'end_date': config['end_date'],

                # æ•°æ®é›†åˆ’åˆ†
                'train_ratio': config.get('train_ratio', 0.7),
                'valid_ratio': config.get('valid_ratio', 0.15),
                'test_size': config.get('test_size', 0.2),  # å…¼å®¹æ—§ç‰ˆæœ¬

                # ç‰¹å¾é€‰æ‹©
                'use_technical_indicators': config.get('use_technical_indicators', True),
                'use_alpha_factors': config.get('use_alpha_factors', True),
                'selected_features': config.get('selected_features'),

                # æ•°æ®å¤„ç†
                'scaler_type': config.get('scaler_type', 'robust'),
                'scale_features': config.get('scale_features', True),
                'balance_samples': config.get('balance_samples', False),
                'balance_method': config.get('balance_method', 'undersample'),

                # æ¨¡å‹å‚æ•°
                'feature_config': config.get('feature_config', {}),
                'model_params': config.get('model_params', {}),

                # GRUç‰¹å®šå‚æ•°
                'seq_length': config.get('seq_length', 20),
                'batch_size': config.get('batch_size', 64),
                'epochs': config.get('epochs', 100),

                # LightGBMç‰¹å®šå‚æ•°
                'early_stopping_rounds': config.get('early_stopping_rounds', 50),
            })

            # æ„å»ºtrain_metrics JSON
            train_metrics_json = json.dumps(sanitize_float_values(metrics))

            # æ„å»ºfeature_importance JSONï¼ˆå¯é€‰ï¼‰
            feature_importance_json = json.dumps(sanitize_float_values(feature_importance)) if feature_importance else None

            # æ„å»ºbacktest_metrics JSONï¼ˆå¯é€‰ï¼‰
            backtest_metrics_json = json.dumps(sanitize_float_values(backtest_metrics)) if backtest_metrics else None

            # è®¡ç®—ç»¼åˆè¯„åˆ†ï¼ˆä¸è‡ªåŠ¨å®éªŒç³»ç»Ÿä¿æŒä¸€è‡´ï¼‰
            rank_score = None
            if backtest_metrics:
                from app.services.model_ranker import ModelRanker
                ranker = ModelRanker(self.db)
                rank_score = ranker.calculate_rank_score(metrics, backtest_metrics)
                logger.info(f"âœ… æ¨¡å‹è¯„åˆ†: {rank_score:.2f}")

            model_path = task['model_path']
            started_at = datetime.fromisoformat(task['started_at'])
            train_duration = int((completed_at - started_at).total_seconds())

            # æ’å…¥åˆ°æ•°æ®åº“ï¼ˆåŒ…å«å›æµ‹æŒ‡æ ‡å’Œè¯„åˆ†ï¼‰
            query = """
                INSERT INTO experiments (
                    batch_id, experiment_name, experiment_hash, config,
                    model_id, model_path, train_metrics, feature_importance, backtest_metrics, rank_score,
                    status, train_started_at, train_completed_at, train_duration_seconds,
                    created_at
                )
                VALUES (%s, %s, %s, %s::jsonb, %s, %s, %s::jsonb, %s::jsonb, %s::jsonb, %s, %s, %s, %s, %s, %s)
                ON CONFLICT (experiment_hash) DO UPDATE SET
                    train_metrics = EXCLUDED.train_metrics,
                    feature_importance = EXCLUDED.feature_importance,
                    backtest_metrics = EXCLUDED.backtest_metrics,
                    rank_score = EXCLUDED.rank_score,
                    train_completed_at = EXCLUDED.train_completed_at,
                    train_duration_seconds = EXCLUDED.train_duration_seconds,
                    status = EXCLUDED.status
            """

            params = (
                None,  # batch_id (æ‰‹åŠ¨è®­ç»ƒä¸º NULL)
                experiment_name,
                experiment_hash,
                config_json,
                model_name,
                model_path,
                train_metrics_json,
                feature_importance_json,
                backtest_metrics_json,  # å›æµ‹æŒ‡æ ‡
                rank_score,  # ç»¼åˆè¯„åˆ†
                'completed',
                started_at,
                completed_at,
                train_duration,
                started_at
            )

            await asyncio.to_thread(self.db._execute_update, query, params)
            logger.info(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°æ•°æ®åº“: {model_name}")

        except Exception as e:
            logger.error(f"ä¿å­˜æ¨¡å‹åˆ°æ•°æ®åº“å¤±è´¥: {e}", exc_info=True)
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œé¿å…å½±å“è®­ç»ƒæµç¨‹

    def get_task(self, task_id: str) -> Optional[Dict[str, Any]]:
        """è·å–ä»»åŠ¡ä¿¡æ¯"""
        return self.tasks.get(task_id)

    def list_tasks(
        self,
        status: Optional[str] = None,
        limit: int = 50
    ) -> list[Dict[str, Any]]:
        """
        åˆ—å‡ºä»»åŠ¡

        å‚æ•°:
            status: çŠ¶æ€è¿‡æ»¤
            limit: è¿”å›æ•°é‡é™åˆ¶

        è¿”å›:
            ä»»åŠ¡åˆ—è¡¨
        """
        tasks = list(self.tasks.values())

        # çŠ¶æ€è¿‡æ»¤
        if status:
            tasks = [t for t in tasks if t['status'] == status]

        # æŒ‰åˆ›å»ºæ—¶é—´å€’åº
        tasks.sort(key=lambda t: t['created_at'], reverse=True)

        return tasks[:limit]

    def delete_task(self, task_id: str) -> bool:
        """åˆ é™¤ä»»åŠ¡"""
        if task_id in self.tasks:
            del self.tasks[task_id]
            self._save_metadata()
            logger.info(f"åˆ é™¤ä»»åŠ¡: {task_id}")
            return True
        return False

    async def predict(
        self,
        model_id: str,
        symbol: str,
        start_date: str,
        end_date: str
    ) -> Dict[str, Any]:
        """
        ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹

        å‚æ•°:
            model_id: æ¨¡å‹IDï¼ˆå³task_idï¼‰
            symbol: è‚¡ç¥¨ä»£ç 
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ

        è¿”å›:
            é¢„æµ‹ç»“æœ
        """
        # è·å–ä»»åŠ¡ä¿¡æ¯
        task = self.get_task(model_id)
        if not task or task['status'] != 'completed':
            raise ValueError(f"æ¨¡å‹ä¸å­˜åœ¨æˆ–æœªå®Œæˆè®­ç»ƒ: {model_id}")

        config = task['config']

        # åŠ è½½æ¨¡å‹
        logger.info(f"ä½¿ç”¨æ¨¡å‹ {model_id} è¿›è¡Œé¢„æµ‹...")

        # è·å–æ•°æ®ï¼ˆä½¿ç”¨ç›¸åŒçš„Pipelineé…ç½®ï¼‰
        pipeline = DataPipeline(
            target_periods=config.get('target_period', 5),
            scaler_type=config.get('scaler_type', 'robust'),
            cache_features=False,
            verbose=False
        )

        X, y = await asyncio.to_thread(
            pipeline.get_training_data,
            symbol=symbol,
            start_date=start_date,
            end_date=end_date,
            use_cache=False
        )

        # åŠ è½½scaler
        scaler_path = Path(task['model_path']).with_name(
            Path(task['model_path']).stem + '_scaler.pkl'
        )
        if scaler_path.exists():
            with open(scaler_path, 'rb') as f:
                scaler = pickle.load(f)
            pipeline.set_scaler(scaler)

        # ç¼©æ”¾ç‰¹å¾ï¼ˆä½¿ç”¨ä¿å­˜çš„scalerï¼‰
        X_scaled = await asyncio.to_thread(
            scaler.transform,
            X
        )

        import pandas as pd
        X_scaled = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)

        # åŠ è½½æ¨¡å‹å¹¶é¢„æµ‹
        # ä»è®­ç»ƒé…ç½®æ¢å¤æ¨¡å‹å‚æ•°ï¼ˆGRUéœ€è¦input_sizeç­‰å‚æ•°æ‰èƒ½åˆå§‹åŒ–ï¼‰
        model_params = config.get('model_params') or {}

        # å¯¹äºGRUæ¨¡å‹ï¼Œç¡®ä¿åŒ…å«input_size
        if config['model_type'] == 'gru':
            if 'input_size' not in model_params:
                model_params['input_size'] = len(X.columns)

        trainer = ModelTrainer(
            model_type=config['model_type'],
            model_params=model_params,
            output_dir=str(self.models_dir)
        )

        model_name = Path(task['model_path']).stem
        await asyncio.to_thread(
            trainer.load_model,
            model_name
        )

        # é¢„æµ‹ï¼ˆGRUæ¨¡å‹éœ€è¦seq_lengthå‚æ•°ï¼‰
        if config['model_type'] == 'gru':
            seq_length = config.get('seq_length', 20)
            predictions = await asyncio.to_thread(
                trainer.model.predict,
                X_scaled,
                seq_length=seq_length
            )
            # GRUé¢„æµ‹ä¼šæŸå¤±å‰seq_lengthä¸ªæ ·æœ¬ï¼Œéœ€è¦å¯¹é½
            y = y.iloc[seq_length:]
            X = X.iloc[seq_length:]
        else:
            predictions = await asyncio.to_thread(
                trainer.model.predict,
                X_scaled
            )

        # ç»„è£…é¢„æµ‹ç»“æœ
        results = []
        for date, pred, actual in zip(X.index, predictions, y.values):
            results.append({
                'date': date.strftime('%Y-%m-%d'),
                'prediction': float(pred),
                'actual': float(actual)
            })

        # è®¡ç®—é¢„æµ‹æŒ‡æ ‡
        from sklearn.metrics import mean_squared_error, r2_score
        import numpy as np

        rmse = np.sqrt(mean_squared_error(y.values, predictions))
        r2 = r2_score(y.values, predictions)

        # è®¡ç®—IC
        from scipy.stats import pearsonr, spearmanr
        ic, _ = pearsonr(predictions, y.values)
        rank_ic, _ = spearmanr(predictions, y.values)

        metrics = {
            'rmse': float(rmse),
            'r2': float(r2),
            'ic': float(ic),
            'rank_ic': float(rank_ic),
            'samples': len(predictions)
        }

        return {
            'predictions': results,
            'metrics': sanitize_float_values(metrics)
        }

    async def predict_from_experiment(
        self,
        experiment_id: int,
        symbol: str,
        start_date: str,
        end_date: str
    ) -> Dict[str, Any]:
        """
        ä½¿ç”¨å®éªŒè¡¨ä¸­çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹ï¼ˆæ–°ç‰ˆAPIï¼‰

        å‚æ•°:
            experiment_id: å®éªŒIDï¼ˆexperimentsè¡¨ä¸»é”®ï¼‰
            symbol: è‚¡ç¥¨ä»£ç 
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ

        è¿”å›:
            é¢„æµ‹ç»“æœ
        """
        import sys
        from database.db_manager import DatabaseManager

        # æŸ¥è¯¢å®éªŒä¿¡æ¯
        db = DatabaseManager()
        query = """
            SELECT id, model_id, config, model_path, status
            FROM experiments
            WHERE id = %s
        """

        results = await asyncio.to_thread(db._execute_query, query, (experiment_id,))

        if not results or len(results) == 0:
            raise ValueError(f"å®éªŒä¸å­˜åœ¨: {experiment_id}")

        exp_id, model_id, config, model_path, status = results[0]

        if status != 'completed':
            raise ValueError(f"å®éªŒæœªå®Œæˆè®­ç»ƒ: {experiment_id} (çŠ¶æ€: {status})")

        if not model_path:
            raise ValueError(f"å®éªŒç¼ºå°‘æ¨¡å‹æ–‡ä»¶è·¯å¾„: {experiment_id}")

        # åŠ è½½æ¨¡å‹
        logger.info(f"ä½¿ç”¨å®éªŒ {experiment_id} çš„æ¨¡å‹ {model_id} è¿›è¡Œé¢„æµ‹...")

        # è·å–æ•°æ®ï¼ˆä½¿ç”¨ç›¸åŒçš„Pipelineé…ç½®ï¼‰
        pipeline = DataPipeline(
            target_periods=config.get('target_period', 5),
            scaler_type=config.get('scaler_type', 'robust'),
            cache_features=False,
            verbose=False
        )

        X, y = await asyncio.to_thread(
            pipeline.get_training_data,
            symbol=symbol,
            start_date=start_date,
            end_date=end_date,
            use_cache=False
        )

        # åŠ è½½scaler
        scaler_path = Path(model_path).with_name(
            Path(model_path).stem + '_scaler.pkl'
        )
        scaler = None
        if scaler_path.exists():
            with open(scaler_path, 'rb') as f:
                scaler = pickle.load(f)

            # æ£€æŸ¥åŠ è½½çš„scaleræ˜¯å¦æœ‰æ•ˆï¼ˆæ—§æ¨¡å‹å¯èƒ½ä¿å­˜äº†Noneï¼‰
            if scaler is not None:
                pipeline.set_scaler(scaler)
                logger.info(f"âœ… å·²åŠ è½½scaler: {scaler_path}")
            else:
                logger.warning(f"âš ï¸ Scaleræ–‡ä»¶æŸåï¼ˆå†…å®¹ä¸ºNoneï¼‰: {scaler_path}")

        # å¦‚æœscaleræ— æ•ˆæˆ–ä¸å­˜åœ¨ï¼Œä½¿ç”¨å½“å‰æ•°æ®é‡æ–°fit
        if scaler is None:
            logger.warning(f"âš ï¸ ä½¿ç”¨å½“å‰æ•°æ®é‡æ–°fit scalerï¼ˆè¿™å¯èƒ½å¯¼è‡´é¢„æµ‹ä¸å‡†ç¡®ï¼‰")
            from sklearn.preprocessing import RobustScaler
            scaler = RobustScaler()
            scaler.fit(X)
            logger.warning(f"âš ï¸ å»ºè®®é‡æ–°è®­ç»ƒæ¨¡å‹ä»¥è·å¾—å‡†ç¡®çš„scaler")

        # ç¼©æ”¾ç‰¹å¾ï¼ˆä½¿ç”¨ä¿å­˜çš„scalerï¼‰
        X_scaled = await asyncio.to_thread(
            scaler.transform,
            X
        )

        import pandas as pd
        X_scaled = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)

        # åŠ è½½æ¨¡å‹å¹¶é¢„æµ‹
        # ä»è®­ç»ƒé…ç½®æ¢å¤æ¨¡å‹å‚æ•°ï¼ˆGRUéœ€è¦input_sizeç­‰å‚æ•°æ‰èƒ½åˆå§‹åŒ–ï¼‰
        model_params = config.get('model_params') or {}

        # å¯¹äºGRUæ¨¡å‹ï¼Œç¡®ä¿åŒ…å«input_size
        if config['model_type'] == 'gru':
            if 'input_size' not in model_params:
                model_params['input_size'] = len(X.columns)

        trainer = ModelTrainer(
            model_type=config['model_type'],
            model_params=model_params,
            output_dir=str(self.models_dir)
        )

        model_name = Path(model_path).stem
        await asyncio.to_thread(
            trainer.load_model,
            model_name
        )

        # é¢„æµ‹ï¼ˆGRUæ¨¡å‹éœ€è¦seq_lengthå‚æ•°ï¼‰
        if config['model_type'] == 'gru':
            seq_length = config.get('seq_length', 20)
            predictions = await asyncio.to_thread(
                trainer.model.predict,
                X_scaled,
                seq_length=seq_length
            )
            # GRUé¢„æµ‹ä¼šæŸå¤±å‰seq_lengthä¸ªæ ·æœ¬ï¼Œéœ€è¦å¯¹é½
            y = y.iloc[seq_length:]
            X = X.iloc[seq_length:]
        else:
            predictions = await asyncio.to_thread(
                trainer.model.predict,
                X_scaled
            )

        # ç»„è£…é¢„æµ‹ç»“æœ
        results = []
        for date, pred, actual in zip(X.index, predictions, y.values):
            results.append({
                'date': date.strftime('%Y-%m-%d'),
                'prediction': float(pred),
                'actual': float(actual)
            })

        # è®¡ç®—é¢„æµ‹æŒ‡æ ‡
        from sklearn.metrics import mean_squared_error, r2_score
        import numpy as np

        rmse = np.sqrt(mean_squared_error(y.values, predictions))
        r2 = r2_score(y.values, predictions)

        # è®¡ç®—IC
        from scipy.stats import pearsonr, spearmanr
        ic, _ = pearsonr(predictions, y.values)
        rank_ic, _ = spearmanr(predictions, y.values)

        metrics = {
            'rmse': float(rmse),
            'r2': float(r2),
            'ic': float(ic),
            'rank_ic': float(rank_ic),
            'samples': len(predictions)
        }

        return {
            'predictions': results,
            'metrics': sanitize_float_values(metrics)
        }
