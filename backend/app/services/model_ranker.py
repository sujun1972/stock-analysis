"""
æ¨¡å‹æ’åå’Œç­›é€‰ç³»ç»Ÿ
æ ¹æ®å¤šç»´åº¦æŒ‡æ ‡è‡ªåŠ¨ç­›é€‰æœ€ä¼˜æ¨¡å‹
"""

from typing import Dict, List, Optional, Any
import numpy as np
from loguru import logger

from src.database.db_manager import DatabaseManager


class ModelRanker:
    """æ¨¡å‹æ’åå™¨"""

    def __init__(self, db_manager: Optional[DatabaseManager] = None):
        self.db = db_manager or DatabaseManager()

        # é»˜è®¤æƒé‡é…ç½®
        self.default_weights = {
            # è®­ç»ƒæŒ‡æ ‡æƒé‡
            'ic': 10.0,              # IC (Information Coefficient)
            'rank_ic': 8.0,          # Rank IC
            'r2': 5.0,               # RÂ² (æ‹Ÿåˆä¼˜åº¦)

            # å›æµ‹æŒ‡æ ‡æƒé‡
            'annual_return': 5.0,    # å¹´åŒ–æ”¶ç›Šç‡
            'sharpe_ratio': 15.0,    # å¤æ™®æ¯”ç‡ï¼ˆé£é™©è°ƒæ•´åæ”¶ç›Šï¼‰
            'max_drawdown': -10.0,   # æœ€å¤§å›æ’¤ï¼ˆè´Ÿæƒé‡ï¼Œè¶Šå°è¶Šå¥½ï¼‰
            'win_rate': 3.0,         # èƒœç‡
            'profit_factor': 5.0,    # ç›ˆäºæ¯”
            'calmar_ratio': 8.0,     # Calmaræ¯”ç‡
        }

    def calculate_rank_score(
        self,
        train_metrics: Dict,
        backtest_metrics: Dict,
        weights: Optional[Dict] = None
    ) -> float:
        """
        è®¡ç®—ç»¼åˆè¯„åˆ†

        è¯„åˆ†å…¬å¼ï¼ˆå¯è‡ªå®šä¹‰æƒé‡ï¼‰:
        score = Î£(wi * normalize(metric_i))

        Args:
            train_metrics: è®­ç»ƒæŒ‡æ ‡ {ic, rank_ic, r2, rmse}
            backtest_metrics: å›æµ‹æŒ‡æ ‡ {annual_return, sharpe, max_drawdown, ...}
            weights: è‡ªå®šä¹‰æƒé‡ï¼ˆå¯é€‰ï¼‰

        Returns:
            ç»¼åˆè¯„åˆ†ï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰
        """
        w = weights or self.default_weights

        score = 0.0

        # è®­ç»ƒæŒ‡æ ‡
        ic = self._safe_float(train_metrics.get('ic', 0))
        rank_ic = self._safe_float(train_metrics.get('rank_ic', 0))
        r2 = self._safe_float(train_metrics.get('r2', 0))

        score += w.get('ic', 0) * ic
        score += w.get('rank_ic', 0) * rank_ic
        score += w.get('r2', 0) * max(0, r2)  # RÂ²å¯èƒ½ä¸ºè´Ÿï¼Œå–max(0, r2)

        # å›æµ‹æŒ‡æ ‡
        annual_return = self._safe_float(backtest_metrics.get('annual_return', 0))
        sharpe_ratio = self._safe_float(backtest_metrics.get('sharpe_ratio', 0))
        max_drawdown = self._safe_float(backtest_metrics.get('max_drawdown', 0))
        win_rate = self._safe_float(backtest_metrics.get('win_rate', 0))
        profit_factor = self._safe_float(backtest_metrics.get('profit_factor', 0))
        calmar_ratio = self._safe_float(backtest_metrics.get('calmar_ratio', 0))

        # å½’ä¸€åŒ–å¹¶åŠ æƒ
        score += w.get('annual_return', 0) * (annual_return / 100.0)  # ç™¾åˆ†æ¯”è½¬å°æ•°
        score += w.get('sharpe_ratio', 0) * sharpe_ratio
        score += w.get('max_drawdown', 0) * abs(max_drawdown) / 100.0  # è´Ÿæƒé‡
        score += w.get('win_rate', 0) * (win_rate / 100.0)
        score += w.get('profit_factor', 0) * profit_factor
        score += w.get('calmar_ratio', 0) * calmar_ratio

        return round(score, 4)

    def _safe_float(self, value: Any) -> float:
        """å®‰å…¨è½¬æ¢ä¸ºfloat"""
        try:
            if value is None:
                return 0.0
            return float(value)
        except (ValueError, TypeError):
            return 0.0

    def filter_models(
        self,
        batch_id: int,
        min_sharpe: Optional[float] = None,
        max_drawdown: Optional[float] = None,
        min_annual_return: Optional[float] = None,
        min_win_rate: Optional[float] = None,
        min_ic: Optional[float] = None,
        top_n: Optional[int] = None
    ) -> List[Dict]:
        """
        æ ¹æ®æ¡ä»¶ç­›é€‰æ¨¡å‹

        Args:
            batch_id: æ‰¹æ¬¡ID
            min_sharpe: æœ€ä½å¤æ™®æ¯”ç‡é˜ˆå€¼
            max_drawdown: æœ€å¤§å›æ’¤é˜ˆå€¼ï¼ˆ%ï¼Œå¦‚-25è¡¨ç¤ºæœ€å¤šå›æ’¤25%ï¼‰
            min_annual_return: æœ€ä½å¹´åŒ–æ”¶ç›Šç‡ï¼ˆ%ï¼‰
            min_win_rate: æœ€ä½èƒœç‡ï¼ˆ%ï¼‰
            min_ic: æœ€ä½ICå€¼
            top_n: è¿”å›å‰Nä¸ªæ¨¡å‹

        Returns:
            ç¬¦åˆæ¡ä»¶çš„æ¨¡å‹åˆ—è¡¨
        """
        conditions = ["batch_id = %s", "status = 'completed'", "backtest_status = 'completed'"]
        params = [batch_id]

        if min_sharpe is not None:
            conditions.append("(backtest_metrics->>'sharpe_ratio')::FLOAT >= %s")
            params.append(min_sharpe)

        if max_drawdown is not None:
            conditions.append("(backtest_metrics->>'max_drawdown')::FLOAT >= %s")
            params.append(max_drawdown)

        if min_annual_return is not None:
            conditions.append("(backtest_metrics->>'annual_return')::FLOAT >= %s")
            params.append(min_annual_return)

        if min_win_rate is not None:
            conditions.append("(backtest_metrics->>'win_rate')::FLOAT >= %s")
            params.append(min_win_rate)

        if min_ic is not None:
            conditions.append("(train_metrics->>'ic')::FLOAT >= %s")
            params.append(min_ic)

        query = f"""
            SELECT
                id,
                experiment_name,
                model_id,
                config,
                train_metrics,
                backtest_metrics,
                rank_score,
                rank_position
            FROM experiments
            WHERE {' AND '.join(conditions)}
            ORDER BY rank_score DESC NULLS LAST
        """

        if top_n:
            query += f" LIMIT {top_n}"

        results = self.db._execute_query(query, tuple(params))

        models = []
        for row in results:
            models.append({
                'id': row[0],
                'experiment_name': row[1],
                'model_id': row[2],
                'config': row[3],
                'train_metrics': row[4],
                'backtest_metrics': row[5],
                'rank_score': float(row[6]) if row[6] else None,
                'rank_position': row[7]
            })

        return models

    def analyze_parameter_importance(self, batch_id: int) -> Dict[str, float]:
        """
        åˆ†æå‚æ•°é‡è¦æ€§
        è®¡ç®—æ¯ä¸ªå‚æ•°å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“

        Args:
            batch_id: æ‰¹æ¬¡ID

        Returns:
            å‚æ•°é‡è¦æ€§å­—å…¸ {param_name: importance_score}
        """
        logger.info(f"ğŸ“Š åˆ†ææ‰¹æ¬¡ {batch_id} çš„å‚æ•°é‡è¦æ€§...")

        # è·å–æ‰€æœ‰å®Œæˆçš„å®éªŒ
        query = """
            SELECT config, rank_score
            FROM experiments
            WHERE batch_id = %s AND status = 'completed' AND rank_score IS NOT NULL
        """

        results = self.db._execute_query(query, (batch_id,))

        if not results:
            logger.warning("æ²¡æœ‰å¯ç”¨çš„å®éªŒæ•°æ®")
            return {}

        # æå–å‚æ•°å’Œè¯„åˆ†
        param_values = {}
        scores = []

        for row in results:
            config = row[0]
            score = float(row[1])
            scores.append(score)

            # æå–å…³é”®å‚æ•°
            for key in ['symbol', 'model_type', 'target_period', 'scaler_type', 'balance_samples']:
                if key in config:
                    if key not in param_values:
                        param_values[key] = []
                    param_values[key].append((config[key], score))

        # è®¡ç®—æ¯ä¸ªå‚æ•°ä¸è¯„åˆ†çš„ç›¸å…³æ€§
        importance = {}

        for param_name, values in param_values.items():
            # åˆ†ç»„è®¡ç®—å¹³å‡å¾—åˆ†
            groups = {}
            for val, score in values:
                val_str = str(val)
                if val_str not in groups:
                    groups[val_str] = []
                groups[val_str].append(score)

            # è®¡ç®—ç»„é—´æ–¹å·®ï¼ˆANOVA F-statisticçš„ç®€åŒ–ç‰ˆæœ¬ï¼‰
            group_means = [np.mean(group) for group in groups.values()]
            overall_mean = np.mean(scores)

            # Between-group variance
            bg_var = np.var(group_means)

            # å½’ä¸€åŒ–é‡è¦æ€§å¾—åˆ†ï¼ˆ0-1ï¼‰
            importance[param_name] = min(1.0, bg_var / (np.var(scores) + 1e-8))

        # æŒ‰é‡è¦æ€§æ’åº
        importance = dict(sorted(importance.items(), key=lambda x: x[1], reverse=True))

        logger.info(f"âœ… å‚æ•°é‡è¦æ€§: {importance}")
        return importance

    def generate_report(self, batch_id: int) -> Dict:
        """
        ç”Ÿæˆå®éªŒæŠ¥å‘Š

        Args:
            batch_id: æ‰¹æ¬¡ID

        Returns:
            å®Œæ•´çš„å®éªŒæŠ¥å‘Š
        """
        logger.info(f"ğŸ“ ç”Ÿæˆæ‰¹æ¬¡ {batch_id} çš„æŠ¥å‘Š...")

        report = {
            'batch_id': batch_id,
            'summary': self._get_summary(batch_id),
            'top_models': self.filter_models(batch_id, top_n=10),
            'parameter_importance': self.analyze_parameter_importance(batch_id),
            'performance_distribution': self._get_performance_distribution(batch_id),
            'best_configurations': self._get_best_configurations(batch_id)
        }

        return report

    def _get_summary(self, batch_id: int) -> Dict:
        """è·å–æ‰¹æ¬¡æ‘˜è¦"""

        query = "SELECT * FROM batch_statistics WHERE batch_id = %s"
        result = self.db._execute_query(query, (batch_id,))

        if result:
            row = result[0]
            return {
                'batch_name': row[1],
                'strategy': row[2],
                'status': row[3],
                'total_experiments': row[4],
                'completed_experiments': row[5],
                'failed_experiments': row[6],
                'success_rate_pct': float(row[8]) if row[8] else 0,
                'avg_rank_score': float(row[13]) if row[13] else None,
                'max_rank_score': float(row[14]) if row[14] else None,
                'duration_hours': float(row[12]) if row[12] else None
            }

        return {}

    def _get_performance_distribution(self, batch_id: int) -> Dict:
        """è·å–æ€§èƒ½åˆ†å¸ƒç»Ÿè®¡"""

        query = """
            SELECT
                COUNT(*) as total,
                AVG((backtest_metrics->>'annual_return')::FLOAT) as avg_return,
                STDDEV((backtest_metrics->>'annual_return')::FLOAT) as std_return,
                AVG((backtest_metrics->>'sharpe_ratio')::FLOAT) as avg_sharpe,
                AVG((backtest_metrics->>'max_drawdown')::FLOAT) as avg_drawdown,
                AVG((train_metrics->>'ic')::FLOAT) as avg_ic
            FROM experiments
            WHERE batch_id = %s AND status = 'completed' AND backtest_status = 'completed'
        """

        result = self.db._execute_query(query, (batch_id,))

        if result and result[0][0]:
            row = result[0]
            return {
                'total_models': row[0],
                'avg_annual_return': round(float(row[1]), 2) if row[1] else None,
                'std_annual_return': round(float(row[2]), 2) if row[2] else None,
                'avg_sharpe_ratio': round(float(row[3]), 2) if row[3] else None,
                'avg_max_drawdown': round(float(row[4]), 2) if row[4] else None,
                'avg_ic': round(float(row[5]), 4) if row[5] else None
            }

        return {}

    def _get_best_configurations(self, batch_id: int) -> Dict:
        """æ‰¾å‡ºæœ€ä½³é…ç½®ç»„åˆ"""

        # æŒ‰æ¨¡å‹ç±»å‹åˆ†ç»„
        query = """
            SELECT
                config->>'model_type' as model_type,
                AVG(rank_score) as avg_score,
                COUNT(*) as count
            FROM experiments
            WHERE batch_id = %s AND status = 'completed'
            GROUP BY config->>'model_type'
            ORDER BY avg_score DESC
        """

        result = self.db._execute_query(query, (batch_id,))

        best_model_type = result[0] if result else None

        # æŒ‰é¢„æµ‹å‘¨æœŸåˆ†ç»„
        query = """
            SELECT
                config->>'target_period' as target_period,
                AVG(rank_score) as avg_score,
                COUNT(*) as count
            FROM experiments
            WHERE batch_id = %s AND status = 'completed'
            GROUP BY config->>'target_period'
            ORDER BY avg_score DESC
        """

        result = self.db._execute_query(query, (batch_id,))

        best_target_period = result[0] if result else None

        return {
            'best_model_type': {
                'model_type': best_model_type[0] if best_model_type else None,
                'avg_score': float(best_model_type[1]) if best_model_type and best_model_type[1] else None,
                'count': best_model_type[2] if best_model_type else 0
            },
            'best_target_period': {
                'target_period': int(best_target_period[0]) if best_target_period and best_target_period[0] else None,
                'avg_score': float(best_target_period[1]) if best_target_period and best_target_period[1] else None,
                'count': best_target_period[2] if best_target_period else 0
            }
        }


class ModelSelector:
    """
    æ¨¡å‹é€‰æ‹©å™¨
    æä¾›å¤šç§ç­–ç•¥é€‰æ‹©æœ€ä¼˜æ¨¡å‹ç»„åˆ
    """

    def __init__(self, db_manager: Optional[DatabaseManager] = None):
        self.db = db_manager or DatabaseManager()
        self.ranker = ModelRanker(db_manager)

    def select_diverse_portfolio(
        self,
        batch_id: int,
        n_models: int = 5,
        diversity_weight: float = 0.3
    ) -> List[Dict]:
        """
        é€‰æ‹©å¤šæ ·åŒ–çš„æ¨¡å‹ç»„åˆ
        å¹³è¡¡æ€§èƒ½å’Œå¤šæ ·æ€§

        Args:
            batch_id: æ‰¹æ¬¡ID
            n_models: é€‰æ‹©æ¨¡å‹æ•°é‡
            diversity_weight: å¤šæ ·æ€§æƒé‡ï¼ˆ0-1ï¼‰

        Returns:
            æ¨¡å‹åˆ—è¡¨
        """
        # è·å–æ‰€æœ‰å€™é€‰æ¨¡å‹
        candidates = self.ranker.filter_models(batch_id, top_n=50)

        if len(candidates) <= n_models:
            return candidates

        # åˆå§‹åŒ–ï¼šé€‰æ‹©è¯„åˆ†æœ€é«˜çš„æ¨¡å‹
        selected = [candidates[0]]
        remaining = candidates[1:]

        # è´ªå¿ƒé€‰æ‹©ï¼šæ¯æ¬¡é€‰æ‹©ä¸å·²é€‰æ¨¡å‹å·®å¼‚æœ€å¤§ä¸”è¯„åˆ†é«˜çš„æ¨¡å‹
        while len(selected) < n_models and remaining:
            best_score = -float('inf')
            best_idx = 0

            for idx, candidate in enumerate(remaining):
                # æ€§èƒ½å¾—åˆ†
                performance_score = candidate['rank_score'] or 0

                # å¤šæ ·æ€§å¾—åˆ†ï¼ˆä¸å·²é€‰æ¨¡å‹çš„å·®å¼‚ï¼‰
                diversity_score = self._calculate_diversity(candidate, selected)

                # ç»¼åˆå¾—åˆ†
                combined_score = (
                    (1 - diversity_weight) * performance_score +
                    diversity_weight * diversity_score * 100  # ç¼©æ”¾åˆ°ç›¸åŒé‡çº§
                )

                if combined_score > best_score:
                    best_score = combined_score
                    best_idx = idx

            selected.append(remaining.pop(best_idx))

        return selected

    def _calculate_diversity(self, candidate: Dict, selected_models: List[Dict]) -> float:
        """è®¡ç®—å€™é€‰æ¨¡å‹ä¸å·²é€‰æ¨¡å‹çš„å¹³å‡å·®å¼‚åº¦"""

        if not selected_models:
            return 1.0

        differences = []

        for selected in selected_models:
            diff = 0

            # æ¨¡å‹ç±»å‹ä¸åŒ +1
            if candidate['config'].get('model_type') != selected['config'].get('model_type'):
                diff += 1

            # è‚¡ç¥¨ä¸åŒ +1
            if candidate['config'].get('symbol') != selected['config'].get('symbol'):
                diff += 1

            # é¢„æµ‹å‘¨æœŸä¸åŒ +0.5
            if candidate['config'].get('target_period') != selected['config'].get('target_period'):
                diff += 0.5

            # Scalerç±»å‹ä¸åŒ +0.3
            if candidate['config'].get('scaler_type') != selected['config'].get('scaler_type'):
                diff += 0.3

            differences.append(diff)

        return np.mean(differences) / 3.8  # å½’ä¸€åŒ–åˆ°0-1
