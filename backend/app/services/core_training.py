"""
ç»Ÿä¸€çš„æ ¸å¿ƒè®­ç»ƒæ¨¡å—
æä¾›å¯å¤ç”¨çš„è®­ç»ƒæµç¨‹ï¼Œä¾›æ‰‹åŠ¨è®­ç»ƒå’Œè‡ªåŠ¨å®éªŒæœåŠ¡è°ƒç”¨
æ¶ˆé™¤ä»£ç é‡å¤ï¼Œç¡®ä¿è®­ç»ƒé€»è¾‘çš„ä¸€è‡´æ€§
"""

import asyncio
import pickle
import time
from pathlib import Path
from typing import Dict, Any, Optional, Tuple
from datetime import datetime

import pandas as pd
from loguru import logger

import sys
sys.path.insert(0, '/app/src')

from src.data_pipeline import DataPipeline, get_full_training_data
from src.models.model_trainer import ModelTrainer
from app.utils.ic_validator import ICValidator


class CoreTrainingService:
    """
    æ ¸å¿ƒè®­ç»ƒæœåŠ¡

    æä¾›ç»Ÿä¸€çš„è®­ç»ƒæµç¨‹ï¼ŒåŒ…æ‹¬ï¼š
    1. æ•°æ®å‡†å¤‡
    2. æ¨¡å‹è®­ç»ƒ
    3. æ¨¡å‹è¯„ä¼°
    4. æ¨¡å‹ä¿å­˜
    5. å…ƒæ•°æ®ä¿å­˜
    """

    def __init__(self, models_dir: str = '/data/models/ml_models'):
        self.models_dir = Path(models_dir)
        self.models_dir.mkdir(parents=True, exist_ok=True)
        self.ic_validator = ICValidator()

    async def train_model(
        self,
        config: Dict[str, Any],
        model_id: Optional[str] = None,
        save_features: bool = True,
        save_training_history: bool = True,
        evaluate_on: str = 'test',  # 'train', 'valid', 'test'
        use_async: bool = True  # æ˜¯å¦ä½¿ç”¨å¼‚æ­¥è®­ç»ƒ
    ) -> Dict[str, Any]:
        """
        ç»Ÿä¸€çš„è®­ç»ƒæµç¨‹

        Args:
            config: è®­ç»ƒé…ç½®å­—å…¸
                - symbol: è‚¡ç¥¨ä»£ç 
                - model_type: æ¨¡å‹ç±»å‹ ('lightgbm', 'gru')
                - start_date: å¼€å§‹æ—¥æœŸ
                - end_date: ç»“æŸæ—¥æœŸ
                - target_period: é¢„æµ‹å‘¨æœŸ
                - train_ratio: è®­ç»ƒé›†æ¯”ä¾‹
                - valid_ratio: éªŒè¯é›†æ¯”ä¾‹
                - scale_features: æ˜¯å¦ç¼©æ”¾ç‰¹å¾
                - balance_samples: æ˜¯å¦å¹³è¡¡æ ·æœ¬
                - scaler_type: ç¼©æ”¾å™¨ç±»å‹
                - model_params: æ¨¡å‹å‚æ•°
                - selected_features: ç‰¹å¾é€‰æ‹©ï¼ˆå¯é€‰ï¼‰
                - early_stopping_rounds: æ—©åœè½®æ•°ï¼ˆLightGBMï¼‰
                - seq_length: åºåˆ—é•¿åº¦ï¼ˆGRUï¼‰
                - batch_size: æ‰¹æ¬¡å¤§å°ï¼ˆGRUï¼‰
                - epochs: è®­ç»ƒè½®æ•°ï¼ˆGRUï¼‰

            model_id: æ¨¡å‹IDï¼ˆå¦‚æœæœªæä¾›åˆ™è‡ªåŠ¨ç”Ÿæˆï¼‰
            save_features: æ˜¯å¦ä¿å­˜å®Œæ•´ç‰¹å¾æ•°æ®ï¼ˆç”¨äºç‰¹å¾å¿«ç…§ï¼‰
            save_training_history: æ˜¯å¦ä¿å­˜è®­ç»ƒå†å²ï¼ˆGRUæŸå¤±æ›²çº¿ï¼‰
            evaluate_on: åœ¨å“ªä¸ªæ•°æ®é›†ä¸Šè¯„ä¼° ('train', 'valid', 'test')
            use_async: æ˜¯å¦ä½¿ç”¨å¼‚æ­¥è®­ç»ƒï¼ˆæ‰‹åŠ¨è®­ç»ƒ=Trueï¼Œè‡ªåŠ¨å®éªŒ=Falseï¼‰

        Returns:
            è®­ç»ƒç»“æœå­—å…¸:
                - model_id: æ¨¡å‹ID
                - model_name: æ¨¡å‹åç§°ï¼ˆåŒmodel_idï¼Œå…¼å®¹æ€§ï¼‰
                - model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
                - scaler_path: Scaleræ–‡ä»¶è·¯å¾„
                - features_path: ç‰¹å¾æ•°æ®è·¯å¾„ï¼ˆå¦‚æœsave_features=Trueï¼‰
                - metrics: è¯„ä¼°æŒ‡æ ‡
                - feature_importance: ç‰¹å¾é‡è¦æ€§ï¼ˆLightGBMï¼‰
                - training_history: è®­ç»ƒå†å²ï¼ˆGRUï¼‰
                - train_duration: è®­ç»ƒè€—æ—¶ï¼ˆç§’ï¼‰
                - train_samples: è®­ç»ƒé›†æ ·æœ¬æ•°
                - valid_samples: éªŒè¯é›†æ ·æœ¬æ•°
                - test_samples: æµ‹è¯•é›†æ ·æœ¬æ•°
                - feature_count: ç‰¹å¾æ•°é‡
                - trained_at: è®­ç»ƒå®Œæˆæ—¶é—´
        """

        start_time = datetime.now()

        try:
            # ======== æ­¥éª¤1: æ•°æ®å‡†å¤‡ ========
            logger.info(f"[CoreTraining] è·å–è®­ç»ƒæ•°æ®...")

            # ä½¿ç”¨ç»Ÿä¸€çš„æ•°æ®è·å–æ¥å£
            if use_async:
                result = await asyncio.to_thread(
                    get_full_training_data,
                    symbol=config['symbol'],
                    start_date=config['start_date'],
                    end_date=config['end_date'],
                    target_period=config.get('target_period', 5),
                    train_ratio=config.get('train_ratio', 0.7),
                    valid_ratio=config.get('valid_ratio', 0.15),
                    scale_features=config.get('scale_features', True),
                    balance_samples=config.get('balance_samples', False),
                    scaler_type=config.get('scaler_type', 'robust')
                )
            else:
                result = get_full_training_data(
                    symbol=config['symbol'],
                    start_date=config['start_date'],
                    end_date=config['end_date'],
                    target_period=config.get('target_period', 5),
                    train_ratio=config.get('train_ratio', 0.7),
                    valid_ratio=config.get('valid_ratio', 0.15),
                    scale_features=config.get('scale_features', True),
                    balance_samples=config.get('balance_samples', False),
                    scaler_type=config.get('scaler_type', 'robust')
                )

            X_train, y_train, X_valid, y_valid, X_test, y_test, pipeline = result

            logger.info(f"[CoreTraining] æ•°æ®å‡†å¤‡å®Œæˆ: train={len(X_train)}, valid={len(X_valid)}, test={len(X_test)}")

            # ç‰¹å¾é€‰æ‹©ï¼ˆå¦‚æœæŒ‡å®šï¼‰
            if config.get('selected_features'):
                selected = config['selected_features']
                X_train = X_train[selected]
                X_valid = X_valid[selected]
                X_test = X_test[selected]
                logger.info(f"[CoreTraining] ä½¿ç”¨æŒ‡å®šç‰¹å¾: {len(selected)} ä¸ª")

            # ======== æ­¥éª¤2: åˆ›å»ºè®­ç»ƒå™¨ ========
            trainer = ModelTrainer(
                model_type=config['model_type'],
                model_params=config.get('model_params', {}),
                output_dir=str(self.models_dir)
            )

            # ======== æ­¥éª¤3: è®­ç»ƒæ¨¡å‹ ========
            logger.info(f"[CoreTraining] å¼€å§‹è®­ç»ƒ {config['model_type']} æ¨¡å‹...")

            if use_async:
                # å¼‚æ­¥è®­ç»ƒï¼ˆæ‰‹åŠ¨è®­ç»ƒæœåŠ¡ï¼‰
                if config['model_type'] == 'lightgbm':
                    await asyncio.to_thread(
                        trainer.train,
                        X_train, y_train,
                        X_valid, y_valid,
                        early_stopping_rounds=config.get('early_stopping_rounds', 50),
                        verbose_eval=50
                    )
                elif config['model_type'] == 'gru':
                    await asyncio.to_thread(
                        trainer.train,
                        X_train, y_train,
                        X_valid, y_valid,
                        seq_length=config.get('seq_length', 20),
                        batch_size=config.get('batch_size', 64),
                        epochs=config.get('epochs', 100),
                        early_stopping_patience=10
                    )
                else:
                    raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {config['model_type']}")
            else:
                # åŒæ­¥è®­ç»ƒï¼ˆè‡ªåŠ¨å®éªŒæœåŠ¡ï¼‰
                if config['model_type'] == 'lightgbm':
                    trainer.train_lightgbm(
                        X_train=X_train,
                        y_train=y_train,
                        X_valid=X_valid,
                        y_valid=y_valid
                    )
                elif config['model_type'] == 'gru':
                    trainer.train_gru(
                        X_train=X_train,
                        y_train=y_train,
                        X_valid=X_valid,
                        y_valid=y_valid
                    )
                else:
                    raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {config['model_type']}")

            # ======== æ­¥éª¤4: è¯„ä¼°æ¨¡å‹ï¼ˆåœ¨æ‰€æœ‰ä¸‰ä¸ªæ•°æ®é›†ä¸Šï¼‰ ========
            logger.info(f"[CoreTraining] è¯„ä¼°æ¨¡å‹ï¼ˆåœ¨Train/Valid/Testä¸‰ä¸ªæ•°æ®é›†ä¸Šï¼‰...")

            # åœ¨æ‰€æœ‰ä¸‰ä¸ªæ•°æ®é›†ä¸Šè¯„ä¼°
            if use_async:
                train_metrics = await asyncio.to_thread(
                    trainer.evaluate, X_train, y_train, dataset_name='train', verbose=False
                )
                valid_metrics = await asyncio.to_thread(
                    trainer.evaluate, X_valid, y_valid, dataset_name='valid', verbose=False
                )
                test_metrics = await asyncio.to_thread(
                    trainer.evaluate, X_test, y_test, dataset_name='test', verbose=False
                )
            else:
                train_metrics = trainer.evaluate(X_train, y_train, dataset_name='train', verbose=False)
                valid_metrics = trainer.evaluate(X_valid, y_valid, dataset_name='valid', verbose=False)
                test_metrics = trainer.evaluate(X_test, y_test, dataset_name='test', verbose=False)

            # è¾“å‡ºè¯„ä¼°ç»“æœæ‘˜è¦
            logger.info(f"\n{'='*80}")
            logger.info(f"ğŸ“Š æ¨¡å‹è¯„ä¼°ç»“æœ")
            logger.info(f"{'='*80}")
            logger.info(f"Train  - IC: {train_metrics.get('ic', 0):>7.4f}, Rank IC: {train_metrics.get('rank_ic', 0):>7.4f}, RÂ²: {train_metrics.get('r2', 0):>7.4f}")
            logger.info(f"Valid  - IC: {valid_metrics.get('ic', 0):>7.4f}, Rank IC: {valid_metrics.get('rank_ic', 0):>7.4f}, RÂ²: {valid_metrics.get('r2', 0):>7.4f}")
            logger.info(f"Test   - IC: {test_metrics.get('ic', 0):>7.4f}, Rank IC: {test_metrics.get('rank_ic', 0):>7.4f}, RÂ²: {test_metrics.get('r2', 0):>7.4f}")

            # ICå¼‚å¸¸æ£€æµ‹ - é˜²æ­¢æ•°æ®æ³„éœ²æ¨¡å‹è¿›å…¥ç”Ÿäº§ç¯å¢ƒ
            is_valid, alerts = self.ic_validator.validate_all(
                train_ic=train_metrics.get('ic', 0),
                valid_ic=valid_metrics.get('ic', 0),
                test_ic=test_metrics.get('ic', 0),
                train_r2=train_metrics.get('r2'),
                test_r2=test_metrics.get('r2'),
                model_id=model_id,
                symbol=config.get('symbol')
            )

            # æ‰“å°å‘Šè­¦ä¿¡æ¯
            self.ic_validator.print_alerts(alerts)

            # è·å–éªŒè¯æ€»ç»“å¹¶è®°å½•
            validation_summary = self.ic_validator.get_validation_summary(is_valid, alerts)
            logger.info(f"{validation_summary}")
            logger.info(f"{'='*80}\n")

            # ä½¿ç”¨æµ‹è¯•é›†æŒ‡æ ‡ä½œä¸ºä¸»è¦æŒ‡æ ‡ï¼ˆå‘åå…¼å®¹ï¼‰
            metrics = test_metrics.copy()

            # æ·»åŠ åˆ†å±‚æŒ‡æ ‡åˆ°ç»“æœä¸­
            metrics['train_ic'] = train_metrics.get('ic', 0)
            metrics['train_rank_ic'] = train_metrics.get('rank_ic', 0)
            metrics['train_r2'] = train_metrics.get('r2', 0)
            metrics['valid_ic'] = valid_metrics.get('ic', 0)
            metrics['valid_rank_ic'] = valid_metrics.get('rank_ic', 0)
            metrics['valid_r2'] = valid_metrics.get('r2', 0)

            # æ·»åŠ ICéªŒè¯çŠ¶æ€
            if not is_valid:
                metrics['validation_failed'] = True
                metrics['validation_summary'] = validation_summary
                logger.warning(f"âš ï¸  æ¨¡å‹ICéªŒè¯å¤±è´¥: {model_id}")
            else:
                metrics['validation_failed'] = False

            # ======== æ­¥éª¤5: ç”Ÿæˆæ¨¡å‹ID ========
            if model_id is None:
                model_id = self._generate_model_id(config)

            # ======== æ­¥éª¤6: ä¿å­˜æ¨¡å‹ ========
            logger.info(f"[CoreTraining] ä¿å­˜æ¨¡å‹: {model_id}")

            if use_async:
                await asyncio.to_thread(
                    trainer.save_model,
                    model_id,
                    save_metrics=True
                )
            else:
                trainer.save_model(
                    model_name=model_id,
                    save_metrics=True
                )

            # ç¡®å®šæ¨¡å‹æ–‡ä»¶è·¯å¾„
            if config['model_type'] == 'lightgbm':
                model_path = self.models_dir / f"{model_id}.txt"
            else:
                model_path = self.models_dir / f"{model_id}.pth"

            # ======== æ­¥éª¤7: ä¿å­˜Scaler ========
            scaler_path = self.models_dir / f"{model_id}_scaler.pkl"
            with open(scaler_path, 'wb') as f:
                pickle.dump(pipeline.get_scaler(), f)
            logger.info(f"[CoreTraining] âœ… Scalerå·²ä¿å­˜: {scaler_path}")

            # ======== æ­¥éª¤8: ä¿å­˜ç‰¹å¾æ•°æ®ï¼ˆå¯é€‰ï¼‰ ========
            features_path = None
            if save_features:
                # åˆå¹¶æ‰€æœ‰æ•°æ®é›†
                X_all = pd.concat([X_train, X_valid, X_test]).sort_index()
                y_all = pd.concat([y_train, y_valid, y_test]).sort_index()

                features_path = self.models_dir / f"{model_id}_features.pkl"
                with open(features_path, 'wb') as f:
                    pickle.dump({'X': X_all, 'y': y_all}, f)
                logger.info(f"[CoreTraining] âœ… ç‰¹å¾æ•°æ®å·²ä¿å­˜: {len(X_all)} æ¡è®°å½•")

            # ======== æ­¥éª¤9: æå–å¯è§†åŒ–æ•°æ® ========
            feature_importance = None
            training_history = None

            if config['model_type'] == 'lightgbm' and hasattr(trainer.model, 'get_feature_importance'):
                # LightGBM: ç‰¹å¾é‡è¦æ€§
                try:
                    importance_df = trainer.model.get_feature_importance('gain', top_n=20)
                    if importance_df is not None and not importance_df.empty:
                        feature_importance = dict(zip(
                            importance_df['feature'].tolist(),
                            importance_df['gain'].tolist()
                        ))
                except Exception as e:
                    logger.warning(f"[CoreTraining] è·å–ç‰¹å¾é‡è¦æ€§å¤±è´¥: {e}")

            if config['model_type'] == 'gru' and save_training_history:
                # GRU: è®­ç»ƒå†å²
                history = trainer.training_history
                if history and 'train_loss' in history:
                    training_history = {
                        'train_loss': [float(loss) for loss in history['train_loss']],
                        'valid_loss': [float(loss) for loss in history.get('valid_loss', [])]
                    }

            # ======== æ­¥éª¤10: è®¡ç®—è®­ç»ƒè€—æ—¶ ========
            end_time = datetime.now()
            train_duration = (end_time - start_time).total_seconds()

            # ======== è¿”å›ç»“æœ ========
            result = {
                'model_id': model_id,
                'model_name': model_id,  # å…¼å®¹æ—§æ¥å£
                'model_path': str(model_path),
                'scaler_path': str(scaler_path),
                'features_path': str(features_path) if features_path else None,
                'metrics': metrics,
                'feature_importance': feature_importance,
                'training_history': training_history,
                'train_duration': int(train_duration),
                'train_samples': len(X_train),
                'valid_samples': len(X_valid),
                'test_samples': len(X_test),
                'feature_count': len(X_train.columns),
                'trained_at': end_time.isoformat()
            }

            logger.info(f"[CoreTraining] âœ… è®­ç»ƒå®Œæˆ! IC={metrics.get('ic', 0):.4f}, è€—æ—¶={train_duration:.1f}ç§’")

            return result

        except Exception as e:
            logger.error(f"[CoreTraining] âŒ è®­ç»ƒå¤±è´¥: {e}", exc_info=True)
            raise

    def _generate_model_id(self, config: Dict) -> str:
        """
        ç”Ÿæˆæ¨¡å‹ID

        æ ¼å¼: {symbol}_{model_type}_T{target_period}_{scaler_type}_{timestamp}
        """
        symbol = config['symbol']
        model_type = config['model_type']
        target_period = config.get('target_period', 5)
        scaler_type = config.get('scaler_type', 'robust')
        timestamp = int(time.time() * 1000)  # æ¯«ç§’çº§æ—¶é—´æˆ³

        return f"{symbol}_{model_type}_T{target_period}_{scaler_type}_{timestamp}"
